\documentclass[a4paper,12pt]{amsart}
\usepackage{amsmath,amssymb,latexsym,amscd,xypic}
\usepackage{graphicx}
\parskip=5pt
\parindent=0pt
\usepackage[french]{babel}
\DeclareMathOperator{\trdeg}{\mathrm{trdeg}}
\DeclareMathOperator{\Gal}{\mathrm{Gal}}
\DeclareMathOperator{\bx}{\mathbf{x}}
\DeclareMathOperator{\by}{\mathbf{y}}
\DeclareMathOperator{\cc}{\mathbb{C}}
\DeclareMathOperator{\ff}{\mathbb{F}}
\DeclareMathOperator{\hh}{\mathbb{H}}
\DeclareMathOperator{\nn}{\mathbb{N}}
\DeclareMathOperator{\qq}{\mathbb{Q}}
\DeclareMathOperator{\rr}{\mathbb{R}}
\DeclareMathOperator{\zz}{\mathbb{Z}}
\DeclareMathOperator{\uu}{\mathbf{u}}
\DeclareMathOperator{\vv}{\mathbf{v}}
\DeclareMathOperator{\ww}{\mathbf{w}}
\DeclareMathOperator{\ba}{\mathbf{a}}
\DeclareMathOperator{\vo}{\mathbf{O}}
\DeclareMathOperator{\oo}{\mathcal{O}}
\DeclareMathOperator{\disc}{\mathrm{disc}}
\DeclareMathOperator{\Trd}{\mathrm{Trd}}
\DeclareMathOperator{\End}{\mathrm{End}}
\DeclareMathOperator{\Id}{\mathrm{Id}}
\DeclareMathOperator{\M}{\mathrm{M}}
\DeclareMathOperator{\Tr}{\mathrm{Tr}}
\DeclareMathOperator{\im}{\rm Im}
\DeclareMathOperator{\diff}{{\mathfrak{d}}}
\DeclareMathOperator{\pp}{\mathfrak{p}}
\DeclareMathOperator{\PP}{\mathfrak{P}}
\DeclareMathOperator{\ed}{\mathrm{ed}}
\DeclareMathOperator{\vp}{\mathrm{vp}}
\DeclareMathOperator{\Mat}{\mathrm{Mat}}
\renewcommand{\Re}{\mathrm{Re}}
\newcommand{\ds}{\displaystyle}

%\def\norm#1{{\vert\vert#1\vert\vert}}


\def#1{{erline{#1}}}
\def\zn#1{\zz/#1\zz}
\def\fq#1{\ff_{#1}}
%\def\ps#1#2{{\langle#1#2\rangle}}



\newtheorem{prop}{Proposition}[section]
\newtheorem{thm}[prop]{Th\'{e}or\`{e}me}
\newtheorem{lem}[prop]{Lemme}
\newtheorem{coro}[prop]{Corollaire}


\theoremstyle{definition}

\newtheorem{defn}{D\'{e}finition}
\newtheorem{ex}[prop]{Exemple}
\newtheorem{exs}[prop]{Exemples}
\newtheorem{exe}[prop]{Exercise}
\newtheorem{rem}[prop]{Remarque}
\newtheorem{rems}[prop]{Remarques}
\newcommand{\trick}{\begin{flushleft}
\medskip
\end{flushleft}}

\parskip=5pt
\parindent=0pt

\begin{document}

\begin{center}
\bf Alg\`{e}bre bilin\'{e}aire et analyse de Fourier, premi\`ere partie.
\end{center}

%\tableofcontents

%\newpage
\section{Motivations}
Dans cette section, nous allons consid\'erer deux probl\`emes de 
physique dont la solution semble n\'ecessiter des techniques math\'ematiques
plus sophistiqu\'ees que celles que vous avez utilis\'ees jusqu'\`a pr\'esent.

\subsection{L'\'equation de la chaleur.}
Supposons donn\'ee une barre chauff\'ee de fa\c con inhomog\`ene. 
Comment se diffuse la chaleur dans cette barre ? \\ \\
On consid\`{e}re une barre d'un mat\'{e}riau homog\`{e}ne de longueur finie $L$, 
la temp\'{e}rature initiale (au temps $t=0$) \'{e}tant donn\'{e}e par une fonction 
$\varphi:[0,L]\to \rr$, ou $x\mapsto \varphi(x).$\\ \\ 
On suppose que les \'echanges de chaleur entre la barre et l'air sont 
n\'egligeables et que les extremit\'es de la barre sont au contact d'un parfait
isolant, ce qui implique qu'il n'y a pas de flux de chaleur \`a travers 
ces extr\'emit\'es. En particulier le gradient de la chaleur y est nul.  
On veut comprendre comment la chaleur se diffuse dans la barre avec 
le temps ; autrement dit, si $T(x,t)$ est la temp\'erature dans la barre au point $x$ en un temps $t$, alors on
veut comprendre l'\'evolution de la valeur de $T(x,t)$ avec $t$. \\ \\
Des consid\'erations physiques montrent que $T$ doit satisfaire \`a 
l'\'equation, dite \'equation de la chaleur :
\[\frac{\partial T}{\partial t}= k \frac{\partial^2 T}{\partial^2 x} \]
o\`u $k$ est une constante positive
(la {\it conductivit\'e thermique}) qui d\'epend du mat\'eriau.
Nous avons en plus les conditions au bord \[\frac{\partial T}{\partial x}(0,t)= 
\frac{\partial T}{\partial x}(L,t)= 0\mbox{ pour tout }t,\] 
qui traduisent l'absence de flux de chaleur \`a travers les extr\'emit\'es, et
la condition initiale \[T(x,0)= \varphi(x).\] 
Oublions d'abord la condition $T(x,0)=\varphi(x)$. Autrement dit, on cherche 
les solutions v\'{e}rifiant seulement les conditions au bord 
\[\frac{\partial T}{\partial x}(0,t)=
\frac{\partial T}{\partial x}(L,t)= 0\mbox{ pour tout }t.\]
L'\'equation \'etant beaucoup trop compliqu\'ee pour \^etre r\'esolue 
avec les m\'ethodes dont nous disposons actuellement, nous allons 
commencer par simplement chercher des {\it exemples} de fonctions qui la satisfont.
Les fonctions {\it \`a variables s\'epar\'es} (c'est-\`a-dire s'\'ecrivant dans la forme 
$T(x,t)=f(x)g(t)$) sont
une source f\'econde d'exemples satisfaisant \`a des \'equations aux d\'eriv\'ees
partielles, puisque de telles \'equations se simplifient souvent dans ce cas.
Nous commencerons donc 
par chercher des solutions de la forme $T(x,t)=f(x)g(t)$. On a alors que 
$$f(x)g'(t)=kf''(x)g(t),$$  
soit $$\frac{f''(x)}{f(x)}=\frac{g'(t)}{kg(t)},$$
au moins sur la r\'egion ou ni $f$ ni $g$ ne s'annule.
Notons que le membre de gauche est une fonction qui ne d\'epend que de $x$ et
le membre de droite est une fonction qui ne d\'epend que de $t$ :
comme $x$ et $t$ sont ind\'{e}pendantes, cela implique qu'il 
existe $\alpha\in\rr$ tel que 
$$\frac{f''(x)}{f(x)}=\frac{g'(t)}{kg(t)}=\alpha.$$
Ainsi, on a $$f''(x)-\alpha f(x)=0$$ et $$g'(t)-k\alpha g(t)=0.$$
On a donc $g(t)=\lambda e^{k\alpha t}$ pour $\lambda\in\rr$, et donc 
$g(t)\neq 0$ pour tout $t\geq 0$ (car on cherche $T$ non identiquement nulle).
La contrainte 
\[\frac{\partial T}{\partial x}(0,t)=\frac{\partial T}{\partial x}(L,t)
=0\] entra\^{\i}ne alors $f'(0)=f'(L)=0$. Pou r\'esoudre l'\'equation en $f$ il nous faut 
maintenant distinguer 3 cas.
\begin{enumerate}
\item 
Cas 1 : $\alpha=0$. On a alors $f''(x)=0$, et donc $f(x)=b_0x+a_0$. Les conditions $f(0)=f(L)=0$ imposent alors facilement $f(x)=a_0$ pour tout $x$. 
On a donc une 
premi\`ere solution de base
\[T_0(x,t)=1.\]
\item Cas 2 : $\alpha>0$. On peut alors 
poser $\alpha=\omega^2$ et  $f$ est de la forme 
$f(x)=a{e^{\omega x}+ be^{-\omega x}$. Les conditions que $f'(0)=0$ et $f'(L)=0$ impliquent 
alors $a=b=0$, et $f$ est identiquement nulle, ce qui est exclu.
 \item Cas 3 : $\alpha<0$. On peut alors poser $\alpha=-\omega^2$
et  $$f(x)=a\cos(\omega x)+b\sin(\omega x), a,b,\in\rr.$$
Puisque $f'(0)=0$ on a $b=0$, et puisque $f(L)=0$ on a $a\sin(\omega L)=0$. Puisque l'on cherche $T$ non nulle, on a 
$a\neq 0$ et donc $\sin(\omega L)=0$. 

Ainsi $\omega L=\pi n$ pour $n\geq 0$, et donc pour chaque $n$, on a une 
solution de la forme 
$$T_n(x,t)=\cos\left(\frac{n\pi x}{L}\right)e^{-\frac{\pi^2 n^2}{L^2}kt\right}.$$
\end{enumerate}
Pour chaque entier positif $n\geq 0 $ nous avons donc une solution de l'\'equation de la chaleur
 $$T_n(x,t)=\cos\left(\frac{n\pi x}{L}\right)e^{-\frac{\pi^2 n^2}{L^2}kt\right}.$$
(Nous pouvons int\'egrer la solution $T_0(x,t)=1$ dans cette famille de solutions
en consid\'erant qu'il s'agit de  $T_0(x,t)= \cos(0x)e^{-0t}$.)
La condition initiale $\varphi_n(x)$ correspondant \`a la solution $T_n(x,t)$
 est donn\'ee par $\varphi_n(x)= T_n(x,0)$, c'est \`a dire  
\[ \varphi_n(x)= \cos\left(\frac{n\pi x}{L}\right).\]
Nous avons donc trouv\'e une solution \`a l'\'equation de la chaleur pour certaines conditions initiales
bien particuli\`eres, c'est \`a dire certains cosinus. 
Est ce qu'on peut en construire d'autres solutions pour d'autres conditions 
initiales ?\\ \\
Notons tout d'abord que 
l'\'equation de la chaleur \`a une propri\'et\'e tr\`es utile :
\begin{rem}[Lin\'earit\'e de l'\'equation de la chaleur.]
 Si $T_1(x,t)$
et $T_2(x,t)$ sont deux solutions \`a l'\'equation de la chaleur alors toute fonction
$T(x,t)$ telle qu'il existe des r\'eels $\lambda, \mu\in \rr$ tels que 
\[ T(x,t)= \lambda T_1(x,t)+\mu T_2(x,t)\]
est encore une solution de cette \'equation.
(Une telle fonction est appell\'ee une {\it combinaison lin\'eaire} de $T_1$ et $T_2$). 
On dit alors que l'\'equation de la 
chaleur est une {\bf \'equation lin\'eaire}.
\end{rem} 
{\bf Exercice.} D\'emontrer que l'\'equation de la chaleur est une \'equation
lin\'eaire.\\ \\
En particulier, toute fonction qui est une combinaison 
lin\'eaire finie
\[T(x,t)=\lambda_0 T_0(x,t)+\lambda_1 T_1(x,t)+\lambda_2T_2(x,t)+\ldots +\lambda_n T_n(x,t)\] 
avec des nombres r\'eels $\lambda_0, \ldots, \lambda_n$ est encore
une solution de l'\'equation de la chaleur. Cette solution corresponde \`a la 
condition initiale \[\varphi(x)=T(x,0)\] c'est \`a dire
\[\varphi (x)= \lambda_0+
\lambda_1 \cos\left(\frac{\pi x}{L}\right)+\lambda_2\cos\left(\frac{2 \pi x}{L}\right)
+\ldots + \lambda_n\cos\left(\frac{n\pi x}{L}\right).\]
Nous savons donc trouver une solution pour l'\'equation de la chaleur pour certaines  
conditions initiales bien particuli\`eres : celles qui s'\'ecrivent  comme des
sommes finies de cosinus. \\ \\
Voil\`a maintenant 
une id\'ee : {\it est ce qu'on pourrait r\'esoudre cette \'equation pour
une condition initiale $\varphi$ quelconque en l'\'ecrivant comme une ``somme infinie''
de cosinus ?} \\ \\
%Supposons que pour chaque $n$ on choisisse, parmi les fonctions de la forme
%\[ \lambda_0+\lambda_1 \cos\left(\frac{\pi x}{L}\right)+
%\lambda_2\cos\left(\frac{2\pi x}{L}\right)
%+\ldots + \lambda_n\cos\left(\frac{n\pi x}{L}\right)\]
%celle qui est ``la plus proche'' de $\varphi$, appelons-la $S_n(\varphi)$. 
%On a alors une suite infinie 
%de fonctions $S_1(\varphi)$, $S_2(\varphi)$,$\ldots$, qui 
%approchent ``de mieux en mieux'' la fonction $\varphi$. Pour chaque
%condition initiale $S_n(\varphi)$ il existe
%une solution $T_n(\varphi)$ de l'\'equation de la 
%chaleur : peut-\^etre quand $n\rightarrow \infty$ aura-t-on que :
%\begin{enumerate}
%\item la suite
% $S_n(\varphi)$ converge vers $\varphi$ et
%\item  
%la suite $T_n(\varphi)$ converge vers une solution de l'\'equation de la 
%chaleur pour la condition initiale
%$\varphi$ ?
%\end{enumerate} 
\subsection{L'\'equation des ondes.}
Nous nous penchons maintenant sur un autre cas, en apparence diff\'erent. Un fil horizontal
de longueur $L$, soumis \`a une tension $T$ et de densit\'e lin\'eaire $\mu$, est tenu aux deux extremit\'es. Au temps $t=0$ il
est relach\'e et se met \`a osciller librement dans un plan vertical.\\ \\
Soit $D(x,t) $ la fonction \'egale au d\'eplacement vertical\footnote{par rapport \`a l'\'equilibre}
\`a l'instant $t$ de la partie du fil qui se trouve (\`a l'\'equilibre) \`a une distance $x$ d'une 
des extremit\'es.
\\
 Nous avons cette fois les conditions aux bords
\[ D(0, t)= D(L,t)=0,\]
qui traduisent le fait que le fil est attach\'e aux extr\'emit\'es.
Si le d\'eplacement initial du fil est d\'ecrit par la fonction $\phi(x)$ alors nous avons
aussi les conditions initiales \[D(x,0)= \phi(x)\mbox{ et }\frac{\partial D}{\partial t}(x,0)=0,\]
cette derni\`ere condition traduisant le fait que le fil est rel\^ach\'e \`a l'instant $t=0$
et se trouve donc \`a ce moment-l\`a au repos. 
Des consid\'erations physiques montrent que l'\'evolution de $D$ est d\'ecrite 
par l'\'equation des ondes
\[ \frac{\partial ^2 D}{\partial t^2}=k\frac{\partial^2 D}{\partial x^2}\]
o\`u $k$ est la constante positive $k=\frac{T}{\mu}$.
Cherchons comme ci-dessus des solutions de la forme $f(x)g(t)$. 
 On a alors
$$f(x)g''(t)=kf''(x)g(t),$$
soit $$\frac{f''(x)}{f(x)}=\frac{g''(t)}{kg(t)}.$$
Notons que le membre de gauche est une fonction qui ne d\'epend que de $x$ et
le membre de droite est une fonction qui ne d\'epend que de $t$ :
comme $x$ et $t$ sont deux variables ind\'{e}pendantes, cela implique qu'il
existe $\alpha\in\rr$ tel que
$$\frac{f''(x)}{f(x)}=\frac{g''(t)}{kg(t)}=\alpha.$$
Ainsi, on a $$f''(x)-\alpha f(x)=0\mbox{ et }g''(t)-k\alpha g(t)=0.$$
Le m\^eme raisonnement que ci-dessus nous montre que  cette \'equation a une solution
telle que  $D(0,t)= D(L, t)=0$ si et seulement si il existe un entier $n$ tel que
$\alpha= \frac{n^2\pi^2}{L^2}$ et dans ce cas on a une solution donn\'ee par
\[ D_n(x,t)= \sin\left(\frac{n\pi x}{L}\right)\cos\left(\frac{ \sqrt{k}n\pi t}{L}\right)
.\] 
Ceci nous donne une solution au probl\`eme pour une condition initiale
\[ \varphi_n(x)= \sin\left(\frac{n\pi x}{L}\right).\]
\begin{rem} L'\'equation des ondes est encore une \'equation lin\'eaire,
\end{rem} 
{\bf Exercice} D\'emontrer que l'\'equation des ondes est lin\'eaire.\\ \\
Puisque  la fonction $D_n(x,t)$ est une solution pour chaque $n$, 
toute combinaison
lin\'eaire finie
\[D(x,t)=\lambda_1 D_1(x,t)+\lambda_2D_2(x,t)+\ldots +\lambda_k D_k(x,t)\] 
ou les $\lambda_k$ sont des nombres r\'eels est encore
une solution de l'\'equation de la chaleur. Cette solution correspond \`a la
condition initiale
\[\varphi (x)= 
\lambda_1 \sin\left(\frac{\pi x}{L}\right)+\lambda_2\sin\left(\frac{2\pi x}{L}\right)
+\ldots + \lambda_n\sin\left(\frac{n\pi x}{L}\right).\]
Nous savons donc trouver une solution \`a cette \'equation pour des
conditions initiales bien particuli\`eres : celles qui s'\'ecrivent  comme des
sommes finies de sinus. \\ \\
La m\^eme id\'ee se pr\'esente que dans le cas de l'\'equation de la chaleur 
: {\it est ce qu'on pourrait r\'esoudre cette \'equation pour
une condition initiale quelconque $\varphi$ en \'ecrivant $\varphi$ 
comme une ``somme infinie'' de sinus ?} \\ \\
%Ou, pour exprimer la m\^eme id\'ee d'une autre fa\c con : {\it serait il possible 
%d'\'ecrire une condition initiale quelconque $\varphi$ comme une somme infinie
%de sinus, \`a fin d'y appliquer la solution trouv\'ee ci-dessus ?}\\ \\
%Cette id\'ee  de trouver des solutions \`a ces \'equations par approximations
% successives (c'est \`a dire par ``somme infinie'') 
%est s\'eduisante, mais pose plus de questions qu'elle ne r\'esout :
%\begin{enumerate}
%\item Quel sens donner \`a une somme infinie ?
%\item Qu'est ce que \c ca veut dire, quand on dit 
%que deux fonctions sont ``proches'' ?
%\item Plus pr\'ecisement, comment quantifier la ``distance'' entre deux 
%fonctions ?
%\item Comment calculer effectivement une ``bonne approximation'' \`a une 
%fonction donn\'ee ?
%\item Qu'est ce que cela signifie quand on dit 
%qu'une suite de fonctions converge
%vers une autre fonction ?
%\item Supposant qu'on trouve une r\'eponse aux questions 1)-4), est-ce qu'on
%a r\'eellement $S_n(\varphi)\rightarrow \varphi$ ?
%\item Si les fonctions $S_n(\varphi)$ convergent, est-ce que la m\^eme chose 
%est vraie pour les fonctions  $T_n$ ?
%\item Si chaque fonction $T_n$ satisfait l'\'equation de la chaleur, est-ce que
%cela est vraie pour la fonction limite ?
%\end{enumerate}
%R\'epondre \`a toutes ces questions va bien au-del\`a de ce que nous pouvons faire dans ce cours : nous allons
%nous concentrer surtout sur les questions 1) - 3). Dans le prochain chapitre nous allons
%nous pencher sur la question 1) : comment interpreter une somme infinie ? Faute de disposer d'une
%notion math\'ematique pr\'ecise de ``fonctions proches'' ou de ``distance entre deux fonctions'' nous parlerons
%dans ce chapitre uniquement d'une somme infinie de nombres r\'eels.
Avant de se lancer dans des sp\'eculations sur les sommes infinies de fonctions, il faudrait d\'ej\`a 
savoir ce que veut dire une somme infinie de nombres. Dans le prochain chapitre, nous allons \'etudier
les s\'eries\footnote{C'est le nom que les math\'ematiciens donnent aux sommes infinies.} num\'eriques. 
 \newpage
\section{S\'eries num\'eriques.}
Vous avez d\'ej\`a rencontr\'e au cours de vos  \'etudes l'\'equation suivante
\[ 1+\frac{1}{2}+\frac{1}{4}+\frac{1}{8}+\ldots =2\]
ou le symbole ``$\ldots$'' se comprend comme ``et ainsi de suite jusqu'\`a l'infini''. 
Quel sens donner \`a cette \'equation, et en particulier, quel sens donner \`a son membre de gauche 
$1+\frac{1}{2}+\frac{1}{4}+\frac{1}{8}+\ldots$ ? Ca ne peut pas signifer ``le r\'esultat
qu'on obtient en effectuant une infinit\'e d'additions'' puisqu'il est impossible de faire 
une infinit\'e d'additions. \\ \\
La somme infinie \`a gauche doit \^etre comprise comme une limite.  
En \'ecrivant cette \'equation,
nous disons la chose suivante :
\\ \\
{\it En prenant $n$ assez grand, nous pouvons rendre la somme finie \[1+\frac{1}{2}+
\frac{1}{4}+\ldots+\frac{1}{2^{n}}\] aussi proche qu'on veut \`a $2$.}
\\ \\
La somme infinie \[1+\frac{1}{2}+\frac{1}{4}+\frac{1}{8}+\ldots,\] que l'on \'ecrit
aussi $\sum_{n=0}^\infty \frac{1}{2^n}$, doit \^etre compris comme la
 {\bf limite de la suite
des sommes partielles $s_k=\sum_{n=0}^k \frac{1}{2^n}$}. 
\begin{defn}
Soit $(u_n)$ une collection de nombres r\'eels ou complexes, index\'es par les entiers naturels. 
Nous denotons par $(\sum_{n\geq 0} u_n)$ (resp. $(\sum_{n\geq m} u_n)$) la suite de sommes partielles
\[ s_k=u_0+u_1+u_2+\ldots u_k,\]
(resp.
\[s_k= u_m+u_{m+1}+\ldots u_{k+m}.)\]
Nous appelons cette suite {\it la {\bf s\'erie} de terme g\'en\'erale $u_n$}. 
\end{defn}
{\bf Exemples}
\begin{enumerate}
\item Si on pose, comme ci-dessus, $u_n=\frac{1}{2^n}$ et on consid\`ere la s\'erie $(\sum_{n\geq 0} u_n)$ alors la somme
partielle $s_k= \sum_{n=0}^k u_n$ est donn\'ee par 
\[ s_k=1+\frac{1}{2}+\ldots +\frac{1}{2^k}= 2-\frac{1}{2^k}.\]
Si on consid\`ere la s\'erie  $(\sum_{n\geq 3} u_n)$ alors la somme partielle \[s_k= \sum_{n=3}^{k+3} u_n\] est donn\'ee par 
\[ s_k= \frac{1}{8}+\frac{1}{16}+\ldots +\frac{1}{2^{k+3}}= \frac{1}{4}-\frac{1}{2^{k+3}}.\]
\item Si on pose $u_n=1$ pour tout $n$ et on consid\`ere la s\'erie $(\sum_{n\geq 0} u_n)$
alors les sommes partielles $s_k=\sum_{n=0}^k u_n $  sont donn\'ees pour tout $k$
par
\[s_k=1+1+\ldots +1=k+1.\]
\item Si on pose\footnote{ c'est \`a dire $u_0=1$, $u_1=-1$, $u_2=1$, $u_3=-1$ et ainsi de suite.} $u_n=(-1)^n$ 
et on consid\`ere la s\'erie $(\sum_{n\geq 0} u_n)$ alors les sommes partielles 
$s_k=\sum_{n=0}^k u_n$ sont donn\'ees par
\[ s_0=1\]\[s_1=1-1=0\] \[s_2= 1-1+1=1\]
et ainsi de suite, c'est \`a dire que pour tout $k$ paire nous avons que  
$s_{k}=1$ et pour tout $k$ impaire nous avons que $s_{k}=0$.
\item Si on pose $u_n=\frac{1}{n^2}$ et on consid\`ere la s\'erie
$(\sum_{n\geq 1} u_n)$ alors la somme partielle $s_k$ est le nombre r\'eel
\[ s_k=1+\frac{1}{4}+\frac{1}{9}+\ldots +\frac{1}{k^2}.\]
Contrairement aux autres cas, 
nous ne disposons d'aucune formule g\'en\'erale pour cette somme partielle. 
\end{enumerate}
\\ \\
Lorsque cette suite de sommes partielles $(s_k)_{k\geq 0}$ est convergeant, on d\'ecret
que sa limite est la valeur de la ``somme infinie'' 
\[\sum_{n=0}^{\infty} u_n=u_0+u_1+u_2+\ldots\]
\begin{defn}
Soit $(u_n)_{n\geq m}$ une suite infinie et consid\'erons la s\'erie 
$(\sum_{n\geq m} u_n)$. 
Nous disons que la s\'eries $(\sum_{n\geq m} u_n)$ admet comme limite le nombre fini 
$l$ si la suite $(s_k)_{k\geq 0}$ de sommes partielles converge vers $l$,
\[ \lim_'k\rightarrow \infty} s_k=l.\]
Dans ce contexte, nous disons que $l$ est la somme de la s\'erie $(\sum_{n\geq m} u_n)$
et nous \'ecrivons
\[ \sum_{n\geq m}^\infty u_n=l.\]
 \end{defn}
{\bf Attention !} Les deux notations 
\[ \left(\sum_{n\geq m} u_n \right)\]
et 
\[ \sum_{n=m}^\infty u_n,\]
qui sont tr\`es proches, d\'esignent quand m\^eme des choses diff\'erentes. 
Lorsque nous \'ecrivons $(\sum_{n\geq m} u_n )$ nous parlons de la {\bf suite} de sommes partielles $(s_k)_{k\geq 0}$
alors que $\sum_{n=m}^\infty u_n$ denote la {\bf limite} de cette suite (en supposant, 
bien sur, qu'elle existe). 
\begin{rem}\label{sequence}
Soit $(\sum_{n\geq m} u_n)$ une s\'erie et soit $(s_k)_{k\geq 0}$ ses sommes partielles. Si $(\sum_{n\geq m} u_n)$
converge vers $l$ alors on a que
\[ s_k\rightarrow_{k\rightarrow \infty} l\]
\[ s_{k-1}\rightarrow_{k\rightarrow\infty} l\]
donc
\[ s_{k}-s_{k-1}\rightarrow_{k\rightarrow \infty } 0\]
mais $s_k-s_{k-1}=u_{k+m}$ et donc $u_k\rightarrow_{k\rightarrow \infty} 0$.
\end{rem} 
{\bf Exemples.}
\begin{enumerate}
\item Pour la s\'erie $(\sum_{n\geq 0} \frac{1}{2^n})$ nous avons que la somme
partielle  
\[s_k=2-\frac{1}{2^n}\rightarrow_{k\rightarrow \infty} 2.\]
On peut donc \'ecrire \[\sum_{n=0}^\infty \frac{1}{2^n}=2.\] 
\item Soit maintenant  $\lambda$ un nombre r\'eel ou complexe tel que 
$|\lambda|<1$, et consid\'erons la s\'erie $(\sum_{n\geq 0} \lambda^n)$. La 
somme partielle \[s_k=1+\lambda +\ldots +\lambda^k\] peut \^etre calcul\'ee par
l'astuce suivante :
\[ (1-\lambda) s_k= s_k -\lambda s_k\]
\[(1-\lambda) s_k= (1+\lambda +\ldots +\lambda^k) -(\lambda+\lambda^2+\ldots
+\lambda^{k+1})\]
\[ (1-\lambda) s_k= 1-\lambda^{k+1}\]
\[s_k=\frac{1-\lambda^{k+1}}{1-\lambda}.\]
Puisque $|\lambda|<1$ nous avons que $\lambda^k\rightarrow_{k\rightarrow \infty} 0$ donc 
\[ s_k\rightarrow_{k\rightarrow \infty}\frac{1}{1-\lambda}.\]
Autrement dit, \[ \sum_{n=0}^{\infty}\lambda^{n}=\frac{1}{1-\lambda}.\]
\item La s\'erie $(\sum_{n\geq 0}1)$ a pour sommes partielles $s_k=k+1$. 
Cette suite n'est 
pas convergente : sa limite n'est
pas finie. On dit alors que la s\'eries $(\sum_{n\geq 0} 1)$ est {\bf divergente}\footnote{On aurait pu aussi remarquer que $u_n \not\rightarrow_{n\rightarrow  \infty} 0$ et donc cette s\'erie, par Remarque \ref{sequence}, ne converge pas.}.
\item La s\'erie $(\sum_{n\geq 0}(-1)^n)$ a pour sommes partielles \[s_k=1 
\mbox{ si k paire,  } s_k=0 \mbox{ si k impaire}.\] Cette suite
de sommes partielles, bien que born\'ee (les sommes partielles n'approchent pas $\infty$) ne converge pas. On
dit encore une fois que la s\'erie $(\sum_{n\geq 0}(-1)^n)$ est 
divergente.\footnote{De m\^eme, cette s\'erie ne peut pas converger par Remarque \ref{sequence}.}
\item M\^eme si nous ne disponsons d'aucun formule pour les sommes partielles 
$s_k=\sum_{n=1}^k \frac{1}{n^2}$
il est possible de montrer que cette suite converge vers une limite finie. Nous verrons \`a la fin du semestre
que \[ \lim_{k\rightarrow \infty} s_k=\frac{\pi^2}{6}\]
que nous pouvons aussi \'ecrire
\[ \sum_{n=1}^\infty \frac{1}{n^2}= \pi^2/6.\]
\end{enumerate}
La remarque suivante, qui suit des propri\'et\'es de lin\'earit\'e des suites,
est souvent utile dans l'\'etude des s\'eries.
\begin{rem}[Lin\'earit\'e des s\'eries]
Soient $(\sum_{n\geq m} u_n)$ et $(\sum_{n\geq m} v_n)$ deux s\'eries convergeantes r\'eelles ou complexes, 
de limites $u$ et $v$ respectivement. Alors
pour tout $\lambda, \mu\in \mathbb{C}$, la s\'erie
\[(\sum_{n\geq m} \lambda u_n+\mu v_n)\]
est convergeante, avec limite $\lambda u+ \mu v.$
\end{rem} 
Le cas des s\'eries r\'eelles \`a termes positives est particuli\`erement int\'eressant, \`a cause de la dichotomie suivante.
\begin{lem}\label{dichotomy}
Soit $(\sum_{n\geq m} u_n)$ une s\'erie r\'eelle dont toutes les termes $u_n$ sont positives. Pour tout $k\geq m$ soit $s_k$
la somme partielle \[s_k=\sum_m^{k+m} u_n.\] 
Il y a alors deux possibilit\'es
\begin{enumerate}
\item la suite $(s_k)_{k\geq 0}$ converge vers une limite finie $l$. Autrement dit, la s\'eries $(\sum_{n\geq m} u_n)$ est convergente
\item la suite de sommes partielles $(s_k)_{k\geq 0}$ tend vers $+\infty$.
\end{enumerate}
\end{lem}
\begin{proof}
Pour tout $k$ la diff\'erence $s_{k}-s_{k-1}= u_{k+m}\geq 0$ et la suite $s_k$ est donc croissante. Par le th\'eor\`eme des suites
major\'ees croissantes\footnote{Ce th\'eor\`eme dit que toute suite r\'eelle major\'ee et croissante converge.}
il y a donc deux possibilit\'es
\begin{enumerate}
\item la suite croissante $s_k$ est major\'ee et elle converge \`a une limite finie.
\item la suite croissante $s_k$ n'est pas major\'ee et elle tend \`a $+\infty$. 
\end{enumerate}
Le lemme est donc \'etabli.
\end{proof}
A cause de ce lemme, il sera utile de pouvoir parler uniquement de s\'eries positives. Nous introduisons la notion de s\'erie
absolument convergeante.
\begin{defn}
Soit $(\sum_{n\geq m} u_n)$ une s\'erie. On dit que $(\sum_{n\geq m}u_n)$ est absolument convergeant si la s\'erie
$(\sum_{n\geq m}|u_n|)$ est convergeante.
\end{defn}
Nous admettrons le r\'esultat suivant.
\begin{prop}
Toute s\'erie absolument convergeante est convergeante.
\end{prop}
La converse de cette proposition est fausse : il existe des s\'eries r\'eelles convergeantes qui en sont pas absolument convergeantes. 
Leur comportement est pathologique -- par exemple, en permutant les termes d'une telle s\'erie on peut la rendre divergeante, 
ou la faire converger vers n'importe quel nombre r\'eel -- et nous n'en parlerons pas. 
\begin{rem} Le comportement de la s\'eries de terme g\'en\'erale $u_n=(-1)^n$, 
qui diverge sans tendre vers $+\infty$, 
n'est possible que parce que certains termes de cette s\'eries sont negatives.
\end{rem}
Le lemma \ref{dichotomy} entra\^ine le corollaire suivant.
\begin{coro}\label{equivalence}
Soient $(\sum_{n\geq m}u_n)$, $(\sum_{n\geq m}v_n)$ des s\'eries avec un nombre 
fini de termes n\'egatives. Alors :
\begin{enumerate}
\item Si $u_n\leq v_n$ pour tout $n$ et $(\sum_{n\geq m} v_n)$ converge alors $(\sum_{n\geq m} u_n)$ converge aussi.
\item Si $u_n\sim_{n\rightarrow \infty} u'_n$ alors la s\'erie $(\sum_{n\geq m} u_n)$ converge si et seulement si
la s\'erie $(\sum_{n\geq m} v_n)$ converge aussi.
\end{enumerate}
\end{coro}
\begin{proof}
En commen\c cant la somme \`a une indice $m$, on peut supposer que toutes les 
termes des deux suites sont positives. 
\begin{enumerate}
\item  Soit $(s_k)_{k\geq 0}$ la suite de sommes partielles de la s\'erie $(\sum_{n\geq m} u_n)$. Soit $(t_k)_{k\geq 0}$ 
la suite de sommes partielles de la s\'erie
$(\sum_{n\geq m} v_n)$. Puisque $(\sum_{n\geq m} v_n)$ converge la suite $(t_k)_{k\geq 0}$ est major\'ee. 
Puisque $u_n\leq v_n$ pour tout $n$, $s_k\leq t_k$ pour tout $k$, 
donc la suite $s_k$ est major\'ee. Par Lemme \ref{dichotomy}  il suit que  la s\'eries $(\sum_{n\geq m} u_n)$ converge. 
\item Nous pr\'esentons cette d\'emonstration dans le cas ou $u_n$ et $v_n$ sont {\it strictement} positifs : le cas 
ou certaines termes peuvent \^etre z\'ero est similaire. \\
Puisque $u_n\sim_{n\rightarrow \infty} v_n$ et $u_n, v_n>0$ il existe des nombres r\'eels strictement positifs $\alpha, \beta$
tels que, pour tout $n$, 
\[ \alpha v_n\geq u_n.\]
\[ \beta u_n\geq v_n.\]
Nous avons donc par (1) que 
\[ (\sum_{n\geq m} u_n )\mbox{ converge } \Rightarrow (\sum_{n\geq m} \beta u_n) \mbox{ converge }\Rightarrow (\sum_{n\geq m} v_n) \mbox{ converge }.\]
De m\^eme 
\[ (\sum_{n\geq m} v_n) \mbox{ converge } \Rightarrow (\sum_{n\geq m} \alpha v_n) \mbox{ converge } \Rightarrow (\sum_{n\geq m} u_n) \mbox{ converge }.\]
\end{enumerate}
Ceci termine la d\'emonstration du Corollaire \ref{equivalence}.
\end{proof}\\ \\
{\bf Exercice.}
Montrer que le Corollaire \ref{equivalence}  est toujours valable lorsque :
\begin{enumerate}
\item les deux s\'eries sont \`a termes n\'egatives,
 \end{enumerate}
\begin{rem}
Si la suite $u_n$ ne comprend qu'un nombre fini de termes negatives et la suite
$v_n$ satisfait $v_n\sim u_n$ alors la suite $v_n$ ne comprend qu'un nombre fini de termes negatives. Il suffit donc de v\'erifier cette condition sur une seule des deux suites.
\end{rem}
Vous avez \'etudi\'e en MAT123 les developpements limit\'es. Utilisant ces d\'eveloppements, il est tr\`es souvent possible
de montrer qu'une suite donn\'ee est \'equivalente \`a une suite de la forme $(\frac{1}{n^s})_{n\geq 1}$. 
\\ \\
{\bf Exemples}
\begin{enumerate}
\item Consid\'erons $u_n=\sin(\frac{1}{n})$. Nous avons que \[\sin\left(\frac{1}{n}\right)=_{n\rightarrow \infty} 
\frac{1}{n}+o\left(\frac{1}{n}\right),\]
c'est \`a dire que $\sin(\frac{1}{n})\sim_{n\rightarrow \infty}\frac{1}{n}.$
\item Consid\'erons $u_n=e^{1/n}-1$. Nous avons que \[e^{1/n}=_{n\rightarrow \infty} 1+ \frac{1}{n} +o(\frac{1}{n})\]
et donc $(e^{1/n}-1)=_{n\rightarrow \infty} \frac{1}{n} +o(\frac{1}{n})$, c'est \`a dire 
$e^{1/n} \sim_{n\rightarrow \infty} \frac{1}{n}$.
\item Consid\'erons $u_n=\frac{\cos(\frac{1}{n}) -1}{n}$. Nous avons que \[\cos(\frac{1}{n})=_{n\rightarrow \infty}
1-\frac{1}{2n^2}+
o(\frac{1}{n^2}\] et donc $\cos({\frac{1}{n})-1\sim_{n\rightarrow \infty} \frac{-1}{2n^2}$, d'ou il vient que 
\[  \frac{\cos(\frac{1}{n}) -1}{n}\sim_{n\rightarrow \infty} \frac{-1}{2n^3}.\]
\item Consid\'erons $u_n= \frac{ \cos(\frac{1}{n}) -1}{ \sin(\frac{1}{n})}\sim_{n\rightarrow \infty} \frac{\frac{-1}{2n^2}}{\frac{1}{n}}
= \frac{-1}{2n}.$
\end{enumerate}
La corollaire \ref{equivalence} sera donc un outil tr\`es puissant pour \'etablir la convergence des s\'eries,
\`a condition de savoir quand la s\'erie 
\[ \left(\sum_{n>0} \frac{1}{n^s}\right)\]
converge.
\begin{prop}[Crit\`ere de Riemann.]\label{Riemann}
Pour tout  nombre r\'eel positif $s>0$ la suite infinie 
\[ \left(\sum_{n\geq 1} \frac{1}{n^s}\right) \]
 diverge si $s\leq 1$ et converge si $s>1$.
\end{prop}
\begin{proof}
On doit d\'eterminer quand la suite de sommes partielles 
\[ s_k=\sum_{n=1}^k \frac{1}{n^s}\]
converge. Puisque la suite $(u_n)$ est \`a termes positives il suffit par le lemma \ref{dichotomy}
de savoir quand la suite
$s_k$ est major\'ee. Nous allons faire cela par une t\'echnique tr\`es puissante : comparision d'une somme avec 
une int\'egrale. Il y a en effet un lien proche entre  l'int\'egrale $\int_1^k f(x) dx$ et la somme
$\sum_{n=1}^k f(n)$.\\ \\
Puisque $s>0$ pour tout $x$ tel que  $x\in [n, n+1]$ nous avons que 
\[ \frac{1}{n^s}\geq \frac{1}{x^s}\geq \frac{1}{(n+1)^s}.\]Il en suit que 
\[ \int_{n}^{n+1} \frac{1}{n^s} dx \geq \int_{n}^{n+1} \frac{1}{x^s} dx \geq \int_{n}^{n+1} \frac{1}{(n+1)^s} dx.\]
cest \`a dire que pour tout entier positif $n$ nous avons que 
\[ \frac{1}{n^s} \geq \int_n^{n+1} \frac{1}{x^s} dx \geq \frac{1}{(n+1)^s}.\]
En sommant ces in\'egalit\'es, nous obtenons que 
\[ 1+\frac{1}{2^s}+\ldots+\frac{1}{n^s} \geq \int_1^2 \frac{1}{x^s} dx+\int_2^3 \frac{1}{x^s} dx+\ldots+ \int_n^{n+1} 
\frac{1}{x^s} dx\geq 
\frac{1}{2^s}+\frac{1}{3^s}+\ldots \frac{1}{(n+1)^s}.\]
Autrement \'ecrit
\[ \sum_{n=1}^k \frac{1}{n^s} \geq \int_{1}^{k+1} \frac{1}{x^s}dx\geq \sum_{n=2}^{k+1}\frac{1}{n^s},\]
c'est \`a dire
\[ s_k\geq \int_{1}^{k+1} \frac{1}{x^s}dx\geq  s_k-\frac{k}{k+1}.\]
Re-organisant ces \'equations, nous obtenons que 
\[ \int_1^k \frac{1}{x^s} dx \leq s_k\leq \int_1^k \frac{1}{x^s} dx+1.\]
Nous allons maintenant distinguer 3 cas, selon que $s$ est plus grand que, \'egale \`a ou plus petit que $1$.
\begin{enumerate}
\item Cas 1 : $s>1$. Nous avons alors que \[\int_1^k x^{-s} dx= \left[ \frac{x^{1-s}}{1-s}\right]_1^k 
=\frac{1-k^{1-s}}{s-1}\leq 
\frac{1}{s-1}.\]
On a donc que pour tout $k$
\[ s_k\leq \frac{1}{s-1}+1.\]
La suite $s_k$ est donc major\'ee et la s\'erie $(\sum_{n\geq 1} \frac{1}{n^s})$ converge.
\item Cas 2 : $s=1$. Nous avons alors que \[\int_1^k x^{-1} dx=[ \log (x)]_1^k= \log(k)\rightarrow_{k\rightarrow \infty}
\infty.\]
Nous avons donc que $s_k\geq \log(k)$ pour tout $k$ : la suite $s_k$ ne peut donc pas \^etre major\'ee et par le lemma
\ref{dichotomy} la suite $(\sum_{n\geq m}\frac{1}{n})$ doit diverger.
\item Cas 3 : $s<1$. Pour tout entier positif $n$ nous avons alors que $\frac{1}{n^s}\geq \frac{1}{n}>0$.
Comme $(\sum_{n\geq 1} \frac{1}{n})$ ne converge pas, il suit du Lemma \ref{dichotomy} que 
$ (\sum_{n\geq 1} \frac{1}{n^s})$ ne converge pas non plus.
\end{enumerate}
Ceci termine d\'emonstration de la proposition \ref{Riemann} 
\end{proof}
Les exemples qui suivent montrent \`a quel point l'attelage du Corollaire \ref{equivalence} avec la Proposition \ref{Riemann} 
est un outil puissant pour d\'eterminer si des s\'eries positives convergent
ou divergent.
\begin{enumerate}
\item Soit $u_n=\sin(\frac{1}{n})$ pour tout $n\geq 1$. Nous avons vu que 
\[u_n\sim_{n\rightarrow \infty} \frac{1}{n}.\] Puisque la s\'eries 
$(\sum_{n\geq 1} \frac{1}{n})$ diverge par la proposition \ref{Riemann}, 
il suit de la corollaire 
\ref{equivalence} que $(\sum_{n\geq 1} u_n)$ diverge aussi.
\item Soit $u_n=1-\cos(\frac{1}{n})$ pour tout $n\geq 1$. Nous avons vu que
$u_n\sim_{n\rightarrow \infty} \frac{1}{2n^2}$. Puisque la s\'eries
$(\sum_{n\geq 1} \frac{1}{n^2})$ converge par la Proposition 
\ref{Riemann}, il suit de la corollaire
\ref{equivalence} que $(\sum_{n\geq 1} u_n)$ converge aussi.
\item Soit $u_n= \frac{1-\cos(\frac{1}{n})}{\sqrt{n}}$. Nous avons que
\[ u_n\sim_{n\rightarrow \infty} \frac{1}{2n^2\sqrt{n}}= 2n^{-\frac{5}{2}}.\]
Puisque la s\'eries $(\sum_{n\geq 1} \frac{1}{n^{5/2}})$ converge par la Proposition 
\ref{Riemann}, il suit de la corollaire
\ref{equivalence} que $(\sum_{n\geq 1} u_n)$ converge aussi.
\item Soit $u_n= \frac{\sin\left(\frac{1}{n}\right)}{e^{\frac{1}{\sqrt{n}}}-1}$. Par les developpements limit\'es, on a que
\[\sin(\frac{1}{n})\sim_{n\rightarrow \infty}\frac{1}{n}\] et \[e^{\frac{1}{\sqrt{n}}}-1\sim_{n\rightarrow\infty} n^{-1/2}.\] Il en suit que
\[u_n\sim_{n\rightarrow \infty} \frac{1}{n*n^{1/2}}= n^{-3/2}.\] Puisque la s\'eries
$(\sum_{n\geq 1} \frac{1}{n^{3/2}})$ converge par la Proposition 
\ref{Riemann}, il suit de la corollaire
\ref{equivalence} que $(\sum_{n\geq 1} u_n)$ converge aussi.
\end{enumerate}
%Il existe deux types de fonctions qui ne sont pas couvertes par ces th\'eor\`emes : ce sont
%les fonctions logarithmiques et exponentielles. La g\'en\'eralisation suivantes du th\'eor\`eme
%ci-dessus nous permet de traiter les fonctions logarithmiques
%\begin{prop}
%La s\'eries $\sum \frac{1}{n^\alpha \log(n)^\beta}$ convergent si et seulement si 
%\begin{enumerate}
%\item $\alpha>1$ {\bf OU}
%\item $\alpha =1$ et$\beta >1$.
%\end{enumerate}
%\end{prop} 
%Voil\`a quelques exemples de applications de ce th\'eor\`eme.
%\begin{enumerate}
%\item EXEMPLES
%\item EXEMPLES
%\item EXEMPLES
%\end{enumerate}
%Finissons avec le th\'eor\`eme de d'Alembert, qui traite le cas de fonctions exponentielles.
%\begin{prop}
%Soit $\sum u_k$ une s\'eries telle que $\frac{u_{k+1}}{u_k}\rightarrow_{k\rightarrow \infty} \lambda$.
%Si $\lambda <1$ alors la s\'eries $\sum u_k$ est absolument convergente. Si $\lambda >1$ alors la s\'eries
%$\sum u_k$ diverge.
%\end{prop}
%DEMONSTRATION.
%Il existe $l$ tel que $\forall n>l$ $|u_{n+1}|/|u_n|\leq \lambda+\epsilon$. Il en suit que pour tout 
%$m\geq n$ $|u_{m}| \leq (\lambda+\epsilon)^{m-n} u_n$. puisque $\lambda+\epsilon <1$ la s\'eries
%$\sum u_n (\lambda+\epsilon)^{m-n}$ converge et puisque $|u_m|$ est positive la s\'eries $\sum_{m\geq l} |u_m|$ converge 
%CQFD.
%\begin{enumerate}
%\item EXEMPLES
%\item EXEMPLES
%\item EXEMPLES.
%\end{enumerate}
\subsection{Appendice : les s\'eries, cl\'e du calcul sur machine.}\\
Les s\'eries sont beaucoup utilis\'ees dans vos calculettes, ordinateurs et autres machines de calcul pour 
permettre aux ordinateurs de faire des calculs compliqu\'es. \\ \\
La machine calculatoire, typiquement, est capable de faire deux op\'erations de base :
\begin{enumerate}
\item addition de deux nombres
\item multiplication de deux nombres.
\end{enumerate}
Tout autre calcul sur machine n\'ecessite un algorithme permettant de
le mener \`a bout et n'utilisant que des additions et multiplications. 
Or, la plupart des fonctions qu'on souhaite calculer - cos, sin, exp, log et
ainsi de suite - ne s'expriment pas exactement \`a l'aide de fonctions + et * - d'ailleurs, les
seules fonctions qui peuvent \`etre calcul\'ees exactement utilisant seulement ces op\'erations sont les fonctions polynomiales 
\[ P(x)= a_0+a_1 x+a_2x^2+\ldots +a_n x^n\]
ou $x$ est une variable et $a_0,\ldots, a_n$ sont des nombres. \\ \\
Alors, comment faire pour faire calculer $sin(x)$ (par exemple) \`a un ordinateur ? Il y a plusieurs
m\'ethodes : parmi les plus utilis\'es il y a les s\'eries de Taylor. Voil\`a comment cela
se passe dans le cas de la fonction $\sin(x)$.
\begin{enumerate}
\item On applique la th\'eorie de la s\'erie de Taylor pour \'ecrire $\sin(x)$ comme une somme
infinie de puissances de $x$. En l'occurence, nous savons que
\[ \sin(x)= x-\frac{x^3}{3!}+\frac{x^5}{5!}-\frac{x^7}{7!}+\ldots=\sum_{n=0}^\infty \frac{(-1)^n x^{2n+1}}{(2n+1)!}.\]
\item Les sommes partielles $s_k(x)$ de cette s\'eries de Taylor sont des polyn\^omes et peuvent  \^etre calcul\'ees utilisant
seulement les op\'erations d'addition et multiplication. Dans le cas de $\sin(x)$ nous avons que 
\[ s_k(x)= x-\frac{x^3}{3!}+\frac{x^5}{5!}+\ldots +\frac{(-1)^k x^{2k+1}}{(2k+1)!}.\]
\item Pour $k$ assez grand le polyn\^ome $s_k(x)$ sera une approximation acceptable pour $\sin(x)$. {\bf Attention } : ici
le $k$ ``assez grand'' d\'ependra de $x$ : plus $x$ sera petit, plus on pourra utiliser une valeur basse pour $k$. 
\end{enumerate}
La question la plus difficile \`a laquelle il faudrait r\'epondre est la suivante : si je cherche (par exemple)
\`a calculer $\sin(2)$ \`a $10^{-3}$ pr\`es, quelle valeur de $k$ 
dois-je utiliser ? Pour minimiser le
temps de calcul de la machine, il est souhaitable de prendre $k$ le plus petit possible.\\ \\
L'erreur $E_k(x)$ que je commets lorsque j'approche $\sin(x)$ par 
\[s_k(x)= x-\frac{x^3}{3!}+\frac{x^5}{5!}+\ldots +\frac{(-1)^k x^{2k+1}}{(2k+1)!}\]
est donn\'ee par le formule\footnote{La 
 terme g\'en\'erale de la s\'eries $E_k(x)$ a un coefficient $\frac{1}{(2k+1)!}$ qui devient tr\`es petit
tr\`es vite, ce qui fait qu'il est typiquement possible de calculer $\sin(x)$ avec relativement peu de termes.}
\[ E_k(x)= \frac{(-1)^k x^{2k+1}}{(2k+1)!}+\ldots.\]
On peut d\'emontrer que des lors que $x^2< (2k+2)(2k+3)$ nous avons que
\[ |E_k(x)|\leq \frac{|x^{2k+3}|}{(2k+3)!}\]
Pour guarantir que $E_k(x)$ est petit il suffit donc de s'assurer que \[|x|^{2k+3}/|(2k+3)!\] est petit. Ici, comme on voudrait
calculer $\sin(2)$ \`a $10^{-3}$ pr\`es, il suffit de trouver $k$ tel que $2^{2k+3}/(2k+3)!<10^{-3}$. Des calculs nous donnent que
\[ 2^3/3!=4/3\]
\[ 2^5/5!= 4/15\]
\[ 2^7/ 7!= 8/315\]
\[ 2^9/9!= 4/2835\]
\[ 2^1/11!=8/ 155925<10^{-3}.\]
Nous savons donc que les deux expressions 
\[ \sin(2)\]
et
\[ 2- 2^3/3!+2^5/5!-2^7/7!+2^9/9!\]
diff\`erent par moins que $10^{-3}$.
\newpage
\section{Rappels d'alg\`ebre lin\'eaire.}
Nous avons vu dans le chapitre pr\'ecedent comment on peut donner un sens \`a 
une somme infinie de nombres -- mais notre but initial ne concernait pas les
nombres, mais les fonctions. Nous voudrions prendre une fonction $\phi(x)$, d\'efini sur
une intervalle $[0,L]$, et l'\'ecrire comme une somme infini de fonctions trigonom\'etriques, dans l'espoir que
cela nous permettra de r\'esoudre l'\'equation de la chaleur avec condition initiale $\phi$. \\ \\
Notons tout d'abord que la d\'efinition que nous avons donn\'ee d'une somme infinie 
de nombres ne s'applique pas naturellement aux fonctions. En effet, la valeur
d'une somme infinie s'exprime comme une limite d'une suite, et pouvoir parler
de la limite d'une suite on a besoin d'une notion de distance - il faut pouvoir dire
quand deux objets sont ``proches''. Or, si cette notion est facile pour des nombres r\'eels
ou complexes, c'est beaucoup plus d\'elicat de dire quand deux fonctions sont ``proches''
ou d\'efinir une ``distance'' entre deux fonctions. \\ \\
Mettons brevement de c\^ot\'e cette difficult\'e. Si on nous donne une fonction $\phi$ sur une intervalle $[0,L]$, 
comment pourrait-on essayer d'\'ecrire $\phi$ comme une somme infinie de fonctions trigonom\'etriques ? Une
premi\`ere id\'ee pourrait \^etre de calculer cette somme par approximations successives : pour chaque
entier $k$, on pourrait essayer de calculer $S_k(\phi)$, qui serait le ``meilleur approximant'' 
\`a $\phi$ de la forme 
\[ a_0+ a_1\cos(\pi x/L)+a_2\cos(2\pi x/L)+\ldots + a_k\cos(k\pi x/L).\]
Peut \^etre en prenant des valeurs de $k$ de plus en plus grandes, on trouvera des $S_k(\phi)$, sommes trigonom\'etriques
finies, de plus en plus proches \`a $\phi$ ? Peut \^etre lorsque $k$ tend vers $\infty$, les $S_k(\phi)$ convergeront vers
une somme infinie de fonctions trigonom\'etriques dont le r\'esultat est $\phi$ ? \\ \\  
Cette id\'ee  d'\'ecrire $\phi$ comme une somme infinie de fonctions trigonom\'etriques par approximations
 successives  est s\'eduisante, mais pose plus de questions qu'elle ne r\'esout :
\begin{enumerate}
\item Quel sens donner \`a une somme infinie de fonctions ?
\item Qu'est ce que \c ca veut dire, quand on dit 
que deux fonctions sont ``proches'' ?
\item Plus pr\'ecisement, comment quantifier la ``distance'' entre deux 
fonctions ?
\item Comment calculer effectivement cette ``meilleure approximation'' $S_k(\phi)$ ?
\item Qu'est ce que cela signifie quand on dit 
qu'une suite de fonctions converge
vers une autre fonction ?
\end{enumerate}
Nous avons d\'ej\`a commencer, dans le chapitre pr\'ecedent, de r\'epondre \`a la question 1), au moins
dans le cas simple
quie st celui d'une somme infinie de nombres.  
Nous chercherons maintenant \`a comprendre ce que peut vouloir
une ``bonne approximation'' pour des fonctions. En effet, 
le premier probl\`eme que l'on rencontre lorsqu'on essaie de r\'esoudre
ces deux \'equations par une m\'ethode d'approximations successives et celui
de d\'efinir ce qu'on veut dire par une ``bonne approximation'', ou une ``distance'' entre
deux fonctions.\footnote{
Nous serons particuli\`erement attentifs \`a la question de l'approximation d'une fonction 
quelconque par des sommes de fonctions trigonom\'etriques $\sin(n\pi x)$ et
$\cos(n\pi x)$, puisque ces fonctions, qui repr\'esentent math\'ematiquement les ph\'enom\`enes 
physiques {\it ondulatoires}, occupent une place tr\`es importante dans les math\'ematiques au service de la physique.} 
\\ \\Nous  allons en particulier regarder de pr\`es la question suivante :\\ \\
{\bf Supposons donn\'ee sur un intervalle $[0,L]$ une fonction $f$. Comment
faire pour trouver la meilleure approximation pour $f$ de la forme
\[ S_n(f)= a_0+ \sum_{j=1}^n a_j \cos{(j \pi x/L)}+b_j\sin{(j\pi x/L)}\; ?\]}
Si nous ne disposons pas actuellement d'une bonne notion de distance entre des fonctions\footnote{Et encore
moins d'un algorithme permettant de calculer ce ``meilleur approximant'' pour une fonction donn\'ee...}
il existe bien des espaces pour lesquels la notion de distance existe. Ce 
sont les espace g\'eom\'etriques $\rr^2$ et $\rr^3$. De plus, dans ces 
espaces, il existe des algorithmes efficaces 
qui permettent, \'etant donn\'es un point $x$ et un plan ou droite $S$, 
de calculer le point de $S$ le plus proche de $x$.\\ \\
Nous allons nous
baser sur ce que nous savons sur $\rr^2$ et $\rr^3$ pour d\'efinir des 
distances entre 
fonctions (et plein d'autres choses). 
Dans ce chapitre, nous allons \'etudier la notion d'espace
vectoriel, qui r\'eunit (entre autres) les espaces g\'eom\'etriques et les espaces de 
fonctions. 
\subsection{Espaces vectoriels : d\'efinitions et exemples.}
\begin{defn}
Un $\rr$-espace vectoriel est un ensemble $V$ muni d'une loi interne $$V\times V\to V,(x,y)\mapsto x+y,$$ et d'une loi externe $$\rr\times V\to V,(\lambda,x)\mapsto \lambda\cdot x,$$ appel\'{e}e parfois {\it multiplication par un scalaire}, satisfaisant aux propri\'{e}t\'{e}s suivantes: 
\begin{enumerate}

\item Il existe un \'{e}l\'{e}ment $0_V\in V$ tel que $0_V+x=x+0_V=x$ pour tout $x\in V$.

\item $x+(y+z)=(x+y)+z$ pour tout $x,y\in V$

\item $x+y=y+x$ pour tout $x,y\in V$

\item Pour tout $x\in V$, il existe un \'{e}l\'{e}ment $x'\in V$ tel que $x+x'=x'+x=0_V$.

Cet \'{e}l\'{e}ment $x'$ est alors unique, et est not\'{e} $-x$. 

\item $1\cdot x=x$ pour tout $x\in M$

\item $(\lambda\mu)\cdot x=\lambda\cdot(\mu\cdot x)$ pour tout $\lambda,\mu\in \rr,x\in V$

\item $\lambda\cdot (x+y)=\lambda\cdot x+\lambda\cdot y$ pour tout $x,y\in V,\lambda\in \rr$

\item $(\lambda+\mu)\cdot x=\lambda\cdot x+\mu\cdot x$ pour tout $x\in V,\lambda,\mu\in \rr$.
\end{enumerate}
\end{defn}

\begin{rem}
Grosso modo, un $\rr$-espace vectoriel est un ensemble dans lequel :
\begin{itemize}
\item on peut additionner des \'el\'ements,
\item on peut multiplier des \'el\'ements par des scalaires r\'eels,
\item cette addition et cette multiplication satisfont les m\^emes r\`egles 
alg\`ebriques que
l'addition et multiplication habituelle dans $\rr$.
\end{itemize} 
Dans un $\cc$-espace vectoriel il existe une multiplication par des scalaires
complexes.
\end{rem}
\begin{rem} On \'ecrira $\lambda x$ pour $\lambda \dot x$.
\end{rem}
\begin{exs}
\begin{enumerate}
\item $\rr^n$, l'espace de vecteurs colonnes $\underline{X}=\begin{pmatrix}{x_1\\ x_2\\ \vdots \\ x_n}\end{pmatrix}$ avec $x_i\in \rr$,  
est un espace vectoriel r\'eel. L'espace $\cc^n$ de vecteurs colonnes complexes
est un espace vectoriel complexe.
\item $\rr[X]$, l'espace de polyn\^omes r\'eels en une variable $X$, est un 
espace vectoriel r\'eel. De m\^eme, $\cc[Y]$, l'espace de polyn\^omes 
complexes en une variable $Y$ est une espace vectoriel complexe.
\item $\rr_n[X]$,  l'espace de polyn\^omes r\'eels en une variable $X$ de 
degr\'e $\leq n$, est un 
espace vectoriel r\'eel. De m\^eme, $\cc_n[Y]$, l'espace de polyn\^omes 
complexes en une variable $Y$ de degr\'e $\leq n$, 
est une espace vectoriel complexe.
\item ${\rm M}_n(\rr)$, l'espace de matrices $n\times n$ \`a coefficients r\'eels, est un espace vectoriel r\'eel,
\item Pour tout $a<b\in \rr$ l'espace $C^0([a,b],\rr)$ de toutes les fonctions continues r\'eelles sur l'intervalle
$[a,b]$, est un espace vectoriel r\'eel.
\item Pour tout $a<b\in \rr$ et tout entier $i>Ã $ l'espace $C^i([a,b],\cc)$ de toutes les fonctions contin\^ument 
$i$-fois d\'erivables complexes 
sur l'intervalle $[a,b]$, est un espace vectoriel complexe.
\end{enumerate}
\end{exs}
 Dans la pratique, nous travaillerons souvent avec des espaces vectoriels qui sont inclus dans d'autres.
\begin{defn}
Soit $V$ un $\rr$-espace vectoriel. 
Un {\bf sous-espace vectoriel} $W$ de $V$ est un sous-ensemble de $W\subset V$ {\bf non vide}, tel que
\begin{enumerate}
\item pour tout $w_1, w_2\in W$ nous avons que $w_1+w_2\in W$
\item pour tout $w_1\in W$ et $\lambda\in \rr$ nous avons que $\lambda w_1\in W$
\end{enumerate}
\end{defn}
L'ensemble $W$ est alors toujours un espace vectoriel avec l'addition et la multiplication h\'erit\'ees de $V$.
\begin{exe}
Montrer que les sous-ensembles suivants sont tous  des sous-espaces vectoriels.
\begin{enumerate}
\item L'ensemble de tous les $(x,y)\in \cc^2$ tels que $x+y=0$.
\item Un plan d'\'{e}quation $ax+by+cz=0$ ($a,b,c\in\rr)$ dans $\rr^3$.
\item L'ensemble $\{ P\in \rr[X]| P(1)=0\}$ des polyn\^omes qui s'annulent en 1
dans $\rr[X]$.
\item L'ensemble $\{ M\in M_n(\cc)| {}^tM= M\}$ des matrices sym\'etriques dans $M_n(\cc)$. 
\item L'ensemble de toutes les fonctions deux fois d\'erivables $f\in C^2(\rr,\rr)$ 
telles que
$f''= -2f$ dans $C^2(\rr,\rr)$.
\end{enumerate}
\end{exe}
\begin{rem}[Remarque utile.]
Un sous-espace vectoriel $W$ de $V$ contient toujours $0_V$, car si $x\in W$, alors $0\cdot x=0_V\in W$.
\end{rem}
\subsection{Bases et coordonn\'ees.}
Une notion cl\'e dans ce qui suit sera la notion de base, qui permet de 
{\it repr\'esenter} un \'el\'ement d'un espace
vectoriel par un vecteur colonne.
\begin{defn}\label{defbase}
Soit $V$ un espace vectoriel r\'eel. Une famille ordonn\'ee d'\'el\'ements de $V$, ${\bf e}=(e_1, \ldots, e_n)$ est
une base (finie) pour $V$ si pour tout \'el\'ement $v\in V$ il existe des \underline{uniques} scalaires 
$\lambda_1, \lambda_2, \ldots, \lambda_n$ tels que 
\[ v= \lambda_1e_1+\lambda_2e_2+\ldots +\lambda_n e_n.\]
\end{defn}
\begin{defn}
Avec les notations de la d\'efinition \ref{defbase},  
nous dirons que le vecteur colonne 
\[ \begin{pmatrix}{\lambda_1\\ \lambda_2 \\ \vdots \\ \lambda_n}\end{pmatrix}\]
est le vecteur des coordonn\'ees de $v$ dans la base ${\bf e}$.
\end{defn}
\begin{rem}[{\bf Attention !}] Le vecteur de coordonn\'ees de $v$ dans une base 
${\bf e}$ d\'epend autant de la base ${\bf e}$ que du vecteur $v$.
\end{rem} 
\begin{rem}[{\bf Notation}]{\bf  Dans ce qui suit il sera tr\`es important de 
distinguer l'\'el\'ement $v$ dans un espace vectoriel $V$ de dimension finie $n$ (qui peut \^etre un 
vecteur colonne, ou une matrice, ou une fonction, ou un polyn\^ome, ou plein 
d'autres choses) et le vecteur colonne $\underline{V}\in \rr^n$ qui le repr\'esente dans une 
base donn\'ee. }
\\ \\
Pour bien distinguer ces deux objets, nous soulignerons 
syst\'ematiquement les noms des variables qui sont des vecteurs colonnes, 
et ne soulignerons pas ceux qui ne le sont pas.
\end{rem}

\begin{exs}

\trick

\begin{enumerate}

\item Les vecteurs 
$$\left(\begin{array}{c} 1 \cr 0\cr \vdots\cr 0\end{array}\right) ,\cdots,\left(\begin{array}{c} 0 \cr \vdots\cr 0\cr 1\end{array}\right)$$
forment une base de $\rr^n$, appel\'{e}e la {\bf base canonique}.
\\ \\
Si $\left(\begin{array}{c} x_1 \cr x_2\cr \vdots\cr x_n\end{array}\right)$ est un \'el\'ement de $\rr^n$ alors on
peut \'ecrire 
\[ \left(\begin{array}{c} x_1 \cr x_2\cr \vdots\cr x_n\end{array}\right)=
x_1\left(\begin{array}{c} 1 \cr 0\cr \vdots\cr 0\end{array}\right) +\cdots+ x_n\left(\begin{array}{c} 0 \cr \vdots\cr 0\cr 1\end{array}\right) ;\]
autrement dit, le vecteur de coordonn\'ees de $\left(\begin{array}{c} x_1 \cr x_2\cr \vdots\cr x_n\end{array}\right)$
dans la base canonique est $\left(\begin{array}{c} x_1 \cr x_2\cr \vdots\cr x_n\end{array}\right)$. Ceci est une 
source importante de confusion.
\item Montrons que $B=\left(\left(\begin{array}{c} 1\cr 1\end{array}\right), 
\left(\begin{array}{c} 1\cr 2\end{array}\right)\right)$ est une base de $\cc^2$. Nous consid\'erons pour 
un vecteur arbitraire $\left(\begin{array}{c} x\cr y\end{array}\right)$ l'\'equation
\[ \left(\begin{array}{c} x\cr y\end{array}\right)= \lambda_1 \left(\begin{array}{c} 1\cr 1\end{array}\right)+
\lambda_2\left(\begin{array}{c} 1\cr 2\end{array}\right)\]
c'est-\`a-dire
\[ x= \lambda_1+ \lambda_2\]
\[ y= \lambda _1+ 2\lambda_2\]
ce qui (apr\`es pivot de Gauss) nous donne l'unique solution 
\[\lambda_1= 2x-y,\] \[\lambda_2= y-x.\] 
Cette famille est donc une base et
le vecteur de coordonn\'ees de 
$\left(\begin{array}{c} x\cr y\end{array}\right)$ dans la base $B$ est
\[ \left(\begin{array}{c} 2x-y\cr y-x\end{array}\right).\] 
\item La famille $B=(1,X,\ldots,X^n)$ forme une base de l'espace vectoriel $\rr[X]_n$
des polyn\^{o}mes \`{a} coefficients dans $\rr$ de degr\'{e} au plus $n$. Si $P= a_0+a_1X+\ldots a_n X^n$
est un \'el\'ement de $\rr_n[X]$ alors son vecteur  de coefficients dans la base $B$ est
\[ \left(\begin{array}{c} a_0\cr a_1\cr \vdots \cr a_n \end{array}\right).\] 
\item On consid\`ere $M_2(\cc)$, l'espace de matrices carr\'ees complexes 
$2\times 2$. Elle a une base 
\[B=\left(\begin{pmatrix}{1 & 0\\ 0& 0}\end{pmatrix},
 \begin{pmatrix}{0& 1\\ 0& 0}\end{pmatrix},\begin{pmatrix}{0 & 0\\ 1& 0}\end{pmatrix},
\begin{pmatrix}{0 & 0\\ 0& 1}\end{pmatrix}, \right)\]
et dans cette base la matrice $M=\begin{pmatrix}{a & b\\ c& d}\end{pmatrix}$ a 
pour vecteur de coefficients
$\begin{pmatrix}{a \\ b\\ c\\ d}\end{pmatrix}$.
\item On consid\`ere l'espace de fonctions r\'eelles deux fois d\'erivables
sur $\rr$ qui satisfont l'\'equation $f''= -2f$. Vous avez vu dans MAT128
que cette espace est de dimension 2 et 
la famille \[(\cos(\sqrt{2}x), \sin(\sqrt{2} x))\] en est une base. 
Le vecteur de coordonn\'ees de la fonction $f=a\cos(\sqrt{2} x)+
b\sin(\sqrt{2}x)$ dans cette
base est $\begin{pmatrix}{a\\ b}\end{pmatrix}$.
\end{enumerate}

\end{exs}

\begin{defn}
Lorsqu'un espace vectoriel $V$ poss\`ede une base finie on dit que $V$ est de dimension finie.
Toutes les bases de $V$ ont alors le m\^eme nombre d'\'el\'ements (nous admettrons ce th\'eor\`eme) : ce nombre
s'appelle  la {\bf dimension} de $V$.
\end{defn}

\begin{exs}
\begin{enumerate}
\item L'espace $\rr^n$ est de dimension $n$.
\item L'espace $\rr_n[X]$ est de dimension $n+1$. 
\item L'espace $M_2(\rr)$ est de dimension $4$. 
\end{enumerate}
\end{exs}
\begin{rem}
Tout sous-espace d'un espace de dimension finie est de dimension finie.
\end{rem}
Le r\'esultat suivant, que nous rappelons sans d\'emonstration, sera souvent
utilis\'e pour v\'erifier qu'une famille de vecteurs est une base.
\begin{lem}
Soit $V$ un espace vectoriel de dimension $n$ et soit $(e_1,\ldots, e_n)$ une 
famille de $n$ vecteurs dans $V$. Si la famille $(e_1,\ldots, e_n)$ est 
{\it libre} (c'est \`a dire 
que $\sum \lambda_i e_i=0_V\Rightarrow \lambda_i=0$ $\forall i$)
alors elle est une base.
\end{lem} 
Les coordonn\'ees d'un \'el\'ement $v\in V$
dans une base seront essentielles dans la suite, car elles nous permettront
de ramener tous nos calculs \`a 
de simples multiplications de matrices. Il nous sera, d'ailleurs,
souvent utile de simplifier nos calculs au maximum en choississant une base 
bien adapt\'ee. Pour faire cela, il nous 
faut comprendre comment le vecteur $\underline{V}$
des coordonn\'ees d'un \'el\'ement $v\in V$ dans une base ${\bf e}$
se transforme lorsqu'on change
de base.
\begin{defn}
Soit $V$ un espace vectoriel de dimension $n$ et soient ${\bf e}=(e_1, \ldots, e_n)$ et $ {\bf f}=(f_1,\ldots, f_n)$ des
bases de $V$. Soit $\underline{V}_i$ le vecteur de coordonn\'es de $f_i$ dans la base $(e_1,\ldots, e_d)$. Alors, la matrice
de passage de ${\bf e}$ vers ${\bf f}$ est la matrice
\[P=(\underline{V}_1,\ldots, \underline{V}_n).\]
\end{defn}
\begin{rem}[{\bf Attention}]
Il y un cas sp\'ecial quand ${\bf e}$ est la base canonique de 
$\rr^n$ : nous avons alors que la matrice de passage $P$ est donn\'ee par
\[ P= (\underline{f}_1,\ldots, \underline{f}_n).\]
Encore une fois, ce cas sp\'ecial donne lieu \`a beaucoup de confusion.
\end{rem}
L'importance de la matrice de passage vient du th\'eor\`eme fondamental suivant, que nous rappelons :
\begin{thm} 
Soient ${\bf e_1}$ et ${\bf e_2}$ des bases de $V$ et soit $v$ un \'el\'ement de 
$V$. Soient $\underline{V}_1$ et $\underline{V}_2$ les vecteurs de
coordonn\'es de $v$ dans les bases ${\bf e}_1$ et ${\bf e}_2$. Soit $P$ la 
matrice de passage de $B_1$ vers $B_2$. Alors
\[ \underline{V}_1= P \underline{V}_2\]
ou, de fa\c con \'equivalente
\[ \underline{V}_2= P^{-1} \underline{V}_1\]
\end{thm}
\begin{rem} {\bf Attention} le terminologie de ``matrice de passage'' est assez trompeuse ! 
C'est $P^'-1}$, et pas $P$, qui convertit le vecteur qui repr\'esente $v$ dans la base ${\bf e}$ dans 
le vecteur qui repr\'esente $v$ dans la base ${\bf f}$.
\end{rem}
 Il y a une g\'en\'eralisation de la notion de base qui sera utile dans la 
d\'emonstration d'un th\'eor\`eme ult\'erieur.
\begin{defn}
Soient $V_1,\ldots,V_m$ des sous-espaces vectoriels de $V$. 
On dit que $V$ est la {\bf somme directe} des sous-espaces $V_1,\ldots,V_m$, et on \'ecrit 
$V= V_1\oplus V_2\oplus \ldots \oplus V_m$, si et seulement si 
pour tout $v\in V$ il existe des uniques 
\'el\'ements $v_1\in V_1, \ldots, v_m\in V_m$ tels que 
 $$v=v_1+\ldots+v_m.$$
\end{defn}
Le r\'esultat suivant, que nous admettrons, 
sera aussi utile dans un r\'esultat ult\'erieur:
\begin{prop}
Si $V= V_1\oplus V_2\oplus \ldots \oplus V_m$ et pour chaque $i$ nous avons que ${\bf e}_i$ est une base de $V_i$ alors 
la concatentation $({\bf e}_1, {\bf e}_2,\ldots, {\bf e}_m)$ est une base de $V$.
\end{prop}
\subsection{Applications lin\'eaires.}
Consid\'erons maintenant la classe des applications qui pr\'eservent la 
structure d'un espace vectoriel.
\begin{defn}
Soient $V$ et $V'$ deux $\rr$-espaces vectoriels.

Une {\bf application lin\'{e}aire} de  $V$ dans $V'$ est une application $f: V\to V'$  v\'{e}rifiant 

\begin{enumerate}

\item $f(v_1+v_2)=f(v_1)+f(v_2)$ pour tous $v_1,v_2\in V$

\item $f(\lambda v)=\lambda f(v)$ pour tous $\lambda\in \rr,v\in V$
\end{enumerate}
\end{defn}
Dans le cas ou l'espace d'arriv\'ee est $\rr$ on dira que $f$ est une 
{\bf forme lin\'eaire}.
\begin{rem}Pour toute application lin\'eaire $f$ on a n\'{e}cessairement $f(0)=0$.\end{rem}
\begin{exs}
\begin{enumerate}\label{exapp}

\item L'application $\rr^3\rightarrow \rr^2$ donn\'ee par 
$\begin{pmatrix}{x\\y\\z}\end{pmatrix}\mapsto \begin{pmatrix}{x\\y}
\end{pmatrix}$ est lin\'eaire.
\item L'application $\cc^3\rightarrow \cc^2$ donn\'ee par
$\begin{pmatrix}{x\\y\\z}\end{pmatrix}\mapsto \begin{pmatrix}{x\\y+1}
\end{pmatrix}$ n'est pas lin\'eaire.
\item L'application $C^1(\rr, \rr)\mapsto C^0(\rr\rr)$, $f\mapsto f'-2f$
est lin\'eaire.
\item L'application $M_n(\cc) \mapsto M_n(\cc)$ donn\'ee par $M\mapsto
{}^tM$ est lin\'eaire.
\item L'application $\rr_3[X] \mapsto \rr_1[X]$, $P\mapsto P''$, 
est une application lin\'eaire. 
\end{enumerate}
\end{exs}
\begin{exe}
D\'emontrer que les applications 1, 3, 4, 5 de \ref{exapp} sont lin\'eaires et que 2 ne 
l'est pas.
\end{exe}
\begin{defn}
Le {\bf noyau} de $f$, not\'{e} ${\rm Ker}(f)$, est l'ensemble $${\rm Ker}(f)=\{ v\in V \mid f(v)=0\}(\subseteq V).$$
C'est un sous-espace vectoriel de $V$.
\end{defn}

\begin{defn}
{\bf L'image} de $f$, not\'{e}e ${\rm Im}(f)$, est l'ensemble $${\rm Im}(f)=\{ f(v), v\in V\}\subseteq V'.$$
C'est un sous-espace vectoriel de $V'$.
 \end{defn}
 
\begin{exs}
\begin{enumerate}
\item 
Montrer que le noyau et l'image d'une application lin\'eaire sont des sous-espaces vectoriels. 
\item
Pour les applications lin\'eaires donn\'ees dans \ref{exapp}, calculer
leur image et noyau.
\end{enumerate}
\end{exs}     
On rappelle le th\'eor\`eme du rang, dont nous aurons besoin dans une d\'emonstration ult\'erieure.
\begin{thm}
Soit $f:V\rightarrow W$ une application lin\'eaire. On suppose que $V$ est de dimension finie. Alors on a que ${\rm Im}(f)$ est de dimension infinie et

\[ {\rm dim}(V)= {\rm dim}({\rm Ker}(f))+{\rm dim}({\rm Im}(f)).\]
\end{thm}
\subsection{Calcul Matriciel.}
Dans cette section nous ferons des rappels sur les matrices et leurs manipulations. 
Celles-ci seront un \'el\'ement cl\'e de
notre travail ce semestre.
\begin{defn}
Etant donn\'es deux entiers $m$ et $n$ strictement positifs, 
une \emph{matrice \`a $m$ lignes et
$n$ colonnes} est un tableau rectangulaire de r\'eels 
$A=(a_{i,j})$. L'indice de ligne $i$ va de $1$ \`a $m$,
l'indice de colonne $j$ va de $1$ \`a $n$.
$$
A=(a_{i,j})
=
\left(
\begin{array}{ccccc}
a_{1,1}&\cdots&a_{1,j}&\cdots&a_{1,n}\\
\vdots&&\vdots&&\vdots\\
a_{i,1}&\cdots&a_{i,j}&\cdots&a_{i,n}\\
\vdots&&\vdots&&\vdots\\
a_{m,1}&\cdots&a_{m,j}&\cdots&a_{m,n}
\end{array}
\right)
\;.
$$
Les entiers $m$ et $n$ sont les \emph{dimensions} de la matrice, $a_{i,j}$
est son \emph{coefficient d'ordre $(i,j)$}.
\end{defn}
Notons qu'une matrice $A$ peut \^etre pr\'ecis\'ee en donnant une expression 
pour ses coefficients $a_{i,j}$ Par exemple, la matrice $A$ de taille $2\times 2$
donn\'ee par le formule $a_{i,j}= i+j$ est la matrice
\[A= \begin{pmatrix}{ 1+1 & 1+2 \\ 2+1 & 2+2}\end{pmatrix}=  \begin{pmatrix}{ 2 & 3 \\ 3 & 4}\end{pmatrix}.\]

L'ensemble des matrices \`a $m$ lignes et $n$ colonnes et \`a
coefficients r\'eels est not\'e
${\cal M}_{m,n}(\rr)$. Ce qui suit s'applique aussi, si on remplace
$\rr$ par $\cc$, \`a l'ensemble des matrices \`a coefficients complexes. \\ \\
Notons trois cas sp\'eciaux :
\begin{enumerate}
\item Un vecteur colonne \`a $n$ \'el\'ements 
$\begin{pmatrix}{x_1\\x_2\\ \vdots \\ x_n}\end{pmatrix}$ est une matrice 
$n\times 1$.
\item Un vecteur ligne \`a $n$ \'el\'ements $\begin{pmatrix}{x_1,&x_2,&\ldots, 
&x_n}\end{pmatrix}$ est une matrice $1\times n$.
\item Un nombre r\'eel $x$ est une matrice $1\times 1$.
\end{enumerate}
Du point de vue du calcul matriciel - en particulier lorsqu'il s'agit de faire 
des multiplications - un vecteur ligne ne se comporte pas comme un vecteur 
colonne. Nous ferons cette distinction en consid\'erant, par exemple,
que les vecteurs
\[ \begin{pmatrix}{1&2&3}\end{pmatrix}\mbox{ et }\begin{pmatrix}{1\\ 2\\ 3}\end{pmatrix}\]
sont diff\'erents, m\^eme s'ils contiennent les m\^emes nombres dans le m\^eme ordre. 
\\ \\
{\bf Notation.} Si $\underline{X}$ est un vecteur colonne \`a $n$ \'el\'ements,
on notera le coefficient $\underline{X}_{1,i}$ par $\underline{X}_i$. \\ \\ 
L'ensemble ${\cal M}_{m,n}(\rr)$ est naturellement muni d'une addition
 (on peut ajouter deux matrices de m\^emes dimensions terme \`a
terme) et de multiplication par des scalaires (on peut multiplier une matrice
par un r\'eel terme \`a terme). 
\begin{enumerate}
\item[$\bullet$]
\emph{Addition~:}
Si $A=(a_{i,j})$ et $B=(b_{i,j})$ sont deux matrices de ${\cal
M}_{m,n}(\rr)$, leur somme $A+B$ est la matrice $(a_{i,j}+b_{i,j})$. Par
exemple~:
$$
\left(
\begin{array}{rr}
1&1\\
2&3\\
1&-1
\end{array}
\right)
+
\left(
\begin{array}{rr}
-3&1\\
5&-3\\
0&2
\end{array}
\right)
=
\left(
\begin{array}{rr}
-2&\hspace{3mm}2\\
7&0\\
1&1
\end{array}
\right)
$$
\item[$\bullet$]
\emph{Multiplication par un scalaire~:}
Si $A=(a_{i,j})$ est une matrice de ${\cal
M}_{m,n}(\rr)$, et $\lambda$ est un r\'eel, le produit $\lambda A$ est
la matrice $(\lambda a_{i,j})$.
Par exemple~:
$$
-2\,
\left(
\begin{array}{rr}
1&1\\
2&3\\
1&-1
\end{array}
\right)
=
\left(
\begin{array}{rr}
-2&-2\\
-4&-6\\
-2&2
\end{array}
\right)
$$
\end{enumerate}
Observons que ces op\'erations auraient le m\^eme effet si les
matrices \'etaient dispos\'ees comme des $mn$-uplets de r\'eels
(toutes les lignes \'etant concat\'en\'ees, par exemple)
\\ Les matrices de taille $m\times n$ peuvent agir sur des vecteurs colonnes
de taille $n$ pour produire un vecteur de taille $m$, par la formule suivante :
\[ (M\underline{X})_j= \sum_{i=1}^n M_{j,i} \underline{X}_i.\]
\vskip 2mm\noindent
L'op\'eration la plus importante est le \emph{produit matriciel}.
\begin{defn}
\label{def:prodmatrices}
Soient $m,n,p$ trois entiers strictement positifs.
Soit $A=(a_{i,j})$ une matrice de ${\cal M}_{m,n}(\rr)$ et soit
$B=(b_{j,k})$ une matrice de ${\cal M}_{n,p}(\rr)$. On appelle
\emph{produit matriciel} de $A$ par $B$ la matrice 
$C\in {\cal M}_{m,p}(\rr)$ dont le terme g\'en\'eral $c_{i,k}$ est
d\'efini, pour tout $i=1,\ldots,m$ et pour tout $k\in 1,\ldots,p$
par~:
$$
c_{i,k} = \sum_{j=1}^n a_{i,j}\,b_{j,k}\;.
$$ 
\end{defn}
Nous insistons sur le fait que le produit $AB$ de deux matrices n'est
d\'efini que si le nombre de colonnes de $A$ et le nombre de
lignes de $B$ sont les m\^emes. 
Dans le cas particulier ou $B$ est un vecteur colonne de taille $n\times 1$ 
cette op\'eration nous fournit un vecteur colonne de taille $m\times 1$.
$$
\begin{array}{cc}
&
\left(
\begin{array}{ccccc}
b_{1,1}&\cdots&b_{1,k}&\cdots&b_{1,n}\\
\vdots&&\vdots&&\vdots\\
&\cdots&b_{j,k}&\cdots&\\
\vdots&&\vdots&&\vdots\\
b_{n,1}&\cdots&b_{n,k}&\cdots&b_{n,p}
\end{array}
\right)
\\
\left(
\begin{array}{ccccc}
a_{1,1}&\cdots&&\cdots&a_{1,n}\\
\vdots&&\vdots&&\vdots\\
a_{i,1}&\cdots&a_{i,j}&\cdots&a_{i,n}\\
\vdots&&\vdots&&\vdots\\
a_{m,1}&\cdots&&\cdots&a_{m,n}
\end{array}
\right)
&
\left(
\begin{array}{ccccc}
c_{1,1}&&\vdots&&c_{1,p}\\
&&\vdots&&\\
\cdots&\cdots&c_{i,k}&\hspace{5mm}&\\
&&&&\\
c_{m,1}&&&&c_{m,p}
\end{array}
\right)
\end{array}
$$
Posons par exemple~:
$$
A=
\left(
\begin{array}{rr}
1&1\\
2&3\\
1&-1
\end{array}
\right)
\quad\mbox{et}\quad
B=
\left(
\begin{array}{rrrr}
0&1&-1&-2\\
-3&-2&0&1
\end{array}
\right)\;.
$$
La matrice $A$ a 3 lignes et 2 colonnes, la matrice $B$ a 2 lignes et
4 colonnes. Le produit $AB$ a donc un sens~: c'est une matrice \`a 3
lignes et 4 colonnes.
$$
\begin{array}{ll}
&\left(
\begin{array}{rrrr}
0&1&-1&-2\\
-3&-2&0&1
\end{array}
\right)
\\[2ex]
\left(
\begin{array}{rr}
1&1\\
2&3\\
1&-1
\end{array}
\right)
&
\left(
\begin{array}{rrrr}
-3&-1&-1&-1\\
-9&-4&-2&-1\\
3&3&-1&-3
\end{array}
\right)
\end{array}
$$
Le produit matriciel a toutes les propri\'et\'es que l'on attend d'un
produit, sauf qu'il n'est pas commutatif.
\begin{prop}
\label{prop:prodmat}
Le produit matriciel poss\`ede les propri\'et\'es suivantes.
\begin{enumerate}
\item \emph{Associativit\'e~:} Si les produits $AB$ et $BC$ sont
d\'efinis, alors les produits $A(BC)$ et $(AB)C$ le sont aussi et ils
sont \'egaux.
$$
A(BC)=(AB)C\;.
$$
\item \emph{Lin\'earit\'e \`a droite~:} Si $B$ et $C$ sont deux matrices de
m\^emes dimensions, si $\lambda$ et $\mu$ sont deux r\'eels et si $A$
a autant de colonnes que $B$ et $C$ ont de lignes, alors
$$
A(\lambda B+\mu C) = \lambda AB+\mu AC\;.
$$
\item \emph{Lin\'earit\'e \`a gauche~:} Si $A$ et $B$ sont deux matrices de
m\^emes dimensions, si $\lambda$ et $\mu$ sont deux r\'eels et si $C$
a autant de lignes que $A$ et $B$ ont de colonnes, alors
$$
(\lambda A+\mu B)C = \lambda AC+\mu BC\;.
$$
\end{enumerate}
\end{prop}
Ces propri\'et\'es se d\'emontrent \`a partir de la d\'efinition
\ref{def:prodmatrices}. 
\vskip 2mm\noindent
La transposition est une notion importante, dont la justification
provient de la dualit\'e, qui d\'epasse le cadre de ce cours.
\begin{defn}
\label{def:transposee}
\'Etant donn\'ee une matrice $A=(a_{i,j})$ de ${\cal M}_{m,n}(\rr)$, sa
\emph{transpos\'ee} est la matrice de  ${\cal M}_{n,m}(\rr)$ dont le
coefficient d'ordre $(j,i)$ est $a_{i,j}$.
\end{defn}
Pour \'ecrire la transpos\'ee d'une matrice, il suffit de transformer
ses lignes en colonnes. Par exemple~:
$$
A=
\left(
\begin{array}{rr}
1&1\\
2&3\\
1&-1
\end{array}
\right)
\quad,\quad
{^t\!A}=
\left(
\begin{array}{rrr}
1&\hspace{3mm}2&1\\
1&\hspace{3mm}3&-1
\end{array}
\right)\;.
$$
Observons que la transpos\'ee de la transpos\'ee est la matrice
initiale.
$$
{^t({^t\!A})} = A\;.
$$
La transpos\'ee d'un produit est le produit des transpos\'ees, mais il
faut inverser l'ordre des facteurs.
\begin{prop}
\label{prop:produittransposee}
Soient $m,n,p$ trois entiers strictement positifs. 
Soient $A=(a_{i,j})$ une matrice de ${\cal M}_{m,n}(\rr)$ et
$B=(b_{j,k})$ une matrice de ${\cal M}_{n,p}(\rr)$. La transpos\'ee du
produit de $A$ par $B$ est le produit de la transpos\'ee de $B$ par la
transpos\'ee de $A$.
$$
{^t(AB)} = {^t\!B}\,{^t\!A}\;.
$$
\end{prop}
Par exemple, en reprenant les matrices $A$ et $B$ d\'efinies
ci-dessus~:
$$
\begin{array}{rr}
&
\left(
\begin{array}{rrr}
\;1&\quad2&1\\
\; 1&\quad 3&-1
\end{array}
\right)
\\[2ex]
\left(
\begin{array}{rr}
0&-3\\
1&-2\\
-1&0\\
-2&1
\end{array}
\right)
&
\left(
\begin{array}{rrr}
-3&-9&3\\
-1&-4&3\\
-1&-2&-1\\
-1&-1&-3
\end{array}
\right)
\end{array}
$$
\begin{defi}
\label{def:symetrique}
Soit $n$ un entier strictement positif
et $A$ une matrice carr\'ee \`a $n$ lignes et $n$ 
colonnes. On dit que $A$ est sym\'etrique si pour tous
$i,j=1,\ldots,n$, ses coefficients d'ordre $a_{i,j}$ et $a_{j,i}$ sont
\'egaux, ce qui est \'equivalent \`a dire que $A$ est \'egale \`a sa
transpos\'ee.
\end{defi}
Le produit d'une matrice par sa transpos\'ee est toujours
une matrice sym\'etrique. En effet~:
$$
{^t(A\,{^t\!A})} = {^t({^t\!A})}\,{^t\!A}=A\,{^t\!A}\;.
$$
%
\subsection{Matrices carr\'ees}
%
En g\'en\'eral si le produit $AB$ est d\'efini, le produit $BA$ n'a
aucune raison de l'\^etre. Le produit d'une matrice par sa
transpos\'ee  est une exception, les
matrices carr\'ees en sont une autre~: si $A$ et $B$ sont deux
matrices \`a $n$ lignes et $n$ colonnes, les produits $AB$ et $BA$ 
sont tous deux d\'efinis et ils ont les m\^emes dimensions que $A$ et
$B$. En g\'en\'eral ils ne sont pas \'egaux. Par exemple,
$$
\begin{array}{rr}
&
\left(
\begin{array}{rr}
0&-1\\
1&0
\end{array}
\right)
\\[2ex]
\left(
\begin{array}{rr}
0&1\\
1&0
\end{array}
\right)
&
\left(
\begin{array}{rr}
1&0\\
0&-1
\end{array}
\right)
\end{array}
\qquad
\begin{array}{rr}
&
\left(
\begin{array}{rr}
0&1\\
1&0
\end{array}
\right)
\\[2ex]
\left(
\begin{array}{rr}
0&-1\\
1&0
\end{array}
\right)
&
\left(
\begin{array}{rr}
-1&0\\
0&1
\end{array}
\right)
\end{array}
$$
Nous noterons simplement ${\cal M}_n(\rr)$ l'ensemble ${\cal M}_{n,n}(\rr)$
des matrices carr\'ees \`a $n$ lignes et $n$ colonnes, \`a
coefficients r\'eels. Parmi elles la \emph{matrice identit\'e},
not\'ee $I_n$, joue un r\^ole particulier.
$$
I_n=
\left(
\begin{array}{ccccc}
1&0&\cdots&\cdots&0\\
0&1&\ddots&&\vdots\\
\vdots&\ddots&\ddots&\ddots&\vdots\\
\vdots&&\ddots&1&0\\
0&\cdots&\cdots&0&1
\end{array}
\right)
$$
En effet, elle est l'\'el\'ement neutre du produit matriciel~:
pour toute matrice $A\in{\cal M}_{n,m}(\rr)$,
$$
A\,I_n = I_m\,A = A\;.
$$
On le v\'erifie facilement \`a partir de la d\'efinition
\ref{def:prodmatrices}.
\begin{defn}
\label{def:inversematrice}
Soit $A$ une matrice de ${\cal M}_n$. On dit que $A$ est inversible
s'il existe une matrice de ${\cal M}_n$, not\'ee $A^{-1}$, telle que
$$
A\,A^{-1} = A^{-1}\,A = I_n\;.
$$
\end{defn}
Par exemple~:
$$
\left(
\begin{array}{rrr}
1&0&-1\\
1&-1&0\\
1&-1&1
\end{array}
\right)
\left(
\begin{array}{rrr}
1&-1&1\\
1&-2&1\\
0&-1&1
\end{array}
\right)
=
\left(
\begin{array}{rrr}
1&-1&1\\
1&-2&1\\
0&-1&1
\end{array}
\right)
\left(
\begin{array}{rrr}
1&0&-1\\
1&-1&0\\
1&-1&1
\end{array}
\right)
=
\left(
\begin{array}{rrr}
1&0&0\\
0&1&0\\
0&0&1
\end{array}
\right)
$$
Observons que l'inverse, s'il existe, est n\'ecessairement
unique. En effet, soient $B_1$  et $B_2$ deux matrices telles que
$A\,B_1=B_1\,A=I_n$ et $A\,B_2=B_2\,A=I_n$. 
En utilisant l'associativit\'e,
le produit $B_1\,A\,B_2$ vaut $B_1\,(A\,B_2)=B_1\,I_n=B_1$,
mais aussi $(B_1\,A)\,B_2=I_n\,B_2=B_2$. Donc $B_1=B_2$.

Nous rappelons 
la proposition suivante, qui nous dit qu'il suffit de trouver une matrice 
$B$ telle
que $A\,B=I_n$ pour \^etre s\^ur que $A$ est
inversible et que son inverse est $B$.
\begin{prop}
\label{th:invunique}
Soit $A$ une matrice de ${\cal M}_n$. Supposons qu'il existe une
matrice $B$ telle que $A\,B=I_n$ \emph{ou bien} $B\,A=I_n$. Alors $A$ est
inversible et $B=A^{-1}$.
\end{prop}
Si $A$ et $B$ sont deux matrices inversibles de ${\cal M}_n$, leur
produit est inversible.
\begin{prop}
\label{prop:produitinversible}
Soient $A$ et $B$ deux matrices inversibles de ${\cal M}_n(\rr)$. Le
produit $AB$ est inversible et son inverse est $B^{-1}A^{-1}$.
\end{prop}
\begin{proof}
Nous utilisons le th\'eor\`eme \ref{th:invunique}, ainsi que
l'associativit\'e du produit~:
$$
(B^{-1}A^{-1})(AB)=B^{-1}(A^{-1}A)B=B^{-1}I_nB=B^{-1}B=I_n\;.
$$
\end{proof}
Enfin, nous aurons parfois besoin du lemme suivant:
\begin{lem}\label{multzero}
Soit $M\in M_n(\rr)$ une matrice carr\'ee $n\times n$. Si pour tout $\underline{X}, \underline Y}\in \rr^n$
nous avons que ${}^t\underline{X} M \underline{Y}=0$ alors $M=0$.
\end{lem}
\begin{proof}
Soit pour tout $i$ le vecteur colonne $\underline{e}_i\in \rr^n$ d\'efini par
\[ (\underline{e}_i)_j= 1\mbox{ si } i=j,\; 0\mbox{ si } i\neq j.\]
Alors pour tout $1\leq i,j\leq n$ on a que \[{}^t\underline{e}_i 
M \underline{e}_j=M_{i,j}=0\] et donc
$M=0$.
\end{proof}  
R\'ecrivons maintenant notre probl\`eme initial dans le language des espaces 
vectoriels. Nous consid\'erons une fonction r\'eelle continue $f$, d\'efinie
sur une intervalle 
$[0,L]$. Autrement dit, $f$ est un \'el\'ement de l'espace vectoriel r\'eel
$C^0([0,L], \rr)=V$.
Nous voulons chercher une fonction $g_n$ qui est de la forme
\[ g_n(x)= a_0+ \sum_{k=1}^n a_k \cos\left(\frac{ k\pi x}{L}\right)+ b_k\sin\left(\frac{k\pi x}{L}\right)\]
et qui doit \^etre ``aussi proche que possible'' de $f$. \\ \\
Dans le langage des espaces vectoriels on pourrait \'ecrire la chose suivante :\\ \\
Soit $W$ le sous-espace de tous les \'el\'ements $g\inV$
qui peuvent s'\'ecrire sous la forme
\[ g_n(x)= a_0+ \sum_{k=1}^n a_k \cos\left(\frac{ k\pi x}{L}\right)+ b_k\sin\left(\frac{k\pi x}{L}\right).\]
$W$ est alors un sous-espace 
vectoriel de $V$ ({\bf exercice} : d\'emontrez-le !): 
de plus, $W$ est de dimension
finie et admet pour base finie la famille 
\[ {\bf e}=( 1, \cos{\pi x/L},\sin{\pi x/L},
\ldots, \cos{ n\pi x/L}, \sin{n\pi x/L}).\] Nous cherchons \`a identifier un
\'el\'ement $g\in W$ qui est ``le plus proche que possible'' de $f\in V$.\\ \\
Notre probl\`eme initial est donc un exemple particulier du probl\`eme suivant :\\ \\
{\bf Question.} J'ai un espace vectoriel $V$  et un \'el\'ement $v\in V$.
Il y a dans $V$ un sous-espace sp\'ecial de dimension finie $W\subset V$. Je veux
approcher au mieux $v$ par un \'el\'ement $w\in W$. Comment faire ? Et tout
d'abord, qu'est ce que \c ca veut dire ``approcher au mieux'' ?
\\ \\
Dans les deux prochains chapitres, nous aborderons surtout la question : 
qu'est ce
que \c ca veut dire ``approcher au mieux'' ?
\newpage
\section{Formes bilin\'{e}aires.}

\subsection{Le produit scalaire canonique sur $\rr^3$.}
Dans le chapitre pr\'ecedent, nous avons \'etudi\'e la notion d'espace vectoriel. Cette notion est utile parce qu'elle
englobe \`a la fois des espaces g\'eom\'etriques tels que $\rr^2$ et $\rr^3$ et 
des espaces de fonctions tels que $\rr_n[X]$ et
$C^0([0,1], \rr)$. Notre but est maintenant d'utiliser cette notion pour \'etendre des id\'ees g\'eom\'etriques (distance et
angle, par exemple) \`a des espaces de fonctions. Pour faire cela, 
il nous sera n\'ecessaire d'identifier une formule purement alg\`ebrique qui permet de calculer distances et angles dans $\rr^3$.
\\ \\
Ceci sera possible utilisant le produit scalaire canonique sur $\rr^3$.
\begin{defn}
Le produit scalaire canonique sur $\rr^3$ est la fonction de deux vecteurs $\underline{X}=\begin{pmatrix}
{x_1\\x_2\\x_3}\end{pmatrix}$ et $\underline{Y}=\begin{pmatrix}
{y_1\\y_2\\y_3}\end{pmatrix}$
\[ \underline{X}, \underline{Y} \rightarrow \langle \underline{X}, \underline{Y}\rangle\]
donn\'ee par $\langle \underline{X}, \underline {Y}\rangle= x_1y_1+x_2y_2+x_3y_3$. 
\end{defn}
Le produit scalaire canonique est donc une fonction de deux vecteurs donn\'ee par une formule simple sur les coordonn\'ees 
des vecteurs. Il tire son int\'er\^et du fait qu'il encode la g\'eom\'etrie de l'espace $\rr^3$.
\begin{theorem}
Soient $\underline{X}$ et $\underline{Y}$ deux vecteurs dans $\rr^3$, soit $\theta$ l'angle entre ces deux vecteurs 
et soit $d$ la distance entre elles : 
nous avons alors que
\[ d= (\langle \underline{X}-\underline{Y}, \underline{X}-\underline{Y}\rangle)^{1/2}, \; \theta=
\arccos\left(\frac{\langle\underline{X},\underline{Y}\rangle}{(\langle\underline{X},\underline{X}\rangle,\langle\underline{Y},\underline{Y}\rangle)^{1/2}}\right).\]
\end{theorem}
{\bf Il existe donc une formule qui permet de calculer la distance et l'angle entre deux vecteurs utilisant seulement le  produit 
scalaire.} Nous allons donc essayer de d\'efinir des classes 
de fonctions sur des espaces vectoriels qui ressemblent au produit scalaire sur $\rr^3$ dans l'espoir qu'elles nous 
livront une bonne notion de ``distance''. 

Une des propri\'et\'es cl\'es du produit scalaire est qu'il se comporte effectivement comme un produit sous les
op\'erations alg\`ebriques de base sur les vecteurs,
c'est-\`a-dire qu'on a, pour tout $\underline{X}, \underline{Y},\underline{Z}\in \rr^3$ et pour tout $\lambda\in \rr$
\begin{enumerate}
\item $\langle\underline{X}+\underline{Y}, \underline{Z} \rangle=\langle\underline{X},\underline{Z}\rangle+\langle\underline{Y},\underline{Z}\rangle$
\item $\langle\underline{X},\underline{Y}+\underline{Z}\rangle=\langle\underline{X},\underline{Y}\rangle+\langle\underline{X},\underline{Z}\rangle$
\item $\langle\underline{X},\lambda \underline{Y}\rangle=\langle\lambda \underline{X},\underline{Y}\rangle=
\lambda \langle\underline{X},\underline{Y}\rangle$
\end{enumerate}

Nous allons donc commencer par \'etudier les fonctions de deux vecteurs qui respectent ces conditions.

\subsection{Formes bilin\'eaires : d\'efinitions et exemples.}
Dans cette section, de nouveau, nous pr\'esenterons la th\'eorie des formes bilin\'eaires r\'eelles, 
mais tous nos r\'esultats seront valables pour des formes complexes. 
\begin{defn} 
Soient $V$ et $V'$ deux $\rr$-espaces vectoriels, et soit $\varphi:V\times V'\to \rr$ une application de $V\times V'$ 
dans 
$\rr$.

On dit que $\varphi :V\times V'\to \rr$ est {\bf une forme bilin\'{e}aire} si : 
\begin{enumerate}
\item pour tout $v_1, v_2\in V$ et $v'\in V'$ nous avons que $\varphi(v_1+v_2, v')= \varphi(v_1, v)+\varphi(v_2,v')$
\item pour tout $v\in V$ et $v_1', v_2'\in V'$ nous avons que $\varphi(v, v_1'+v_2')= \varphi(v, v_1')+\varphi(v,v_2')$
\item pour tout $v\in V$, $v'\in V'$ et $\lambda\in \rr$ nous avons que
$\varphi(\lambda v, v')= \varphi(v, \lambda v') =\lambda \varphi(v,v')$.
\end{enumerate}
Dans le cas ou $V=V'$, on dit que $\varphi:V\times V\to \rr$ est {\bf sym\'{e}trique} si 
$\varphi(y,x)=\varphi(x,y)$ pour tout $x,y\in V$, et on dit que $\varphi:V\times V\to \rr$ est 
{\bf antisym\'{e}trique} si 
$\varphi(y,x)=-\varphi(x,y)$ pour tout $x,y\in V$. 
\end{defn}

\begin{exs}
\trick

$(1)$ L'application $$\varphi:\rr\times \rr\to \rr, (x,y)\mapsto xy$$
est une forme bilin\'{e}aire sym\'{e}trique.

$(2)$ Le produit scalaire  $$\varphi:\rr^n \times \rr^n\to \rr, \left(\left(\begin{array}{c}x_1 \cr \vdots \cr x_n
\end{array}\right),\left(\begin{array}{c}y_1 \cr \vdots \cr y_n\end{array}\right)\right)\mapsto x\cdot y=\sum_{i=1}^n x_iy_i$$
est une forme bilin\'{e}aire sym\'{e}trique. Lorsque $n=2$ ou $3$, on retrouve le produit scalaire 
\'etudi\'e ci-dessus. Nous appelons cette forme le {\it produit scalaire canonique} sur $\rr^n$.

$(3)$ L'application $$\varphi:\cc[X]\times\cc[X]\to\cc, (P,Q)\mapsto P(0)Q(1)$$ est une forme bilin\'{e}aire 
ni sym\'{e}trique ni antisym\'etrique.


$(4)$ L'application $$\varphi: \M_n(\rr)\times \M_n(\rr)\to \rr, (M,N)\mapsto {\rm tr}(MN)$$ 
est une forme bilin\'{e}aire sym\'{e}trique.

$(5)$ L'application $$\varphi:\cc^2\times \cc^2\to \cc, 
\left(\left(\begin{array}{c}x_1 \cr x_2\end{array}\right\right), 
\left(\begin{array}{c}y_1 \cr y_2\end{array}\right))\mapsto x_1x_2+2x_1y_2$$
n'est pas bilin\'{e}aire.

En effet, posons $\underline{U}=\left(\begin{array}{c}x_1 \cr x_2\end{array}\right),
\underline{V}=\left(\begin{array}{c}y_1 \cr y_2\end{array}\right)$. 
On a $$\varphi(\lambda \underline{U}, \underline{V})=(\lambda x_1)(\lambda x_2)+2(\lambda x_1)y_2=
\lambda^2 x_1x_2+2\lambda x_1y_2\neq \lambda \varphi(\underline{U},\underline{V}).$$

$(6)$ L'application $$\varphi: 
C^0([0,1], \rr)\times C^0([0,1], \rr) \rightarrow \rr, f,g \rightarrow \int_0^1 f(x) g(x) dx
$$ est une forme bilin\'eaire sym\'etrique.

$(7)$ Pour toute fonction continue $p:[0,1]\rightarrow\rr$,
l'application $$\varphi: 
C^0([0,1], \rr)\times C^0([0,1], \rr) \rightarrow \rr, f,g \rightarrow \int_0^1 p(x) f(x) g(x) dx
$$ est une forme bilin\'eaire sym\'etrique.
\end{exs}

Un cas particulier int\'eressant est celui ou on applique une forme bilin\'eaire \`a deux vecteurs identiques.

\begin{defn}
Soit $V$ un espace vectoriel sur $\rr$ et soit $\varphi: V\times V\rightarrow \rr$ une forme bilin\'eaire sym\'etrique. Alors la forme quadratique associ\'ee \`a $\varphi$, not\'ee $q_\varphi$, est la fonction
\[ q_\varphi: V\rightarrow \rr,\;  q_\varphi(v) =\varphi(v,v).\]
\end{defn}
La forme quadratique associ\'ee \`a une forme bilin\'eaire est un analogue de la fonction carr\'ee d'un nombre r\'eel, ou de $|v|^2$ quand $v$ est un vecteur 
dans $\rr^2$ ou $\rr^3$. 
Les formules suivants (dit ``formule de polarisation'' et ``formule du 
parall\`elogramme'') permettent entre autres de r\'ecuperer une forme bilin\'eaire sym\'etrique \`a partir de sa forme quadratique.
\begin{lem}
Soit $V$ un espace vectoriel, $\varphi$ une forme bilin\'eaire sur 
$V\times V$ et $q_\varphi$ la forme quadratique associ\'ee. Alors pour tout
$v,w\in V$ on a que
\[\varphi(v,w)= \frac{1}{2}(q_\varphi(v+w)-q_\varphi(v)-q_\varphi(w))\]
\[q_\varphi(v+w)+q_\varphi(v-w)= 2(q_\varphi(v)+q_\varphi(w)).\]
\end{lem}
\begin{proof}
La d\'emonstration de ce lemme est laiss\'ee en exercice. 
\end{proof}
\begin{rem}
Ces formules sont les g\'en\'eralisations des r\'elations suivantes sur $\rr$ :
\[ xy= \frac{1}{2}((x+y)^2-x^2-y^2).\]
\[ (x+y)^2+ (x-y)^2= 2(x^2+y^2).\]
\end{rem}
\subsection{Formes bilin\'eaires : repr\'esentation matricielle.}
Nous allons maintenant d\'efinir la matrice d'une forme bilin\'eaire dans une base, qui va nous permettre,
modulo le choix d'une base, de r\'eduire le calcul des formes bilin\'eaires sur
des espaces de dimension finie \`a des multiplications de matrices.
\begin{defn}
Soit $V$ un $\rr$-espace vectoriel de dimension finie $n$, soit ${\bf e}=(e_1,\ldots,e_n)$ une base de $V$, et soit $\varphi:V\times V\to \rr$ une forme bilin\'{e}aire. 
La {\bf matrice repr\'{e}sentative} de $\varphi$ dans la base {\bf e} est la matrice $n\times n$,
$M$, dont les coefficients sont 
donn\'es par
$$M_{i,j}=(\varphi(e_i,e_j))_{1\leq i,j\leq n}.$$
\end{defn}

\begin{lem}\label{lembilrep}
Soit $V$ un espace vectoriel de dimension finie $n$. Soient $x,y\in V$, et soit ${\bf e}=(e_1,\ldots,e_n)$ une base de $V$. Finalement, soient 
$\underline{X}=\begin{pmatrix}{x_1\\\vdots\\ x_n}\end{pmatrix}$ et $Y= \begin{pmatrix}{y_1\\\vdots\\ y_n}\end{pmatrix}$ les vecteurs coordonn\'{e}es de $x$ et $y$ dans la base {\bf e} 
(autrement dit $x=\ds\sum_{i=1}^n x_i e_i, y=\sum_{i=1}^ny_i e_i$). 
Soit $\varphi:V\times V\to \rr$ une forme bilin\'{e}aire, et soit $M$ la matrice qui repr\'esente $\varphi$ dans la base ${\bf e}$. 
Alors on a $$\varphi(x,y)={}^t\underline{X}M\underline{Y}=\sum_{i,j}\varphi(e_i,e_j)x_iy_j.$$
\end{lem}

\begin{proof}
On a $$\varphi(x,y)=\varphi(\sum_{i=1}^n x_i e_i,\sum_{j=1}^n y_j e_j)=\sum_{j=1}\varphi(\sum_{i=1}^n x_i e_i,y_j e_j)=\sum_{j=1}y_j 
\varphi(\sum_{i=1}^n x_i e_i,e_j),$$
puisque $\varphi$ est lin\'{e}aire en $y$.
Or on a aussi $$\varphi(\sum_{i=1}^n x_i e_i,e_j)=\sum_{i=1}^n \varphi(x_i e_i,e_j)=\sum_{i=1}^n x_i\varphi(e_i,e_j).$$
Ainsi, on obtient $$\varphi(x,y)=\sum_{j=1}^n y_j(\sum_{i=1}^n x_i\varphi(e_i,e_j))=\sum_{i,j} \varphi(e_i,e_j)x_iy_j.$$

On a aussi $$M\underline{Y}=\left(\begin{array}{c}\vdots \cr \ds\sum_{j=1}^n \varphi(e_i,e_j)y_j\cr \vdots \end{array}\right),$$
et donc $${}^t\underline{X}B\underline{Y}=\left(\begin{array}{ccc}\cdots & x_i& \cdots \end{array}\right)\left(\begin{array}{c}\vdots \cr \ds \sum_{j=1}^n \varphi(e_i,e_j)y_j\cr \vdots \end{array}\right)=\sum_{i,j} x_i\varphi(e_i,e_j)y_j=\sum_{i,j} \varphi(e_i,e_j)x_iy_j.$$
\end{proof}

\begin{coro}\label{corosym}
Soit $V$ un espace vectoriel de dimension finie $n$.
Soit $\varphi:V\times V\to \rr$ une forme bilin\'{e}aire. Les propositions 
suivantes sont \'{e}quivalentes.

\begin{enumerate}

\item $\varphi$ est sym\'{e}trique

\item Pour tout base {\bf e} de $V$, la matrice $M$ de $\varphi$ dans la base {\bf e} est sym\'etrique.

\item Il existe une base {\bf e} de $V$ telle que la matrice $M$ de $\varphi$ dans la base {\bf e} est sym\'{e}trique.

\end{enumerate}
\end{coro}
\begin{proof}
Soit $\varphi:V\times V\to \rr$ une forme bilin\'eaire, et soit {\bf e} une base de $V$.

Si $\varphi$ est sym\'etrique, alors on a $$\varphi(e_i,e_j)=\varphi(e_j,e_i)\mbox{ pour tout }i,j,$$
et ceci s'\'{e}crit matriciellement ${}^tM=M$, par d\'{e}finition de la matrice repr\'{e}sentative. On a donc $(1)\Rightarrow (2)$.
L'implication $(2)\Rightarrow (3)$ \'{e}tant claire, il reste \`{a} montrer $(3)\Rightarrow (1)$.

Supposons qu'il existe une base {\bf e} de $V$ telle que $M$ est sym\'etrique. Soient $x,y\in V$, et soient $\underline{X}, \underline{Y}$
leurs vecteurs de coordonn\'ees dans la base ${\bf e}$.
On a alors que
\[ \varphi(x,y)= {}^t\underline{X} M \underline{Y}\]
Le membre de droit est une matrice $1\times 1$ : elle est donc \'egale \`a sa propre  transpos\'ee et on a 
\[ \varphi(x,y)= {}^t\underline{X} M \underline{Y}= {}^t({}^t\underline{X} M \underline{Y})= {}^t \underline{Y}{}^t M \underline{X}= {}^t\underline{Y}M \underline{X}=
\varphi(y,x)\] 
CQFD.
\end{proof}

Le lemme pr\'{e}c\'{e}dent admet une r\'{e}ciproque, bien utile pour d\'{e}montrer qu'une application est bilin\'{e}aire et donner sa matrice repr\'{e}sentative dans une base fix\'{e}e.

\begin{lem}
 Soit $V$ un $\rr$-espace vectoriel de dimension finie, et soit ${\bf e}=(e_1,\ldots,e_n)$ une base de $V$. 
 Pour tout $a_{ij}\in \rr, 1\leq i,j\leq n$, l'application $$\varphi:V\times V\to \rr, 
(\sum_{i=1}x_ie_i,\sum_{j=1}y_je_j)\mapsto \sum_{1\leq i,j\leq n} a_{ij}x_iy_j$$ est une forme bilin\'{e}aire, dont la matrice $A$ dans la base {\bf e}
est donn\'ee par $A_{ij}=(a_{ij}).$
\end{lem}


\begin{exs}

\trick

$(1)$ L'application $$\varphi:\rr^2\times \rr^2\to\rr, \left(\left(\begin{array}{cc}x_1 \cr x_2\end{array}\right),\left(\begin{array}{cc}y_1 \cr y_2\end{array}\right) \right)\mapsto x_1y_1+x_2y_2+3x_1y_2-x_2y_1$$ est bilin\'{e}aire, et sa matrice repr\'{e}sentative dans la base canonique de $\rr^2$ est $$M=\left(\begin{array}{cc} 1 & 3 \cr -1 & 1\end{array}\right).$$

$(2)$ Consid\'{e}rons l'application $$\varphi: \cc_2[X]\times \cc_2[X]\to \cc, (P,Q)\mapsto P(1)Q(0).$$
On peut v\'{e}rifier directement que $\varphi$ est bilin\'{e}aire, mais on peut aussi utiliser la remarque pr\'{e}c\'{e}dente. 
Pour cela, consid\'{e}rons la base $1,X,X^2$ de $\rr_2[X]$. On \'{e}crit $$P=x_1+x_2X+x_3 X^2, Q=y_1+y_2X+y_3X^2.$$
On v\'{e}rifie alors que $\varphi(P,Q)=x_1y_1+x_2y_1+x_3y_1$. Donc $\varphi$ est bilin\'{e}aire et sa matrice repr\'{e}sentative dans la base $1,X,X^2$ est 
$$M=\left(\begin{array}{ccc} 1 & 0& 0 \cr 1 & 0& 0\cr 1& 0& 0\end{array}\right).$$


\end{exs}


Regardons maintenant ce qui se passe lorsque l'on effectue un changement de base.

\begin{prop}\label{chgtbase}
Soit $V$ un $\rr$-espace vectoriel de dimension finie $n$, soient ${\bf e}$ et ${\bf e'}$ deux bases de $V$, et soit $P$ la matrice de passage de la base 
${\bf e}$ 
\`{a} la base ${\bf e}'$, c'est-\`{a}-dire la matrice des coordonn\'{e}es des vecteurs de ${\bf e'}$ dans la base {\bf e}.
Soit $\varphi:V\times V\to \rr$ une forme bilin\'{e}aire, soit $M$ sa matrice dans la base ${\bf e}$ et soit $N$ sa matrice dans la base ${\bf e'}$. Alors on a $N={}^tPMP.$
\end{prop}

\begin{proof}
Soient $x,y\in V$, soient $\underline{X}, \underline{Y}$ leur vecteurs de coordonn\'ees dans la base ${\bf e}$ et
 soient $\underline{X}', \underline{Y}'$ leurs coordonn\'ees dans la base ${\bf e'}$. On a alors $\underline{X}= P\underline{X}'$ et $ \underline{Y}= P\underline{Y}'$ pour tout $x,y$ et donc
\[ \varphi(x,y)= {}^t \underline{X}M \underline Y= {}^t(P\underline{X}') M P\underline{Y}'= {}^t\underline{X}'{}^tP MP \underline{Y}'={}^t\underline{X}'N\underline{Y}'.\]
c'est \`a dire que $N= {}^t P MP$ par \ref{multzero}.
\end{proof}



Nous sommes pr\^{e}ts \`{a} d\'{e}finir la notion de rang.

\begin{defn}
Soit $\varphi:V\times V\to \rr$ une forme bilin\'{e}aire. Le {\bf rang} de $\varphi$ est le rang de n'importe quelle matrice 
repr\'{e}sentative de $\varphi$ dans une base de $V$.

Le rang est bien d\'{e}fini et ne d\'{e}pend pas de la base choisie d'apr\`{e}s la proposition pr\'{e}c\'{e}dente.
\end{defn}
\subsection{Orthogonalit\'e.}

Les expressions permettant de calculer $\varphi(x,y)$ peuvent se simplifier grandement lorsque la base {\bf e} est choisie convenablement. Par exemple, il est souvent utile de se d\'{e}barasser des termes crois\'{e}s lorsque c'est possible. On introduit pour cela la notion d'orthogonalit\'{e}.

\begin{defn}
Soit $V$ un espace vectoriel de dimension $n$ sur $\rr$, et soit $\varphi:V\times V\to \rr$ une forme bilin\'{e}aire {\bf sym\'{e}trique}.

On dit que deux vecteurs $x,y\in V$ sont $\varphi$-{\bf orthogonaux} si $\varphi(x,y)=0$.

On le note $x\underset{\varphi}{\perp} y$, ou $x\perp y$ s'il n'y a pas de confusion possible.

On dit que la base ${\bf e}=(e_1,\ldots,e_n)$ est $\varphi$-{\bf orthogonale} si les vecteurs de la base sont $\varphi$-orthogonaux deux \`{a} deux, c'est-\`{a}-dire si on a $$\varphi(e_i,e_j)=0\mbox{ pour tout }i\neq j.$$
\end{defn}
\begin{lem}
La base ${\bf e}$ est $\varphi$-orthogonale si et seulement si $M$, la matrice de $\varphi$ dans la base ${\bf e}$,  est diagonale.
\end{lem}
\begin{proof}
La base ${\bf e}$ est $\varphi$-orthogonale $\Leftrightarrow$ $\varphi(e_i, e_j)=0$ si $i\neq j$ $\Leftrightarrow$ $M_{i,j}=0$ si $i\neq j$ $\Leftrightarrow$ $M$ est diagonale.
\end{proof}
 
On dit que {\bf e} est $\varphi$-{\bf orthonorm\'{e}e} si on a $$\varphi(e_i,e_j)=\left\lbrace\begin{array}{l}0 \mbox{ si }i\neq j \cr 1 \mbox{ si }i=j\end{array}\right.$$ 

\begin{lem}
La base ${\bf e}$ est $\varphi$-orthonorm\'ee si et seulement si $\Mat(\varphi, {\bf e})$ est la matrice identit\'e.
\end{lem}
\begin{proof}
Laiss\'ee en exercice.
\end{proof}

\begin{defn}
On dit que deux sous-espaces $W,W'$ de $V$ sont {\bf orthogonaux} si on a $$\varphi(w,w')=0\mbox{ pour tout }w\in W,w'\in W'.$$ 

On dit que $V$ est la somme directe {\bf orthogonale} des sous-espaces $V_1,\ldots,V_m$ si $V=V_1\oplus\ldots\oplus V_m$ et les sous-espaces $V_1,\ldots,V_m$ sont orthogonaux deux \`{a} deux. On
note alors $$V=V_1\underset{\perp}{\oplus}\ldots\underset{\perp}{\oplus} V_m.$$
\end{defn}
Nous aurons besoin du r\'esultat suivant, que nous admettrons :
\begin{lem}\label{orthbase}
Soit $V$ un espace vectoriel et soit $\varphi$ une forme bilin\'eaire sur $V$. Soient $V_1,\ldots V_k$
des sous-espaces de $V$ tels que $V= \underset{\perp}{\oplus}_i V_i$. 
Si pour chaque $i$ on a que ${\bf v_i}$ est une base orthonorm\'ee 
de $V_i$ alors la concatenation $({\bf v}_1, {\bf v}_2, \ldots, {\bf v}_k)$
est une base orthonorm\'ee de $V$
\end{lem}
\begin{exs}

\trick 

$(2)$  L'application $$\varphi:\rr_2[X]\times \rr_2[X]\to\rr, (P,Q)\mapsto \int_{-1}^1 P(t)Q(t){\rm d}t$$

est bilin\'{e}aire sym\'{e}trique. De plus, $1\underset{\varphi}{\perp}X$ et $X\underset{\varphi}{\perp} X^2$. Par contre, $1$ et $X^2$ ne sont pas $\varphi$-orthogonaux, puisque l'on a 
$\varphi(1,X^2)=\ds\frac{2}{3}$; la base $1,X,X^2$ n'est donc pas $\varphi$-orthogonale.
Par contre, on peut v\'{e}rifier que la base $$1,X,X^2-\frac{1}{3}$$ est $\varphi$-orthogonale.
Elle n'est pas $\varphi$-orthonorm\'{e}e puisque \[\varphi(1,1)=2,\; 
\varphi(X, X)= 2/3,\; 
\varphi(X^2-\frac{1}{3}, X^2-\frac{1}{3})=  8/45.\] Par contre, on peut la rendre $\varphi$-orthonorm\'ee en
multipliant chaque \'el\'ement de la base par une constante bien choisie.    Plus pr\'ecisement, la base
\[\frac{1}{\sqrt{2}}\; \sqrt{\frac{3}{2}} X\; \sqrt{\frac{45}{8}}
(X^2-\frac{1}{3})\]
est une base $\varphi$-orthonorm\'ee.

$(3)$ La base canonique de $\rr^n$ est $\varphi$-orthonorm\'{e}e pour la forme bilin\'{e}aire sym\'{e}trique
$$\varphi:\rr^n \times \rr^n\to \rr, \left(\left(\begin{array}{c}x_1 \cr \vdots \cr x_n\end{array}\right),\left(\begin{array}{c}y_1 \cr \vdots \cr y_n\end{array}\right)\right)\mapsto x\cdot y=\sum_{i=1}^n x_iy_i$$
\end{exs}

$(4)$ Soit $V=C^0([-1,1],\rr)$, et soient $\mathcal{P}$ et $\mathcal{I}$ le sous-espace des fonctions paires et impaires respectivement.
On sait que l'on a $$V=\mathcal{P}\oplus\mathcal{I}.$$
Consid\'{e}rons l'application $$\varphi:V\times V\to\rr, (f,g)\mapsto \int_{-1}^1 f(t)g(t){\rm d}t$$ 
Alors, on a $$\varphi(f,g)=0\mbox{ pour tout }f\in\mathcal{P},g\in\mathcal{I}.$$
On a donc $$V=\mathcal{P}\underset{\perp}{\oplus}\mathcal{I}.$$  

Le lemme \ref{lembilrep} entra\^{\i}ne imm\'{e}diatement:

\begin{lem}\label{calculorth}
Soit $V$ un espace vectoriel de dimension finie $n$, soit ${\bf e}=(e_1,\ldots,e_n)$ une base de $V$, et soient $$x=\ds\sum_{i=1}^n x_i e_i, y=\sum_{i=1}^ny_i e_i$$ 
deux vecteurs de $V$.
Soit $\varphi:V\times V\to \rr$ une forme bilin\'{e}aire sym\'{e}trique. Si ${\bf e}$ est $\varphi$-orthogonale, on a $$\varphi(x,y)=\sum_{i=1}^n \varphi(e_i,e_i)x_iy_i.$$
En particulier, si ${\bf e}$ est $\varphi$-orthonorm\'{e}e, on a  $$\varphi(x,y)=\sum_{i=1}^n x_iy_i.$$

\end{lem}

Une base $\varphi$-orthonorm\'{e}e n'existe pas toujours. En effet, si $\varphi:V\times V\to \rr$ est bilin\'{e}aire sym\'{e}trique et il existe une base 
$\varphi$-orthonorm\'{e}e
alors le  lemme pr\'{e}c\'{e}dent montre que $\varphi(x,x)>0$ pour tout $x\neq 0$. 

Par exemple, la forme bilin\'{e}aire sym\'{e}trique $$\varphi:\rr^2\times \rr^2\to \rr, ((x_1,x_2),(y_1,y_2))\mapsto x_1y_1-x_2y_2.$$
n'admet pas de base $\varphi$-orthonorm\'{e}e, puisque $\varphi((0,1),(0,1))=-1<0$.

En revanche, on a le th\'{e}or\`{e}me suivant:

\begin{thm}\label{existorthogo}
Soit $V$ un espace vectoriel de dimension finie sur $\rr$, et soit $\varphi:V\times V\to \rr$ une forme bilin\'{e}aire sym\'{e}trique.
Alors il existe une base de $V$ qui est $\varphi$-orthogonale.
\end{thm}

\begin{proof}
On d\'{e}montre l'existence d'une base $\varphi$-orthogonale par r\'{e}currence sur $n=\dim(V)$.

Soit $(P_n)$ la propri\'{e}t\'{e}:

$(P_n)$  Pour tout $\rr$-espace vectoriel de dimension $n$ et tout $\varphi:V\times V\to \rr$, il existe une base $\varphi$-orthogonale. 

Si $n=1$, il n'y a rien \`{a} d\'{e}montrer.

Supposons que $(P_n)$ soit vraie, et soit $\varphi:V\times V\to \rr$ une forme bilin\'{e}aire sym\'{e}trique  avec $\dim(V)=n+1$.

Si $\varphi=0$, toute base est $\varphi$-orthogonale, et on a fini. On suppose donc que $\varphi\neq 0$. Soit $q$ la forme quadratique associ\'ee.
Par le formule de polarisation, si $q=0$ alors $\varphi=0$, ce qui n'est pas le cas. Il existe donc un $e_0$ tel que $q(e_0)\neq 0$, c'est \`a dire,
 $\varphi(e_0,e_0)\neq 0$.

L'application $$f:V\to \rr\; y\mapsto \varphi(e_0,y)$$ est alors une application lin\'{e}aire non nulle, puisque 
\[f(e_0)=\varphi(e_0,e_0)\neq 0\] et son image est donc $=\rr$. 
Par le th\'eor\`eme du rang, \[\dim{\rm Ker}(f)=n+1-1=n.\] 

Par hypoth\`{e}se de r\'{e}currence, il existe une base $(e_1,\ldots,e_n)$ de ${\rm Ker}(f)$ qui est orthogonale pour 
la forme 
$$\varphi':{\rm Ker}(f)\times {\rm Ker}(f)\to \rr, (x,y)\mapsto \varphi(x,y)$$


Montrons que ${\bf e}=(e_0,e_1,\ldots,e_n)$ est une base de $V$.
Puisque $\dim(V)=n+1$, il suffit de montrer que la famille $(e_0,\ldots,e_n)$ est libre. Soient 
$\lambda_0,\ldots,\lambda_n\in \rr$ tels que $$\lambda_0 e_0+\lambda_1 e_1+\ldots+\lambda_n e_n=0.$$
En appliquant $f$ \`{a} cette \'{e}galit\'{e} et en utilisant la lin\'{e}arit\'{e}, on obtient $$\lambda_0 f(e_0)+\lambda_1 f(e_1)+\ldots+\lambda_n f(e_n)=0.$$

Puisque $e_1,\ldots,e_n\in{\rm Ker}(f)$, on obtient $\lambda_0 f(e_0)=0$. Comme $f(e_0)\neq 0$, on obtient $\lambda_0=0$. On a donc $$\lambda_1 e_1+\ldots+\lambda_n e_n=0.$$
Comme $(e_1,\ldots,e_n)$ est une base de ${\rm Ker}(f)$, ils sont lin\'{e}airement ind\'{e}pendants, et on obtient donc  $$\lambda_1=\cdots=\lambda_n=0.$$

Ceci prouve que {\bf e} est une base de $V$. Il reste \`{a} v\'{e}rifier que cette base est $\varphi$-orthogonale.

Par choix des $e_i$, on a $$\varphi(e_i,e_j)=\varphi'(e_i,e_j)=0\mbox{ pour tout }i\neq j,1\leq i,j\leq n$$
et aussi $$\varphi(e_0,e_j)=f(e_j)=0\mbox{ pour tout }j>0$$
parce que $e_j\in {\rm Ker}(f)$.  
On a donc que $$\varphi(e_i,e_j)=0\mbox{ pour tout }0\leq i\neq j\leq n.$$
Ainsi, $(e_0,e_1,\ldots,e_n)$ est une base $\varphi$-orthogonale.
Ceci ach\`{e}ve la r\'{e}currence.
\end{proof}

\begin{rems}
\trick

 Le r\'{e}sultat pr\'{e}c\'{e}dent peut \^{e}tre faux si $\varphi$ n'est pas bilin\'{e}aire sym\'{e}trique.
Par exemple, si $\varphi: V\times V\to \rr$ est antisym\'{e}trique, c'est-\`{a}-dire si on a $$\varphi(y,x)=-\varphi(x,y)\mbox{ pour tout }x,y\in V,$$
et si $\varphi$ est {\bf non nulle}, alors il n'existe pas de base de $V$ qui est $\varphi$-orthogonale.

En effet, si $\varphi$ est une telle forme, alors on a $$\varphi(x,x)=-\varphi(x,x)\mbox{ pour tout }x\in V.$$
On a donc $$\varphi(x,x)=0\mbox{ pour tout }x\in V.$$
Supposons maintenant que ${\bf e}=(e_1,\ldots,e_n)$ est une base $\varphi$-orthogonale. On a donc $$\varphi(e_i,e_i)=0\mbox{ pour tout }i=1,\ldots,n.$$
Comme $\varphi(e_i,e_j)=0$ pour tout $i\neq j$ puisque {\bf e} est $\varphi$-orthogonale, on en d\'{e}duit que si $M$ est la matrice de $\varphi$ dans ${\bf e}$
alors $M=0.$

Le Lemme \ref{lembilrep} entra\^{\i}ne alors que l'on a $$\varphi(x,y)=0\mbox{ pour tout }x,y\in V,$$
ce qui contredit le fait que $\varphi$ est non nulle.

Un exemple d'une telle forme bilin\'{e}aire $\varphi$ est donn\'{e} par exemple par $$\varphi:\rr^2\times \rr^2\to\rr, 
\left(\left(\begin{array}{c}x_1\cr x_2\end{array}\right),\left(\begin{array}{c}y_1\cr y_2\end{array}\right)\right)\mapsto x_1y_2-x_2y_1.$$
\end{rems}
\subsection{Calcul effectif d'une base $\varphi$-orthogonale.}
Nous allons calculer une base $\varphi$-orthogonale en exploitant la forme
quadratique $q$ qui lui est associ\'ee. Rappelons que la forme 
bilin\'eaire sym\'etrique $\varphi$ peut \^etre reconstruite de la forme 
quadratique $s$ via la formule de polarisation
\[ \varphi(x,y)= \frac{1}{2} (q(x+y)-q(x)-q(y)).\]
Nous disons alors que $\varphi$ est la forme polaire de $q$, que
nous noterons parfois $\varphi_q$. 


\begin{exs}

\trick

$(1)$ L'application $$q:\rr^n\to \rr, x=\begin{pmatrix}x_1\cr \vdots \cr x_n\end{pmatrix}
\mapsto x_1^2+\ldots+x_n^2$$ est une forme quadratique, de forme polaire 
$$\varphi_q: \rr^n\times \rr^n\to \rr, \left( \begin{pmatrix}x_1\cr \vdots \cr x_n\end{pmatrix},\begin{pmatrix}y_1\cr \vdots \cr y_n\end{pmatrix}\right)\mapsto x_1y_1+\ldots+x_ny_n.$$

En effet, l'application $$\varphi:\rr^n\times \rr^n\to \rr, \left( \begin{pmatrix}x_1\cr \vdots \cr x_n\end{pmatrix},\begin{pmatrix}y_1\cr \vdots \cr y_n\end{pmatrix}\right)\mapsto x_1y_1+\ldots+x_ny_n$$
est bilin\'{e}aire sym\'{e}trique et on a clairement $\varphi(x,x)=q(x)$.

V\'erifions la formule de polarisation. On a que
$$q(x+y)=\sum_{i=1}^n (x_i+y_i)^2=\sum_{i=1}^n x_i^2+2x_iy_i+y_i^2=q(x)+q(y)+2
\varphi(x,y).$$

$(2)$ L'application $$q:C^0([0,1],\rr)\to\rr, f\mapsto \int_0^1 f(t)^2{\rm d}t$$ est une forme quadratique, de forme polaire 
$$\varphi_q:C^0([0,1],\rr)\times C^0([0,1],\rr)\to\rr, (f,g)\mapsto \int_0^1 f(t)g(t){\rm d}t.$$ 


En effet, l'application $$\varphi:C^0([0,1],\rr)\times C^0([0,1],\rr)\to\rr, (f,g)\mapsto \int_0^1 f(t)g(t){\rm d}t$$
est bilin\'{e}aire sym\'{e}trique et on a clairement $\varphi(f,f)=q(f)$.
\\ \\ V\'erifions de nouveau la formule de polarisation. 
$$q(f+g)=\int_0^1 (f(t)+g(t))^2{\rm d}t=\int_0^1 f(t)^2+2f(t)g(t)+g(t)^2$$
$${\rm d}t=q(f)+q(g)+2\int_0^1 f(t)g(t){\rm d}t.$$
\end{exs}

\begin{defn}
Soit $V$ un $\rr$-espace vectoriel de dimension finie $n$, et soit $q:V\to \rr$
une forme quadratique.
Soit {\bf e} une base de $V$. La matrice $M$ de $q$ dans la base {\bf e} est la matrice de la forme polaire $\varphi_q$ dans la base ${\bf e}$. 
C'est une matrice sym\'{e}trique par le Corollaire \ref{corosym}.

Le {\bf rang} de $q$, not\'{e} ${\rm rg}(q)$, est le rang de sa forme polaire.   

On dit que {\bf e} est $q$-orthogonale (resp. $q$-orthonorm\'{e}e) si elle est $\varphi_q$-orthogonale (resp. $\varphi_q$-orthonorm\'{e}e).
\end{defn}


L'\'{e}galit\'{e} $q(x)=\varphi_q(x,x)$ 
et le Lemme \ref{lembilrep} donnent imm\'{e}diatement:

\begin{lem}
Soit $V$ un espace vectoriel de dimension finie $n$ et ${\bf e}$ une base 
pour $V$. Soit $x\in V$, et soit $\underline{X}$ le vecteur 
coordonn\'{e}es de $x$  dans la base {\bf e}.


Soit $q:V\to \rr$ une forme quadratique, et soit $M$ sa matrice dans la base
${\bf e}$. Alors on a $$q(x)={}^t\underline{X} M \underline{X}.$$
En particulier, si {\bf e} est $q$-orthogonale, c'est \`a dire si $M$ est 
sym\'etrique, alors on a $$q(x)=\sum_{i=1}^n q(e_i)x_i^2.$$
\end{lem}

Le lemme suivant nous permet de passer directement de la forme quadratique 
$q$ a sa matrice $M$ sans calculer le forme polaire $\varphi$.

\begin{lem}\label{lemast}
Soit $V$ un espace vectoriel de dimension finie $n$. Soient $x,y\in V$, et soit ${\bf e}=(e_1,\ldots,e_n)$ une base de $V$. Alors pour tout $a_{ij}\in \rr,
1\leq i\leq j\leq n$, l'application 
$$q: V\to \rr, \sum_{i=1}^n x_ie_i\mapsto \sum_{i=1}^n a_{ii}x_i^2+2\sum_{1\leq i<j\leq n}a_{ij}x_ix_j$$
est une forme quadratique, et sa matrice $A$ dans la base
${\bf e}$ est donn\'ee par $$A=(a_{ij}).$$
\end{lem}

La d\'{e}monstration est laiss\'{e}e en exercice au lecteur. {\bf Attention au facteur} 2 !

\begin{ex}
L'application $$q:\rr^2\times \rr^2\to\rr, \begin{pmatrix} x_1 \cr x_2\end{pmatrix}\mapsto 3x_1^2+ 4x_1x_2+5x_2^2$$ est une forme quadratique, et sa matrice repr\'{e}sentative dans la base canonique de $\rr^2$ est donn\'{e}e par 
$$\begin{pmatrix}3 & 2 \cr 2 & 5\end{pmatrix}.$$
\end{ex}
Soient maintenant $\varphi$ une forme bilin\'eaire sur un espace $V$,
$q$ sa forme polaire, ${\bf e}$ une base pour $V$. Soit $x\in V$ un \'el\'ement arbitraire et $\underline{X}=\begin{pmatrix}{x_1\\ 
\vdots \\ x_n}\end{pmatrix}$ son vecteur de coordonn\'ees dans la 
base ${\bf e}$. Alors
\[ {\bf e}\mbox{ est $\varphi$-orthogonale }\]
\[ \Updownarrow\]
\[ \mbox{ la matrice de $\varphi$ dans la base ${\bf e}$ est diagonale }\]
\[ \Updownarrow\]
\[ \mbox{ la matrice de $q$ dans la base ${\bf e}$ est diagonale }\]
\[ \Updownarrow\]
\[ \exists a_i\in\rr \mbox{ tel que } q(x)=\sum_{i=1}^n a_i x_i^2.\]

Nous allons maintenant d\'{e}crire un algorithme, dit algorithme de Gauss, qui permet de trouver une base $q$-orthogonale. Nous ne justifierons pas compl\`{e}tement toutes les \'{e}tapes de cet algorithme. L'id\'ee de base sera la 
suivante : on cherche des coordonn\'ees $x_i(v)$ telles que 
\[q(v)=\sum_{i=1}^n a_i (x_i(v))^2\]
et une fois trouv\'ee les coordonn\'ees $x_i(v)$ on cherchera la base 
orthogonale $e_i$ correspondante, c-a-d, telle que 
\[ v=\sum_i x_i(v)e_i.\] 

{\bf Algorithme de Gauss}

Soit $V$ un $\rr$-espace vectoriel de dimension finie $n$, et soit {\bf e} une base de $V$. 
Soit $q:V\to \rr$ une forme quadratique, et soit $M=(a_{ij})_{1\leq i,j\leq n}$ sa matrice repr\'{e}sentative dans la base {\bf e}.
Si $x=\ds\sum_{i=1}^n x_i e_i$, on a donc $$q(x)=\sum_{i=1}^n a_{ii} x_i^2+2\sum_{1\leq i<j\leq n}a_{ij}x_ix_j=:P(x_1,\ldots,x_n).$$

On proc\`ede par r\'ecurrence sur le nombre de variables. A chaque \'etape, il
y a deux cas.

$(1)$ S'il existe un indice $k$ tel que $a_{kk}\neq 0$, on regroupe tous les termes faisant intervenir la variable $x_k$, et on compl\`{e}te le carr\'{e}. On \'{e}crit

$$P(x_1,\ldots,x_n)=a_{kk}x_k^2+2f_k x_k+P_0,$$
o\`{u} $f_k$ est une forme lin\'{e}aire en les variables $x_i,i\neq k$, et $P_0$ est une  forme quadratique en les variables $x_i,i\neq k$.

On a alors 

$$\begin{array}{lll}P(x_1,\ldots,x_n)&=&a_{kk}(x_k^2+\frac{2}{a_{kk}}f_k x_k)+P_0\cr &=&a_{kk}((x_k+\frac{f_k}{a_{kk}})^2-\frac{f_k^2}{a_{kk}^2})+P_0\end{array}.$$

On peut donc \'{e}crire $$P(x_1,\ldots,x_n)=a_{kk}(x_k+\frac{f_k}{a_{kk}})^2+P_1,$$
o\`{u} $P_1$ est une  forme quadratique en les variables $x_i,i\neq k$.

$(2)$ Si $a_{kk}=0$ pour tout $k$, mais qu'il existe $k$ et $\ell$ tels
que $k<\ell$ et $a_{k\ell}\neq 0$. C'est le cas p\'{e}nible.

%On peut toujours supposer pour l'explication, quitte \`{a} renum\'{e}roter, que $(k,\ell)=(n-1,n)$.

On \'{e}crit 
$$P(x_1,\ldots,x_n)=2a_{k\ell}x_k x_\ell+2f_{k}x_{k}+2f_\ell x_\ell+P_0,$$
o\`{u} $f_k$ et $f_\ell$ sont des formes lin\'{e}aires en les variables $x_i,i\neq k,\ell$, et $P_0$ est une forme quadratique en les variables $x_i,i\neq k,\ell$.

%On \'{e}crit $$P(x_1,\ldots,x_n)=2a_{n-1,n}x_{n-1}x_n+2(\sum_{i\leq n-2}a_{i,n-1}x_i)x_{n-1}+2(\sum_{i\leq n-2}a_{in}x_i)x_n+P_0(x_1,\ldots,x_{n-2}).$$

On a ainsi 

$$P(x_1,\ldots,x_n)=2a_{k\ell }(x_{k}+\frac{1}{a_{k\ell}}f_\ell)(x_{\ell}+\frac{1}{a_{k\ell}}f_k)
-\frac{2}{a_{k\ell}}f_kf_\ell+P_0.$$

On a donc $$P(x_1,\ldots,x_n)=2a_{k\ell}AB+P_1,$$
avec $A=\ds x_{k}+\frac{1}{a_{k\ell}}f_\ell, B=\ds x_{n}+\frac{1}{a_{k\ell}}f_k$, et $P_1$ est une forme quadratique en
les variables $x_i,i\neq k,\ell$. Pour la suite de al r\'ecurrence : si $P_1=0$
on arr\^ete, sinon on recommence le proc\'ed\'e avec $P_1$. 

On a alors $$P(x_1,\ldots,x_n)=\frac{a_{k\ell}}{2}((A+B)^2-(A-B)^2)+P_1.$$  


* Si $P_1=0$, on arr\^{e}te. Sinon, on recommence le proc\'{e}d\'{e} avec $P_1$.

On peut montrer que l'on obtient alors une \'{e}criture de la forme 
$$q(x)=\alpha_1(L_1(x))^2+\ldots+\alpha_r (L_r(x))^2,$$
o\`{u}:
 \begin{enumerate}
\item chaque $\alpha_i\in \rr^*$
\item chaque $L_i$ est une forme lin\'eaire sur $V$
\item la famille de formes $(L_1,\ldots, L_r)$ est ind\'ependante.
\end{enumerate}


* Ensuite, on choisit des formes lin\'eaires $L_{r+1}, L_{r+2},\ldots, L_n$ de
 telle fa\c con \`a ce que la famille $(L_1, \ldots, L_n)$ soit 
libre et on \'ecrit
$$q(x)=\alpha_1(L_1(x))^2+\ldots+\alpha_r (L_r(x))^2+ 0 (L_{r+1})^2+
\ldots + 0(L_n(x)))^2.$$

*Enfin, on cherche une base ${\bf e}'=(e'_1,e'_2,\ldots, e'_n)$ telle que 
pour tout $x\in V$ le 
vecteur de coordonn\'ees de $x$ dans la base ${\bf e}'$ soit
\[ \underline{X}=\begin{pmatrix}{L_1(x)\\ L_2(x) \\ \vdots \\ L_n(x)}
\end{pmatrix}.\]
Autrement dit, on doit avoir que 
\[ x= L_1(x) e'_1+ L_2(x) e'_2+\ldots L_n(x) e'_n,\]
ce qui est un syst\`eme d'\'equations qu'on peut r\'esoudre par le pivot de 
Gauss
pour trouver $e'_1,\ldots, e'_n$.


Si $x'_1,\ldots,x'_n$ sont les coordonn\'{e}es de $x\in V$ dans cette nouvelle base, on a 
$$q(x)=\alpha_1x_1^{'2}+\ldots+\alpha_r x_r^{'2}.$$
par d\'efinition.
\begin{rem}
En particulier, $r={\rm rg}(q)$.
\end{rem}
Il suit du lemme \ref{lemast} que la matrice de $q$ dans la base ${\bf e}'$ est
la matrice
$$M={\rm diag}(\alpha_1,\alpha_2, \ldots, \alpha_r,0,\ldots, 0)$$
\begin{rem}
Si $\phi:V\times V\to \rr$ est bilin\'{e}aire sym\'{e}trique, alors en appliquant l'algorithme de Gauss \`{a} la forme quadratique $$q_b:V\to \rr, x\mapsto 
\varphi(x,x),$$
on trouve une base {\bf v} qui est $q_\varphi$-orthogonale. Mais par 
d\'{e}finition, {\bf v} est donc orthogonale pour la forme polaire de 
$q_\varphi$, qui est $\varphi$.

Cet algorithme permet donc de trouver une base $\varphi$-orthogonale pour n'importe quelle forme bilin\'{e}aire sym\'{e}trique $\varphi$, ainsi que son rang.
\end{rem}

\begin{ex}
Soit $q:\rr^4\to \rr$ l'application qui a ${\bf u}=\begin{pmatrix} x\cr y \cr z\cr t\end{pmatrix}$ associe $$q({\bf u})=x^2+2xy+2xz+2xt+y^2+6yz-2yt+z^2+10zt+t^2.$$
L'application $q$ est une forme quadratique can c'est un polyn\^ome de degr\'e
$2$ homog\`ene.

Appliquons l'algorithme de Gauss \`{a} $q$ pour trouver une base $q$-orthogonale.

On a $$\begin{array}{lll}q({\bf u})&=&x^2+2(y+z+t)x+y^2+6yz-2yt+z^2+10zt+t^2
\cr &=& (x+y+z+t)^2-(y+z+t)^2+y^2+6yz-2yt+z^2+10zt+t^2\cr
&=& (x+y+z+t)^2+4yz-4yt+8zt
.\end{array}$$

On a maintenant $$\begin{array}{lll}
4yz-4yt+8zt&=& 4(yz +(-t)y +(2t)z)\cr
&=&4((y+2t)(z-t)+2t^2)\cr

&=& 4(y+2t)(z-t)+8t^2\cr
&=& (y+z+t)^2-(y-z+3t)^2+8t^2\end{array}.$$
Finalement, on obtient 
$$q({\bf u})=(x+y+z+t)^2 +(y+z+t)^2-(y-z+3t)^2+8t^2.$$

On a donc ${\rm rg}(q)=4$. On a 
\[ L_1(u)= x+y+z+t; L_2(u)= y+z+t; L_3(u)= y-z+t; L_4(u)= t\]
Nous cherchons ${e}_1',\ldots, e'_4$ tels que pour tout 
$\begin{pmatrix}{x\\y\\z\\t}\end{pmatrix}\in \R^4$ 
\[ (x+y+z+t) e'_1+ (y+z+t) e'_2+ (y-z+t) e'_3+ te'_4=
\begin{pmatrix}{x\\y\\z\\t}\end{pmatrix}\]
ce qui donne, en isolant chaque variable $x,y,z,t$
\[ e'_1= \begin{pmatrix}{1\\0\\0\\0}\end{pmatrix}\; e'_1+e'_2+e'_3= 
\begin{pmatrix}{0\\1\\0\\0}\end{pmatrix}, e'_1+e'_2-e'_3= \begin{pmatrix}{0\\0
\\1\\0}\end{pmatrix}, e'_1+e'_2+e'_3+e'_4=
\begin{pmatrix}{0\\0\\0\\1}\end{pmatrix}.\]
et donc apr\`es r\'esolution par le  pivot de Gauss on trouve que
\[e_1'=\begin{pmatrix}{1\\0\\0\\0}\end{pmatrix},\; 
e_2'=\begin{pmatrix}{-1\\1/2\\1/2\\0}\end{pmatrix},\;
e_3'= \begin{pmatrix}{0\\1/2\\-1/2\\0}\end{pmatrix},\;
e_4'=\begin{pmatrix}{0\\-1\\0\\1}\end{pmatrix}\
\end{ex}
Ces vecteurs $(e_1', e_2',e_3', e_4')$ forment donc une base $q$-orthogonale.
\\ \\
Le th\'eor\`eme suivant n'est valable que pour des formes r\'eelles.

\begin{thm}[Th\'eor\`eme d'inertie de Sylvester]
Soit $V$ un $\rr$-espace vectoriel de dimension finie $n$, et soit $q:V\to \rr$ une forme quadratique. Soit ${\bf e}$ une base $q$-orthogonale.
Soit $$r_+={\rm card}\{ i \vert q(e_i)>0\}\mbox{ et }r_-={\rm card}\{ i \vert q(e_i)<0\}.$$ 

Alors le couple $(r_+,r_-)$ ne d\'{e}pend pas de la base $q$-orthogonale choisie. De plus, $r_++r_-={\rm rg}(q)$.
\end{thm}

\begin{proof}
Soit ${\bf e}=(e_1,\ldots,e_n)$ une base $q$-orthogonale. Posons $\alpha_i=q(e_i)=\varphi_q(e_i,e_i)$ et $r=r_++r_-$.
Changer l'ordre des vecteurs de ${\bf e}$ ne change pas $r_+$ et $r_-$, ni le fait que la base soit $q$-orthogonale. 
On peut donc supposer sans perte de g\'{e}n\'{e}ralit\'{e} que l'on a $$q(e_i)>0, i=1,\ldots, r_+, q(e_i)<0, i=r_++1,\ldots, r,q(e_i)=0, i=r+1,\ldots,n.$$

Puisque ${\bf e}$ est $q$-orthogonale (c'est-\`{a}-dire 
$\varphi_q$-orthogonale), on obtient que $M$, la matrice de $q$ dans la base 
${\bf e}$, s'\'ecrit

$$M=\begin{pmatrix}q(e_1) & & \cr & \ddots & \cr & & q(e_n) \end{pmatrix}.$$
Or, seuls les r\'{e}els $q(e_1),\ldots,q(e_r)$ sont non nuls. Le rang d'une matrice diagonale \'{e}tant le nombre de termes diagonaux non nuls, on a bien $rg(q)=r=r_++r_-$. 

Soit maintenant ${\bf e'}$ une autre base $q$-orthogonale.
Soient $(r'_+,r'_-)$ le couple d'entiers correspondant. Remarquons que l'on a $r'_++r'_-=rg(q)=r$ par le point pr\'{e}c\'{e}dent. Comme pr\'{e}c\'{e}demment, quitte \`{a} changer l'ordre des vecteurs, on peut  supposer que  

$$q(e'_i)>0, i=1,\ldots, r'_+, q(e_i)<0, i=r'_++1,\ldots, r,q(e'_i)=0,i=r+1,\ldots,n.$$

Montrons que $e_1,\ldots,e_{r_+},e'_{r'_+ +1},\ldots,e'_n$ sont lin\'{e}airement ind\'{e}pendants.
Supposons que l'on ait une relation $$\lambda_1e_1+\ldots+\lambda_{r_+}e_{r_+}+\lambda_{r'_++1}e'_{r'_+ +1}+\ldots+\lambda_ne'_n=0.$$
On a donc $$\lambda_1e_1+\ldots+\lambda_{r_+}e_{r_+}=-(\lambda_{r'_++1}e'_{r'_+ +1}+\ldots\lambda_n e'_n).$$
En appliquant $q$ des deux c\^ot\'{e}s, et en utilisant le fait que les bases {\bf e} et ${\bf e'}$ sont $q$-orthogonales, on obtient 
$$\sum_{i=1}^{r_+}q(e_i)\lambda_i^2=\sum_{i=r'_++1}^{n}q(e'_i)\lambda_i^2.$$
Par choix de ${\bf e}$ et de ${\bf e'}$, le membre de gauche est $\geq 0$ et le membre de droite est $\leq 0$.

On en d\'{e}duit que l'on a $$\sum_{i=1}^{r_+}q(e_i)\lambda_i^2=0,$$
et puisque $q(e_i)>0$ pour $i=1,\ldots,r_+$, on en d\'{e}duit $$\lambda_1=\ldots=\lambda_{r_+}=0.$$
Mais alors, on a $$\lambda_{r'_++1}e'_{r'_+ +1}+\ldots\lambda_n e'_n=0,$$
et comme ${\bf e'}$ est une base, on en d\'{e}duit $$\lambda_{r'_++1}=\ldots=\lambda_n=0.$$

Ainsi, $e_1,\ldots,e_{r_+},e'_{r'_+ +1},\ldots,e'_n$ sont $r_++(n-r'_+)$ vecteurs lin\'{e}airement ind\'{e}pendants dans un espace vectoriel de dimension $n$. On a donc $$r_++(n-r'_+)\leq n,$$
et donc $r_+\leq r'_+$. En \'{e}changeant les r\^{o}les de ${\bf e}$ et ${\bf e'}$, on a de m\^{e}me $r'_+\leq r_+$.

On a donc $r_+=r'_+$, et comme on a ${\rm rg}(q)=r_++r_-=r'_++r'_-$, on en d\'{e}duit $r_-=r'_-$. Ceci ach\`{e}ve la
d\'{e}monstration.
\end{proof}

Cela conduit \`{a} la d\'{e}finition suivante.

\begin{defn}
Soit $V$ un $\rr$-espace vectoriel de dimension finie $n$, et soit $q:V\to \rr$ une forme quadratique.
Le couple $(r_+,r_-)$ est appel\'{e} la {\bf signature} de $q$.
\end{defn}

\begin{rem}
Pour calculer la signature d'une forme quadratique $q$, il suffit d'utiliser l'algorithme de Gauss pour \'{e}crire $q(x)$ sous la forme 
$$\alpha_1(u_{11}x_1+\ldots+u_{1n}x_n)^2+\ldots+\alpha_r (u_{r1}x_1+\ldots+u_{rn}x_n)^2,$$
et de compter le nombre de coefficients $\alpha_i$ qui sont strictement plus grand que $0$ et strictement plus petit que $0$.

En effet, on a vu que si ${\bf v}=(v_1,\ldots,v_n)$ est la base $q$-orthogonale obtenue \`{a} la fin de l'algorithme de Gauss, et $M$ est la matrice de 
$q$ dans cette base, alors 
$$M=diag(\alpha_1,\ldots,\alpha_r,0,\ldots,0).$$
Mais les coefficients diagonaux de $M$ 
sont exactement les r\'{e}els $q(v_i)$, et on conclut en utilisant la d\'{e}finition de $r_+$ et $r_-$.
\end{rem}

\begin{ex}
La signature de la forme quadratique $q$ de l'exemple pr\'{e}c\'{e}dent est $(3,1)$.
\end{ex}
\newpage
\section{Produits scalaires.}
Contrairement aux premiers chapitres, cette section n'est valable 
que pour les espaces vectoriels r\'eels. Nous survolerons en fin de chapitre sa g\'en\'eralisation \`a des espaces complexes.

\subsection{Produit scalaires, d\'efinitions et exemples.}
Nous voulons maintenant g\'{e}n\'{e}raliser la notion de produit scalaire - et donc de longueur, de distance et d'angle - \`{a} un espace vectoriel r\'eel arbitraire.
Sur $\rr^2$ ou $\rr^3$, et plus g\'{e}n\'{e}ralement $\rr^n$, l'application produit scalaire canonique 
$$\rr^n \times \rr^n\to \rr, \left (\left(\begin{array}{c}x_1 \cr \vdots \cr x_n\end{array}\right),\left(\begin{array}{c}y_1 \cr \vdots \cr y_n\end{array}\right)\right)\mapsto x\cdot y=\sum_{i=1}^n x_iy_i$$ est bilin\'{e}aire et sym\'{e}trique. Notons que la longeur d'un vecteur $x\in\rr^3$ peut \^etre calcul\'ee par
$\vert\vert x\vert\vert=\sqrt{x\cdot x)$ : nous aurions donc envie d'associer une notion de longueur \`a une forme bilin\'eaire $\varphi$ en posant $||x|| =\sqrt{\varphi(x,x)}$.
Malheureusement, il n'est pas s\^ur que cette quantit\'e existe (si $\varphi(x,x)<0$ elle n'existe pas) ni qu'elle soit strictement positive pour un $x $ non-nul (or nous ne voulons pas
une distance $0$ entre deux vecteurs distincts). 


Ces consid\'erations inspirent les d\'{e}finitions suivantes:


\begin{defn}\label{defprodscal}
Soit $V$ un espace vectoriel r\'eel.
On dit qu'une forme bilin\'{e}aire sym\'{e}trique $\varphi:V\times V\to \rr$ est {\bf positive} si $\varphi(x,x)\geq 0$ pour tout $x\in V$, et {\bf d\'{e}finie positive} si $\varphi(x,x)>0$ pour tout $x\in V,x\neq 0$.

Remarquons que $\varphi$ est d\'{e}finie positive si et seulement si 

$(i)$ $\varphi$ est positive et

$(ii)$ $\varphi(x,x)=0\Rightarrow x=0_V.$

C'est en g\'{e}n\'{e}ral cette reformulation de la d\'{e}finition que l'on utilise en pratique pour v\'{e}rifier si oui ou non une forme bilin\'{e}aire donn\'{e}e est d\'{e}finie positive. 

\end{defn}


\begin{defn}
Soit $V$ un $\rr$-espace vectoriel (non n\'{e}cessairement de dimension finie).
Un {\bf produit scalaire} sur $V$ est une forme bilin\'{e}aire $$\langle\, , \, \rangle:V\times V\to \rr, (x,y)\mapsto \langle x,y\rangle$$ sym\'{e}trique et d\'{e}finie positive. 

On dit alors que le couple $(V,\langle\, , \, \rangle)$ est un espace {\bf prehilbertien}.



\end{defn}


\begin{exs}
\trick

$(1)$ L'application $$\rr^n \times \rr^n\to \rr, \left(\left(\begin{array}{c}x_1 \cr \vdots \cr x_n\end{array}\right),\left(\begin{array}{c}y_1 \cr \vdots \cr y_n\end{array}\right)\right)\mapsto \sum_{i=1}^n x_iy_i$$

est un produit scalaire.

$(2)$ L'application $$\langle \, , \rangle: C^0([a,b], \rr)\times C^0([a,b], \rr)\to \rr, (f,g)\mapsto\langle f,g\rangle=\int_a^b f(t)g(t){\rm d}t$$
est un produit scalaire.

Nous allons traiter cet exemple en d\'{e}tail.

$(a)$ Montrons que $\langle \, , \rangle$ est sym\'{e}trique. En effet, pour tout $f,g\in C^0([a,b], \rr)$, on a
$$\langle g,f\rangle=\int_a^b g(t)f(t){\rm d}t=\int_a^b f(t)g(t){\rm d}t=\langle f,g\rangle.$$

$(b)$ Montrons que $\langle \, , \rangle$ est bilin\'{e}aire. Pour tout $f_1,f_2,f,g\in C^0([a,b], \rr),\lambda\in\rr$, on a 
$$\begin{array}{lll}\langle f_1+f_2,g\rangle &=&\ds\int_a^b (f_1+f_2)(t)g(t){\rm d}t \cr 
&=& \ds\int_a^b (f_1(t)+f_2(t))g(t){\rm d}t \cr 
&=& \ds\int_a^b f_1(t)g(t){\rm d}t + \int_a^b f_2(t)g(t){\rm d}t \cr 
&=& \langle f_1,g\rangle +\langle f_2,g\rangle 
\end{array}$$

et aussi 

$$\begin{array}{lll}\langle \lambda f,g\rangle &=&\ds \int_a^b (\lambda f)(t)g(t){\rm d}t \cr
&=&\ds \int_a^b \lambda f(t)g(t){\rm d}t\cr
&=&\ds \lambda\int_a^b f(t)g(t){\rm d}t\cr
&=& \lambda \langle f,g\rangle\end{array}.$$

Par $(a)$ il d\'ecoule que 
$$\langle f,g_1+g_2\rangle=\langle f,g_1\rangle+\langle f,g_2\rangle \mbox{ et }\langle f,\lambda g\rangle=\lambda \langle f,g\rangle$$
pour tout $f,g,g_1,g_2\in \rr[X],\lambda\in\rr$ ({\bf V\'{e}rifiez-le vous-m\^{e}me !!!}).

Ainsi, $\langle \, ,\rangle$ est bilin\'{e}aire.

$(b)$ Montrons que $\langle \, , \rangle$ est sym\'{e}trique. En effet, pour tout $f,g\in\rr[X]$, on a 
$$\langle g,f\rangle=\int_a^b g(t)f(t){\rm d}t=\int_a^b f(t)g(t){\rm d}t=\langle f,g\rangle.$$

$(c)$ Montrons enfin que $\langle, \rangle$ est d\'{e}finie positive. On va utiliser pour cela la reformulation de la d\'{e}finition \ref{defprodscal}.

Pour tout $f\in C^0([a,b], \rr)$, on a $$\langle f,f\rangle=\int_a^b f(t)^2{\rm d}t.$$
Or, l'int\'{e}grale d'une fonction positive est positive. 

Comme la fonction $f^2(t)$ est positive, on en d\'{e}duit que $$\langle f,f\rangle\geq 0\mbox{ pour tout }f\in C^0([a,b], \rr).$$
Supposons maintenant que l'on a $\langle f,f\rangle=0,$ c'est \`a dire que 
$$\int_a^b Pf(t)^2{\rm d}t=0.$$
Or l'int\'{e}grale d'une fonction {\bf positive} et {\bf continue} $f:[a,b]\to\rr$ est nulle si et seulement si $f$ est identiquement nulle.
{\bf Exercice : d\'emontrez-le vous m\^eme}.)
Comme la fonction $$[a,b]\to \rr, t\mapsto f(t)^2$$ est positive et continue, on en d\'{e}duit $$f(t)^2=0\mbox{ pour tout }t\in [a,b],$$
c'est-\`{a}-dire $f=0$ : CQFD.

$(3)$. Pour tout fonction continue et strictement positive sur $[a,b]$, $p:[a,b]\rightarrow \rr$, la fonction  
$$\langle \, , \rangle: C^0([a,b], \rr)\times C^0([a,b], \rr)\to \rr, (f,g)\mapsto\langle f,g\rangle=\int_a^b p(t) f(t)g(t){\rm d}t$$
est un produit scalaire.


$(4)$ L'application $M_{n}(\rr)\times M_n(\rr)\rightarrow \rr$, $(M,N)\mapsto {\rm Tr}({}^tMN)$ est un produit scalaire sur $M_n(\rr)$. 

$(5)$ L'application $$\varphi : \rr^2\times\rr^2\to \rr, \left(\left(\begin{array}{c}x_1\cr x_2\end{array}\right),\left(\begin{array}{c}y_1\cr y_2\end{array}\right)\right)\mapsto x_1y_1-x_2y_2$$

n'est pas un produit scalaire. Elle est bien bilin\'{e}aire sym\'{e}trique, mais elle n'est pas positive, comme on l'a vu pr\'{e}c\'{e}demment.

$(6)$ L'application $$\varphi:\rr[X]\times\rr[X]\to\rr, (P,Q)\mapsto P(0)Q(0)$$ n'est pas un produit scalaire.
Elle est bien bilin\'{e}aire, sym\'{e}trique, positive, mais pas d\'{e}finie positive. Par exemple, on a $\varphi(X,X)=0$, mais $X\neq 0$.
\end{exs}

\subsection{Produits scalaires : g\'eom\'etrie.}
Les propri\'{e}t\'{e}s du produit scalaire permettent alors, comme dans le cas classique, de d\'{e}finir la ``longueur'', ou {\it norme} d'un vecteur de $V$.

\begin{defn}
Soit $(V,\langle\, , \, \rangle)$ un espace prehilbertien
Pour tout $x\in V$, on d\'{e}finit la {\bf norme} de $x$, not\'{e}e $\vert\vert x\vert\vert$, par $$\vert\vert x\vert\vert=\sqrt{\langle x,x\rangle}.$$
\end{defn}
Notons que par d\'efinition d'un produit scalaire, $||x||\geq 0$, et $||x||=0$ si et seulement si $x=0$.
\begin{defn}
Soit $(V,\langle\, , \, \rangle)$ un espace prehilbertien. Soient $v,w\in V$. On d\'efinit la distance entre $v$ et $w$ par 
\[ d(v,w)= || v-w||.\]
\end{defn}
Encore une fois, la distance entre $v$ et $w$ est positive et n'est $0$ que si $v=w$.
Nous aurions envie de poser la d\'efinition suivante :
\begin{defn}
Soit $(V,\langle\, , \, \rangle)$ un espace prehilbertien. Soient $v,w\in V$ avec $v,w\neq 0$. On d\'efinit l'angle entre $v$ et $w$ par 
\[ \theta= \arccos\left(\frac{\langle v,w\rangle}{||v||\times ||w||}\right).\]
\end{defn}
\begin{rem}
Avec cette d\'efinition de $\theta$, l'angle entre $v$ et $w$, nous avons automatiquement $\theta\in [0, \pi]$. Par ailleurs, il s'agit d'une angle non-orient\'e : $\theta$
ne d\'epend pas de l'ordre de $v$ et $w$.
\end{rem}
Malheureusement, ce n'est pas \'evident que cette d\'efinition soit bien pos\'ee.
En effet, la fonction arccos n'est d\'efinie que pour des nombres r\'eels $x$ satisfaisant 
la condition $-1\leq x\leq 1$ ou autrement dit $|x|\leq 1$. Nous devons donc v\'erifier la proposition suivante :

\begin{prop}[In\'{e}galit\'{e} de Cauchy-Schwarz]
Soit $(V,\langle\, , \, \rangle)$ un espace prehilbertien. Alors
pour tout $x,y\in V$, on a $$|\langle x,y\rangle|\leq \vert\vert x\vert\vert\times \vert\vert y\vert\vert,$$
et on a \'{e}galit\'{e} dans cette expression 
si et seulement si la famille $x,y$ est li\'{e}e sur $\rr$, c'est-\`{a}-dire s'il existe $\lambda,\mu\in\rr, (\lambda,\mu)\neq (0,0)$ tels que $\lambda x+\mu y=0$.
\end{prop}
\begin{proof}
Le r\'esultat \'etant imm\'ediat si $x$ ou $y$ est \'egal \`a $0$, on peut supposer $x,y\neq 0$ : si $x,y\neq 0$ nous avons qu'il  existe $\lambda,\mu\in\rr, (\lambda,\mu)\neq (0,0)$ 
tels que $\lambda x+\mu y=0$ si et seulement si il existe $t\in \rr$ tel que $x+ty=0$.
Consid\`erons la fonction de $t$ \[f(t)=\langle x+ ty, x+ty\rangle= t^2 ||y||^2+2t\langle x,y\rangle + ||x||^2.\]
Ceci est une fonction quadratique de $t$ qui ne prend pas de valeurs n\'egatives : elle a donc un discriminant $\Delta\leq 0$, c'est \`a dire
\[ \Delta= 4(\langle x,y\rangle)^2 - 4||x||^2||y||^2\leq 0.\]
On a donc que 
\[ (\langle x,y\rangle)^2 \leq  ||x||^2||y||^2\]
et
\[ |\langle x,y\rangle| \leq  ||x||||y||.\]
De plus, on a \'galit\'e dans cette expression si et seulement si $\Delta =0$, c'est-\`a-dire si et seulement si il existe $t$ tel que $f(t)=0$. 
Par d\'efinition de $f(t)$, nous avons \'egalit\'e dans cette expression si et seulement si il existe $t$ tel que $x+ty=0$. 
CQFD.\end{proof}
L'in\'egalit\'e de Cauchy-Schwarz est donc valable et notre d\'efinition de $\theta$ est bien pos\'ee.\\ \\
Un certain nombre de formules de la g\'eom\'etrie dans l'espace sont toujours valables dans ce contexte :

\begin{lem}[Th\'eor\`eme de Pythagore]
Soit $(V,\langle\, , \, \rangle)$ un espace prehilbertien et soient $v,w\in V$avec $v,w \neq 0_V$. Soit $\theta$ l'angle entre $v$ et $w$. Alors on a 
\[ d(v,w)^2= d(0_V,v)^2+d(0_V,w)^2\Leftrightarrow \theta= \pi/2.\]
\end{lem}
\begin{proof}
On note tout d'abord que par d\'efinition $\theta=\pi/2$ si et seulement si $\langle v,w \rangle =0$. 
Par d\'efinition, \[d(v,w)^2= \langle v-w, v-w\rangle\]\[ = \langle v,v\rangle + \langle w,w\rangle -2\langle v,w\rangle\]
\[= d(v,0_V)^2+ d(w,0_V)^2- 2 \langle v,w\rangle\]
et donc 
\[ d(v,w)^2= d(0,v)^2+d(0,w)^2\Leftrightarrow \langle v,w \rangle =0 \Leftrightarrow \theta= \pi/2.\]
\end{proof}

\begin{lem}[Identit\'e du parall\'elogramme]
Soit $(V,\langle\, , \, \rangle)$ un espace prehilbertien et soient $v,w\in V$. On a alors
\[ ||v+w||^2+||v-w||^2=2(||v||^2+ ||w||^2).\]
\end{lem}
\begin{proof}
Exercice pour le lecteur.
\end{proof}


\begin{lem}[In\'egalit\'e triangulaire]
Soit $(V,\langle\, , \, \rangle)$ un espace prehilbertien et soient $v,w\in V$. On a alors
\[ ||v+w||\leq ||v||+||w||.\]

\end{lem}
\begin{proof}
On a que 
\[ || v+w||^2= ||v||^2+ ||w||^2+2\langle v,w \rangle.\]
Par l'in\'egalit\'e de Cauchy-Schwarz on a que 
\[ ||v+w||^2\leq ||v||^2+||w||^2+2||v||\times ||w||= (||v||+||w||)^2.\]
Puisque $||v+w||$ et $||v||+||w||$ sont positifs, on peut prend la racine carr\'ee des deux membres pour d\'eduire que
\[|| v+w||\leq ||v||+||w||.\]
\end{proof}
Les deux lemmes suivants sont souvent tr\`{e}s utiles.

\begin{lem}
Soit $(V,\langle\, , \, \rangle)$ un espace prehilbertien, et soient $x_1,\ldots,x_k\in V$ une famille de vecteurs deux \`{a} deux orthogonaux.
Alors on a $$\vert\vert x_1+\ldots+x_k\vert\vert^2=\vert\vert x_1\vert\vert^2+\ldots+\vert\vert x_k\vert\vert^2.$$
\end{lem}

\begin{proof}
Supposons $x_1,\ldots,x_k\in V$ deux \`{a} deux orthogonaux. On a donc $$\langle x_i,x_j\rangle=0\mbox{ pour tout }i\neq j.$$
Par ailleurs, on a  que
$$\vert\vert x_1+\ldots+x_k\vert\vert^2=\langle x_1+\ldots+x_k,x_1+\ldots+x_k\rangle=\sum_{i,j=1}^k\langle x_i,x_j\rangle.$$
Mais puisque $\langle x_i,x_j\rangle=0$ pour tout $i\neq j$, on obtient 
$$\vert\vert x_1+\ldots+x_k\vert\vert^2=\sum_{i=1}^k \langle x_i,x_i\rangle=\sum_{i=1}^k \vert\vert x_i\vert\vert^2,$$
ce que l'on voulait d\'{e}montrer.
\end{proof}

\begin{lem}\label{lem-liborth}
Soit $(V,\langle\, , \, \rangle)$ un espace prehilbertien, et soient $x_1,\ldots,x_k\in V$ des vecteurs {\bf non nuls} deux \`{a} deux orthogonaux. Alors $(x_1,\ldots,x_k)$ est une famille 
libre.
\end{lem}

\begin{proof}
Soient $\lambda_1,\ldots,\lambda_k\in \rr$ tels que $$\lambda_1x_1+\ldots+\lambda_k x_k=0_V.$$ 
Soit $j\in \{1,\ldots, k\}$.
On a $$\langle x_j,  \lambda_1x_1+\ldots+\lambda_k x_k\rangle=\langle x_j, 0_V\rangle=0,$$
et donc $$\sum_{i=1}^k\lambda_i \langle x_j,x_i\rangle=0.$$
Puisque les $x_i$ sont deux \`{a} deux orthogonaux, cela s'\'{e}crit $$\lambda_j \langle x_j,x_j\rangle=0.$$
Puisque par hypoth\`{e}se $x_j\neq 0$, on a $\langle x_j,x_j\rangle >0$, et donc $\lambda_j=0$.
Ceci ach\`{e}ve la d\'{e}monstration.
\end{proof}


Revenons maintenant \`{a} l'existence de bases orthonorm\'{e}es.

\begin{prop}
Soit $(V,\langle \, ,\, \rangle)$ un espace prehilbertien {\bf de dimension finie}. Alors $V$ poss\`{e}de une base $(v_1,\ldots,v_n)$ orthonorm\'{e}e pour le produit scalaire. 

De plus, si $(v_1,\ldots,v_n)$ est une base orthonorm\'{e}e, alors pour tout $x\in V$, on a 
$$x=\langle v_1,x\rangle v_1+\ldots+\langle v_n,x\rangle v_n.$$
\end{prop}

\begin{proof}
D'apr\`{e}s le Th\'{e}or\`{e}me \ref{existorthogo}, il existe une base ${\bf e}=(e_1,\ldots,e_n)$ de $V$ qui est orthogonale pour le produit scalaire.
Puisque ${\bf e}$ est une base, $e_i\neq 0$ pour tout $i$, et on a donc $\vert\vert e_i\vert\vert\neq 0$.

Pour tout $i=1,\ldots,n$, on pose $$v_i=\frac{1}{\vert\vert e_i\vert\vert}e_i.$$
Il est clair que $(v_1,\ldots,v_n)$ est une base de $V$.

De plus, on a $$\langle v_i,v_j\rangle=\langle \frac{1}{\vert\vert e_i\vert\vert}e_i,\frac{1}{\vert\vert e_j\vert\vert}e_j\rangle=\frac{1}{\vert\vert e_i\vert\vert\times
\vert\vert e_j\vert\vert}\langle e_i,e_j\rangle\mbox{ pour tout }i, j.$$

Puisque {\bf e} est une base orthogonale, on obtient $$\langle v_i,v_j\rangle=0\mbox{ pour tout }i\neq j.$$
De plus, pour tout $i$, on a $$\langle v_i,v_i\rangle=\frac{1}{\vert\vert e_i\vert\vert^2}\langle e_i,e_i\rangle=\frac{\langle e_i,e_i\rangle}{\langle e_i,e_i\rangle}=1.$$
Ainsi, $(v_1,\ldots,v_n)$ est une base orthonorm\'{e}e.


Soit maintenant $(v_1,\ldots,v_n)$ une base orthonorm\'{e}e, et soit $x\in V$. Puisque $v_1,\ldots,v_n$ est une base, on peut \'{e}crire $$x=\lambda_1 v_1+\ldots+\lambda_n v_n.$$
Pour tout $j$, on a alors $$\langle v_j,x\rangle=\sum_{i=1}^n \lambda _i \langle v_j,v_i\rangle=\lambda_j,$$
la derni\`{e}re \'{e}galit\'{e} provenant du fait que $v_1,\ldots,v_n$ est une base orthonorm\'{e}e.
On a donc bien l'\'{e}galit\'{e} annonc\'{e}e. 
\end{proof}

Nous avons donc maintenant une notion satisfaisante de la distance entre deux \'el\'ements d'un espace vectoriel muni d'un produit scalaire. 
Rappelons que la question qui a motiv\'e ce travail est la suivante : je veux construire dans un espace vectoriel $V$ un ``bon approximant'' $w$ pour un
\'el\'ement $v$ sous la contrainte que $w$ doit \^etre contenu dans un sous-espace $W$. Nous savons maintenant ce qu'on veut dire exactement par un ``bon approximant'' : on veut que
la distance $d(v,w)$ entre $v$ et $w$ soit la plus petite possible. Le lemme suivant nous donne un crit\`ere num\'erique pour que $w\in W$ soit le ``meilleur approximant''
pour $v$.

\begin{lem}
Soit $V$ un espace prehilbertien, $W$ un sous espace de $V$ et $v$ un \'el\'ement de $V$. Si $w\in W$ a la propri\'et\'e que $\langle v-w,w'\rangle =0$ pour tout $w'\in W$
alors pour tout $w'\in W$ on a que $d(v,w)\leq d(v,w')$, avec \'egalit\'e si et seulement si $w'=w$.
\end{lem}

Autrement dit, si la droite qui relie $v$ \`a $w\in W$ est perpendiculaire \`a $W$ alors  $w$ est le point de $W$ le plus proche de $v$. Ce r\'esultat vous est familier depuis le lyc\'ee 
pour le cas ou $v\in \rr^3$ et $W$ est un plan dans $\rr^3$.
\begin{proof}
On a que
\[ d(v,w')= || v-w'|| =|| (v- w) + (w-w')||.\]
Maintenant, $w-w'\in W$ donc par hypoth\`ese $(v-w) \perp (w-w')$ et par le th\'eor\`eme de Pythagore
\[ d(v,w')^2= ||(v-w)||^2+ ||(w-w')||^2 \geq d(v,w)^2\]
avec \'egalit\'e si et seulement si $||w-w'||=0$, c'est-\`a-dire $w=w'$.
\end{proof} 

Notre crit\`ere est que $(v-w)$ doit \^etre orthogonal \` a tous les \'el\'ements de $W$. Etudions donc l'ensemble constitu\'e de tels 
\'el\'ements. 

\begin{defn}
Soit $(V,\langle \, , \,\rangle)$ un espace prehilbertien et soit $S$ un sous-ensemble de $V$. L'orthogonal de $S$, not\'{e} $S^\perp$, est le sous-ensemble de $V$ d\'{e}fini par 
$$S^\perp=\{ x\in V \mid \langle s,x\rangle=0 \mbox{ pour tout }s\in S \}.$$
\end{defn}
{\bf Exercice.} D\'emontrer que $S^\perp$ est toujours un sous-espace vectoriel de $W$. 
 
\begin{thm}\label{thmprojorth}
Soit $(V,\langle \, , \,\rangle)$ un espace prehilbertien et soit $W$ un sous-espace vectoriel de $V$. Alors:

\begin{enumerate}
\item Pour tout $w\in W$ et tout $w'\in W^\perp$, on a $w\perp w'$. De plus, $W\cap W^\perp=\{0_V\}$.
\item Si $W$ est de dimension finie, on a $V=W{\oplus} W^\perp$. Autrement dit, tout $x\in V$ s'\'{e}crit de mani\`{e}re unique sous la forme $$x=w+w', w\in W,w'\in W^\perp.$$
De plus, si $(v_1,\ldots, v_k)$ est une base orthonorm\'ee pour $W$ alors on a $w= \sum_{i=1}^k \langle x, v_i\rangle v_i$.
\end{enumerate}
\end{thm}

\begin{proof}

\trick

$(1)$ Si $w\in W$ et $w'\in W^\perp$, alors on a $\langle w,w'\rangle=0$ par d\'{e}finition de $W^\perp$. On a donc $w\perp w'$.
Soit maintenant $w\in W\cap W^\perp$. Puisque $w\in W^\perp$ et $w\in W$  on a que $\langle w,w\rangle=0$
et donc $w=0$ d'apr\`{e}s les propri\'{e}t\'{e}s du produit scalaire.

Ainsi, on a $W\cap W^\perp=\{ 0 \}$, ce qu'il fallait v\'{e}rifier. 

$(2)$ D'apr\`{e}s $(1)$, il reste \`{a} d\'{e}montrer que $V=W+W^\perp$, c'est-\`{a}-dire que tout vecteur $v\in V$ peut s'\'{e}crire $v=w+w'$ avec $w\in W$ et $w'\in W^\perp$.


Si $W=\{0\}$, on a $W^\perp=V$, et il n'y a rien \`{a} faire. On peut donc supposer que $W$ n'est pas l'espace trivial $\{ 0_V \}$.
La restriction \`{a} $W$ du produit scalaire sur $V$ est encore un produit scalaire.
Puisque $W$ est de dimension finie, $W$ poss\`{e}de une base orthonorm\'{e}e $(v_1,\ldots,v_k)$ d'apr\`{e}s la proposition pr\'{e}c\'{e}dente.

Soit $v\in V$. On pose $$w=\sum_{i=1}^k \langle v_i, v\rangle v_i.$$
Alors $w\in W$. D'autre part, on a 
$$\begin{array}{lll}\langle v_j,v-w\rangle &=& \langle v_j,v\rangle-\langle v_j,w\rangle\cr 
&=& \langle v_j,v\rangle- \langle v_j,\displaystyle\sum_{i=1}^n \langle v_i,v\rangle v_i\rangle \cr
&=& \langle v_j,v\rangle-\displaystyle\sum_{i=1}^k \langle v_i,v\rangle \langle v_j,v_i\rangle.\end{array}$$
Puisque $v_1,\ldots,v_k$ est orthonorm\'{e}e, on en d\'{e}duit:
$$\langle v_j,v-w\rangle=\langle v_j,v\rangle-\langle v_j,v\rangle=0,$$
et ceci pour tout $j=1,\ldots,k$.

Soit $s\in W$. Alors on peut \'{e}crire $s=s_1v_1+\ldots+s_k v_k$, et donc $$\langle s, v-w\rangle=\sum_{i=j}^k {s}_j\langle v_j,v-w\rangle=0.$$
Ainsi, $v-w\in W^\perp$, et donc on a la d\'{e}composition voulue en posant $w'=v-w$. Si maintenant on a deux d\'{e}compositions $$v=w_1+w'_1=w_2+w'_2,w_i\in W,w'_i\in W^\perp,$$ on a $$w_1-w_2=w'_2-w'_1\in W\cap W^\perp,$$
car $W$ et $W^\perp$ sont des sous-espaces vectoriels de $V$. Par le premier point, on en d\'{e}duit $w_1-w_2=w'_2-w'_1=0_V$, et donc $w_1=w_2, w'_1=w'_2$, CQFD.
\end{proof}

\begin{rem}
Le point $(2)$ est faux sans hypoth\`{e}se de finitude de la dimension de $W$.
\end{rem}

D'apr\`{e}s le deuxi\`{e}me point du th\'{e}or\`{e}me, lorsque $W$ est de dimension finie, tout $x\in V$ se d\'{e}compose de mani\`{e}re unique sous la forme $$x=w+w',w\in W,w'\in W^\perp.$$

Cela conduit \`{a} la d\'{e}finition suivante:

\begin{defn}
Soit $(V,\langle \, , \,\rangle)$ un espace prehilbertien, et soit $W$ un sous-espace de $V$ de dimension finie.
Pour tout $x=w+w'\in V$ avec $w\in W$ et $w'\in W^\perp$ on pose $$p_W(x)=w.$$
Le vecteur $p_W(x)\in W$ est appel\'{e} la {\bf projection orthogonale} de $x$ sur $W$. Si $(v_1,\ldots v_k)$ est une base orthonorm\'ee de $W$ alors on a
\[p_W(x)= \sum_{i=1}^k \langle x, v_i\rangle v_i.\]

Le lecteur v\'{e}rifiera \`{a} titre d'exercice les propri\'{e}t\'{e}s suivantes:

\begin{enumerate}

\item L'application $p_W:V\to V$ est lin\'{e}aire. 

\item Pour tout $x\in V$, on a et $p_W(x)\in W,$ $(x-p_W(x))\in W^\perp$. 

%\item Pour tout $x\in V$, on a $p_W(x)=x\iff x\in W$.

%\item Pour tout $x\in V$, on a $p_W(x)=0_V\iff x\in W^\perp$.

%\item Pour tout $x\in V$, on a $p_W(p_W(x))=p_W(x)$. 

\end{enumerate}
\end{defn}

Notons que la projection orthogonale a surtout la propri\'et\'e int\'eressante suivante :
\[ p_W(x)\mbox{  est le point de $W$ le plus proche de }x.\]
Il nous est donc important d'avoir une formule explicite pour calculer cette projection orthogonale si on veut l'utiliser pour
des calculs effectifs. Les r\'esultats ci-dessous nous livrent une telle formule,
\[ p_W(x)=\sum_i \langle x,v_i\rangle v_i,\] \`a condition de disposer d'une base orthonorm\'ee $(v_1\ldots v_n)$ pour $W$. 
Il nous est donc important de pouvoir construire de telles bases orthonorm\'ees, ce que nous faisons dans le prochain paragraphe.
\subsection{Proc\'{e}d\'{e} d'orthonormalisation de Gram-Schmidt.}
 Soit $(V,\langle, \rangle)$ un espace prehilbertien de dimension finie. On suppose donn\'ee une base pour $V$, ${\bf e}= (e_1,\ldots, e_n)$.
Nous allons construire r\'ecursivement une nouvelle base (orthonorm\'ee) ${\bf v}=(v_1,\ldots v_n)$
\`a partir de ${\bf e}$ par la m\'ethode suivante :

\begin{enumerate}
\item  On pose $v_1= \frac{e_1}{||e_1||}$,
\item On suppose donn\'es $(v_1,\ldots, v_k)$, et on se propose de construire $v_{k+1}$.
\item (Orthogonalisation). On introduit un vecteur auxiliaire $f_{k+1}$ d\'efini par 
\[ f_{k+1}= e_{k+1} -\sum_{i=1}^k \langle e_{k+1}, v_j\rangle  v_j.\]
Par construction $f_{k+1}$ est orthogonal aux vecteurs $v_1,\ldots, v_k$. Il n'est pas, par contre, de longueur $1$.
\item (Normalisation). On pose
\[ v_{k+1}=\frac{f_{k+1}}{|| f_{k+1}||}.\]
\item Nous avons maintenant l'ensemble $(v_1,\ldots, v_{k+1})$. On revient \`a l'\'etape (2) pour construire $v_{k+2}$.
\end{enumerate}
V\'erifions maintenant que cette construction donne bien une base orthonorm\'ee.
\begin{prop}
Les vecteurs de la famille ${\bf v}$ construite ci-dessus forment une base orthonorm\'ee pour $V$.
\end{prop}
\begin{proof}
Il suffira de montrer que cette famille est orthonorm\'ee : elle est alors automatiquement libre et comme elle contient $n={\rm dim } V$ \'el\'ements c'est une base. \\ \\
Il suit directement de $v_{k}=f_{k}/|| f_{k}||$ que $|| v_{k}||=1$. On va montrer par r\'ecurrence sur $k\geq 2$ la proposition
\[ P(k) : \mbox{ les \'el\'ements } (v_1,\ldots v_k)\mbox{ sont orthogonaux deux \`a deux } \forall k\in \{ 2,\ldots, n\}.\]
D\'emontrons d'abord $P(2)$. On doit montrer que $\langle v_1, v_2\rangle =0$. Puisque $\langle v_1, v_2\rangle=\frac{1}{|| e_1||\times || f_2 ||\langle e_1, f_2\rangle$
il suffira de montrer que $\langle e_1, f_2\rangle =0$.
\\ \\
Mais \[\langle e_1, f_2\rangle = \langle e_1, e_2-\langle e_2, v_2\rangle v_1\rangle\]
\[=\langle e_1, e_2\rangle - \langle e_1, \langle e_2, v_1\rangle v_1\langle\]
\[= \langle e_1, e_2\rangle -\langle e_2, v_1\rangle \langle e_1, v_1\rangle\]
\[ =  \langle e_1, e_2\rangle -\frac{\langle e_2, e_1\rangle}{||e_1||} ||e_1||=0.\]
$P(2)$ est donc vrai. 
Supposons que $P(k-1)$ est vraie et d\'eduisons $P(k)$. Il suffira
de d\'emontrer que $\langle v_i, v_k \rangle=0$ pour tout $i<k$. Mais on a 
\[ \langle v_i, v_k \rangle = \frac{1}{||f_k||} \langle v_i, f_k\rangle\]
\[ = \frac{1}{||f_k||} \langle v_i, e_k-\sum_{j=1}^{k-1} \langle e_k, v_j\rangle v_j\rangle\]
\[= \frac{1}{|| f_k||} \left(\langle v_i, e_k\rangle -\sum_{j=1}^{k-1} \langle e_k, v_j\rangle \langle v_i, v_j\rangle\right)\]
et maintenant puisque $\langle v_i, v_j\rangle =0$ si $i\neq j$ et $\langle v_i v_j\rangle = 1$ si $i=j$ on a 
\[\frac{1}{|| f_k||} \left(\langle v_i, e_k\rangle -\sum_{j=1}^{k-1} \langle e_k, v_j\rangle \langle v_i, v_j\rangle\right) = \frac{1}{|| f_k||} (\langle v_i, e_k\rangle - \langle e_k, v_i\rangle)=0.\]
On a donc $P(k)$ pour tout $k\leq n$ : en particulier $P(n)$ est vraie et cette famille est une base orthonorm\'ee. 
\end{proof}



\begin{ex}
On consid\`{e}re la base de $\rr^3$ $$e_1=\begin{pmatrix}1\cr 1\cr 0\end{pmatrix},e_2=\begin{pmatrix}1\cr 0\cr 1\end{pmatrix},e_3=\begin{pmatrix}0\cr 1\cr 1\end{pmatrix}.$$
Appliquons le proc\'{e}d\'{e} de Gram-Schmidt \`{a} cette base afin d'obtenir une base orthonorm\'{e}e pour le produit scalaire. 
On pose $v_1=e_1/|| e_1||= \begin{pmatrix}1/\sqrt{2}\cr 1/\sqrt{2}\cr 0\end{pmatrix}$. On a $$f_2=e_2-\langle v_1,e_{2}\rangle v_1=
\begin{pmatrix}\frac{1}{2}\cr -\frac{1}{2}\cr 1\end{pmatrix}.$$ On pose $$v_2= f_2/|| f_2||= \begin{pmatrix}\frac{1}{\sqrt{6}}\cr -\frac{1}{\sqrt{6}}\cr \frac{2}{\sqrt{6}}\end{pmatrix}.$$
Enfin, $$f_3=e_3-\langle v_1,e_3\rangle}v_1-\langle v_2,e_{3}\rangle v_2= \begin{pmatrix}-2/3\cr 2/3\cr 2/3 \end{pmatrix},$$
et donc \[ v_3= \frac{f_3}{||f_3||}= \frac{\sqrt{3}}{2} \begin{pmatrix}-2/3\cr 2/3\cr 2/3 \end{pmatrix}.\]
On a donc $$v_1=\frac{1}{\sqrt{2}}\begin{pmatrix}1\cr 0\cr 0 \end{pmatrix},v_2=\sqrt{\frac{2}{3}}\begin{pmatrix}1/2\cr 1/2\cr 1\end{pmatrix},v_3= \frac{\sqrt{3}}{2}
\begin{pmatrix}-2/3\cr 2/3\cr 2/3 \end{pmatrix}.$$
\end{ex}
\begin{ex}
Construisons une base orthonorm\'ee pour le plan d'\'equation $x+y+z=0$ dans $\rr^3$. Il a une base non orthonorm\'ee $(e_1, e_2)$ donn\'ee par 
\[e_1=\begin{pmatrix}{1\\-1\\0}\end{pmatrix} , e_2= \begin{pmatrix}{1\\0\\-1}\end{pmatrix}.\]
On pose $v_1= \displaystyle{\frac{e_1}{|| e_1||}}= \begin{pmatrix}{1/\sqrt{2}\\-1/\sqrt{2}\\0}\end{pmatrix}$. On introduit alors 
\[f_2= e_2-\langle v_1, e_2\rangle v_1= e_2-\frac{1}{\sqrt2} v_1= \begin{pmatrix}{1/2\\ 1/2 \\ -1}\end{pmatrix}\] 
et on pose \[v_2= f_2/ || f_2||= \begin{pmatrix}{1/\sqrt{6}\\ 1\sqrt{6} \\ -2/\sqrt{6}}\end{pmatrix}.\] Ceci nous donne la base $(v_1, v_2)$ avec
\[ v_1= \begin{pmatrix}{1/\sqrt{2}\\-1/\sqrt{2}\\0}\end{pmatrix}, v_2= \begin{pmatrix}{1/\sqrt{6}\\ 1\sqrt{6} \\ -2/\sqrt{6}}\end{pmatrix}.\]
\end{ex}
\begin{rem}
Le proc\'{e}d\'{e} de Gram-Schmidt permet de calculer la projection orthogonale de tout vecteur $x\in V$ sur un sous-espace $W$ de dimension finie, en calculant une base orthonorm\'{e}e 
$(v_1,\ldots,v_k)$ de $W$ \`{a} partir d'une base quelconque $e_1,\ldots,e_k$ de $W$ (pour le produit scalaire sur $W$ obtenu par restriction du produit scalaire sur $W$).
On aura alors $$p_W(x)=\sum_{j=1}^k \langle v_j,x\rangle v_j.$$
Rappelons que $p_W(x)$ est le meilleur approximant de $x$ dans $W$. 
\end{rem}
\begin{ex}
Utilisons cette m\'ethode pour construire pour tout $v\in \rr^3$ le point le plus proche de $v$ dans $W$, le plan d'\'equation  $x+y+z=0$. Nous avons vu qu'une base orthonorm\'ee
pour ce plan est donn\'ee par $v_1= \begin{pmatrix}{1/\sqrt{2}\\-1/\sqrt{2}\\0}\end{pmatrix}, v_2= \begin{pmatrix}{1/\sqrt{6}\\ 1\sqrt{6} \\ -2/\sqrt{6}}\end{pmatrix}$.
Soit $v=\begin{pmatrix}{x\\y\\z}\end{pmatrix}$ : on a donc
\[ p_W(v) =\langle v,v_1\rangle v_1+ \langle v, v_2\rangle\]
\[ = \frac{(x-y)}{\sqrt{2}}v_1+ \frac{(x+y-2z)}{\sqrt{6}}v_2\]
\[= \begin{pmatrix}{(x-y)/2\\ (-x+y)/2\\ 0}\end{pmatrix} + \begin{pmatrix}{ (x+y-2z)/6\\ (x+y-2z)/6\\ -2x-2y+4z/6}\end{pmatrix}\]
\[= \begin{pmatrix}{ (2x-y-z)/3\\ (-x+2y-z)/3\\ (-x-y+2z)/3}\end{pamtrix}.\]
\end{ex}
Nous pouvons utiliser ces techniques pour r\'{e}soudre des probl\`{e}mes de minimisation.

\begin{ex}
Consid\'{e}rons le probl\`{e}me suivant. On veut mesurer une donn\'{e}e $y$ (pH d'une solution, temp\'{e}rature) en fonction d'un param\`{e}tre $x$ (concentration d'un ion, temps).
Consid\'{e}rons les $n$ points (avec $n\geq 2$) 
$P_1:=(x_1,y_1),\ldots,P_n:=(x_n,y_n)$ de $\rr^2$ repr\'{e}sentant par exemple le r\'{e}sultat de $n$ exp\'{e}rimentations. On suppose que les $x_i$s sont deux \`a deux distincts. 
Supposons que la th\'{e}orie nous dise que $y$ varie  de fa\c con affine  en fonction de $x$.
A cause des erreurs de manipulation, de mesure, les $n$ points $P_1,\ldots,P_n$ ne sont pas
align\'{e}s.

Comment trouver la droite de meilleure approximation, c'est-\`{a}-dire la droite d'\'{e}quation $y=ax+b$ telle que les points th\'{e}oriques $Q_1:=(x_1,ax_1+b),\ldots,Q_n:=(x_n,ax_n+b)$ soient le plus proche possible des points exp\'{e}rimentaux $P_1,\ldots,P_n$ ?

Plus pr\'{e}cis\'ement, comment choisir la droite $y=ax+b$ telle que l'erreur quadratique $$e:=P_1Q_1^2+\ldots+P_nQ_n^2$$ soit minimale?

On veut donc trouver $(a,b)\in\rr^2$ tels que $$e:=(y_1-(ax_1+b))^2+\ldots+(y_n-(ax_n+b))^2$$ soit minimale.
Posons $$\underline{X}=\begin{pmatrix}x_1\cr\vdots\cr x_n\end{pmatrix},\underline{Y}=\begin{pmatrix}y_1\cr\vdots\cr y_n\end{pmatrix}\mbox{ et }
\underline{U}=\begin{pmatrix}1\cr\vdots\cr 1\end{pmatrix}.$$
On a facilement que  $$\underline{Y}-(a\underline{X}+b\underline{U})=
\begin{pmatrix}y_1-(ax_1+b)\cr\vdots\cr y_n-(ax_n+b)\end{pmatrix},$$ et donc 
$$d=\vert\vert \underline{Y}-(a\underline{X}+b\underline{U})\vert\vert^2,$$
o\`u nous utilisons la norme associ\'ee au produit scalaire canonique sur $\rr^n$.
Soit $W$ le sous-espace vectoriel dans $\rr^n$ form\'e de tous les vecteurs
de la forme $a\underline{X}+ b\underline{U}$ lorsque $(a,b)$ d\'ecrit $\rr^2$. On veut donc minimiser 
$\vert\vert \underline{Y}-w\vert\vert$, lorsque $w$ d\'{e}crit $W$.
D'apr\`{e}s les propri\'{e}t\'{e}s de la projection orthogonale, le minimum est obtenu pour $w=p_W(\underline{Y})$.

On doit donc calculer $p_W(\underline{Y})$. Les coefficients $a$ et $b$ seront alors donn\'{e}s par la relation $$p_W(\underline{Y})=a\underline{X}+b
\underline{U}$$ car $(\underline{X},\underline{U})$ est une base de $W$. 
Posons $$\overline{x}=\frac{x_1+\ldots+x_n}{n}, \overline{y}=\frac{y_1+\ldots+y_n}{n}.$$

Appliquons l'algorithme de Gram-Schmidt \`{a} la base $e_1=\underline{U},
e_2=\underline{X}$ de $W$. On a $v_1=\underline{U}/||\underline{U}||= \frac{1}{\sqrt{n}}\underline{U}$. On a aussi $$f_2=e_2-\langle v_1,e_2\rangle v_1=
\underline{X}-\overline{x}\underline{U}$$ et $v_2= f_2/ || f_2||$. 
On a alors $$\begin{array}{lll} p_W(\underline{Y})&=&\langle v_1,\underline{Y}
\rangle v_1+\langle v_2,\underline{Y}\rangle v_2_2\cr
&=&\ds \overline{y}\underline{U}+ \frac{\ds\sum_{i=1}^n x_iy_i-\overline{x}y_i}{\ds\sum_{i=1}^n (x_i-\overline{x})^2}(\underline{X}-\overline{x}\underline{U}).
\end{array}$$

Remarquons que l'on a $$\sum_{i=1}^n (x_iy_i-\overline{x}y_i)=(\sum_{i=1}^n x_iy_i)-n\overline{x}\,\overline{y}=\sum_{i=1}^n (x_iy_i-\overline{x}\,\overline{y}).$$
On a donc 
 
$$p_W(\underline{Y})=\frac{\ds\sum_{i=1}^n (x_iy_i-\overline{x}\,\overline{y})}{\ds\sum_{i=1}^n (x_i-\overline{x})^2}\underline{X}+\left(\overline{y}-\overline{x}\frac{\ds\sum_{i=1}^n (x_iy_i-\overline{x}\,\overline{y})}{\ds\sum_{i=1}^n (x_i-\overline{x})^2}\right)\underline{U}.$$

Ainsi, la droite de meilleure approximation est donn\'{e}e par $$y=\frac{\ds\sum_{i=1}^n (x_iy_i-\overline{x}\,\overline{y})}{\ds\sum_{i=1}^n (x_i-\overline{x})^2}(x-\overline{x})+
\overline{y}.$$  
c'est-\`a-dire qu'on a $a= \frac{\ds\sum_{i=1}^n (x_iy_i-\overline{x}\,\overline{y})}{\ds\sum_{i=1}^n (x_i-\overline{x})^2}$ et
$b= -\frac{\ds\sum_{i=1}^n (x_iy_i-\overline{x}\,\overline{y})}{\ds\sum_{i=1}^n (x_i-\overline{x})^2}\overline{x}+\overline{y}$.
 \end{ex}

\begin{ex}
On peut aussi vouloir approximer une fonction continue $f:[a,b]\to \rr$ par une fonction affine 
$y=\alpha x+\beta$. Dans ce cas, la m\'{e}thode pr\'{e}c\'{e}dente ne marche plus, puisque l'on doit consid\'{e}rer une infinit\'{e} de points.




L'id\'{e}e est de consid\'{e}rer un grand nombre de points sur le graphe de $f$, dont les abcisses sont r\'{e}guli\`{e}rement espac\'{e}s, $P_1=(x_1,f(x_1)),\ldots,P_n=(x_n,f(x_n))$, avec 
$x_i=\ds a+\frac{(b-a)i}{n}$, et de consid\'{e}rer la droite de meilleure approximation pour ces points. Bien s\^{u}r, plus $n$ est grand, meilleure est l'approximation. L'entier $n$ \'{e}tant fix\'{e}, on doit donc minimiser $$d:=(f(x_1)-(\alpha x_1+\beta))^2+\ldots+(f(x_n)-(\alpha x_n+\beta))^2.$$
Ceci revient aussi \`{a} minimiser $$S_n:=\frac{1}{n}\sum_{i=1}^n (f(x_i)-(\alpha x_i+\beta))^2, \mbox{ avec }x_i=a+\frac{(b-a)i}{n}.$$
On voit graphiquement (et on peut d\'{e}montrer rigoureusement) que $S_n$ converge vers $\ds\int_a^b(f(t)-(\alpha t+\beta))^2{\rm d}t$. En particulier, $S_n$ est tr\`{e}s proche de cette int\'{e}grale lorsque $n$ est suffisamment grand.

Il est alors naturel de d\'{e}finir la droite de meilleure approximation $y=\alpha x+\beta$ comme celle qui minimise l'int\'{e}grale 
$\ds\int_a^b(f(t)-(\alpha t+\beta))^2{\rm d}t$.

Ce genre d'int\'{e}grale s'interpr\`{e}te souvent comme l'\'{e}nergie d'un syst\`{e}me. Ainsi, le probl\`{e}me de minimisation pr\'{e}c\'{e}dent revient \`{a} demander 
de minimiser cette \'{e}nergie.


\begin{ex}
Consid\'{e}rons le probl\`{e}me de minimisation suivant: trouver $a,b\in\rr$ tels que $\ds\int_0^{\frac{\pi}{2}}(\cos(x)-a-bx)^2{\rm d}x$ soit minimale.

Soit $V$ l'espace $C^0([0,\frac{\pi}{2}], \rr)$ avec son produit scalaire
$$\langle, \, \rangle:V\times V\to\rr, (f,g)\mapsto \int_0^{\frac{\pi}{2}}f(x)g(x){\rm d}x.$$
On v\'{e}rifie que $\langle, \, \rangle$ est un produit scalaire sur $V$. Consid\'{e}rons maintenant le sous-espace $W$ de $V$ d\'{e}fini par 
$$W={\rm Vect}(1,x)=\{f| f:x\mapsto  a+bx, a,b\in\rr \}.$$
Le probl\`{e}me de minimisation se reformule alors ainsi:

Trouver $g\in W$ tel que $\langle \cos(x)-g(x),\cos(x)-g(x)\rangle $ soit minimal. Autrement dit, on cherche 
$g\in W$ tel que $\vert\vert\cos(x)-g(x)\vert\vert $ soit minimal.
Cela revient \`{a} dire que $g=p_W(\cos(x))$. On cherche donc \`{a} calculer la projection orthogonale de $\cos(x)$ sur $W={\rm Vect}(1,x)$.

Appliquons le proc\'{e}d\'{e} de Gram-Schmidt \`{a} la base $e_1=1,e_2=x$ de $W$.
On a $v_1=e_1/|| e_1||=\sqrt{\frac{2}{\pi}}$. On pose 
$$f_2=e_2-\langle v_1,e_2\rangle v_1=(x-\frac{\pi}{4})$$ et on prend 
$v_2= (x-\frac{\pi}{4})/|| x-\frac{\pi}{4}||$

On a alors $$g=p_W(\cos(x))=\frac{\langle 1,\cos(x)\rangle}{\langle 1,1\rangle}1+\frac{\langle x-\frac{\pi}{4},\cos(x)\rangle}{\langle x-\frac{\pi}{4},x-\frac{\pi}{4}\rangle}(x-\frac{\pi}{4})=
ax+b$$ avec $a= (\frac{24}{\pi^2}-\frac{96}{\pi^3})$ et $b= (\frac{-4}{\pi}+\frac{24}{\pi^2})$. 
\end{ex}

\begin{ex}
On peut aussi vouloir approximer une fonction $f:[a,b]\to\rr$ par une fonction autre qu'une droite. Par exemple, on peut vouloir approximer $f$ par une fonction $g$ appartenant \`{a} un sous-espace vectoriel $W$ de $C^0([a,b],\rr)$, de fa\c{c}on \`{a} ce que l'int\'{e}grale $$\int_a^b(f(t)-g(t))^2{\rm d}t$$ soit minimale, lorsque $g$ d\'{e}crit $W$.

Consid\'{e}rons le probl\`{e}me pos\'{e} dans l'introduction, celui 
d'approcher une fonction par des sommes trigonom\'etriques. 
Soit $f:[-L,L]\rightarrow \rr$ une fonction que l'on supposera continue : 
on veut approximer $f$ par une 
somme finie de fonctions trigonom\'etriques 
$$S_n(f):=a_0+\sum_{k=1}^n a_k\cos(\frac{2 k\pi x}{L})+
b_k\sin(\frac{2 k\pi x}{L}).$$
On veut trouver les coefficients $a_k$ et $b_k$ tels que l'int\'{e}grale 
$$\int_0^L(f(t)-S_n(t))^2{\rm d}t$$ soit minimale.

Soit $V$ l'espace vectoriel $C^0([-L,L], \rr)$ et
$W$ le sous-espace vectoriel de $V$ engendr\'{e} par $$\frac{1},
\cos(\frac{2 k\pi x}{L}),\sin(\frac{2 k\pi x}{L}),k=1,\ldots,n.$$ Autrement dit, $W$ est l'ensemble de fonctions de la forme
\[ g(x)=a_0+\sum_{k=1}^n a_k\cos(\frac{k\pi x}{L})+
b_k\sin(\frac{k\pi x}{L}).\]

Consid\'{e}rons le produit scalaire $$\langle,\,\rangle:V\to\rr, (f,g)\mapsto \langle f,g\rangle=\int_{-L}^Lf(t)g(t){\rm d}t.$$

Le raisonnement pr\'{e}c\'{e}dent montre que la meilleure approximation
$S_n(f)$ est donn\'{e}e par $p_W(f)$.
Or, on peut v\'{e}rifier que  $$\frac{1}{\sqrt{2L}},
\sqrt{\frac{1}{L}}\cos\left(\frac{2 k\pi x}{L}\right),\sqrt{\frac{1}{L}}\sin\left(\frac{2 k\pi x}{T}\right),k=1,\ldots,n$$
fournit une base orthonorm\'{e}e de $W$ -- nous reviendrons en d\'etail sur ce calcul dans le dernier chapitre.

La formule pour la projection orthogonale $p_W(f)$ nous donne alors
$$p_W(f)=\langle 1,f\rangle\frac{1}{2L}+\sum_{k=1}^n \frac{1}{L}\langle \cos(\frac{k\pi x}{L}),f\rangle \cos(\frac{k\pi x}{L})+
\frac{1}{L}\langle \sin(\frac{ k\pi x}{L}),f\rangle \sin(\frac{ k\pi x}{L})$$
$$= \frac{1}{2L}\int_{-L}^L f(t)dt+ \frac{1}{L}\int_{-L}^L f(t)\cos(\frac{k\pi t}{L}){\rm d}t  \cos(\frac{k\pi x}{L})+ 
\frac{1}{L}\int_{-L}^L f(t)\sin(\frac{ k\pi t}{L}){\rm d}t \sin(\frac{ k\pi x}{L}).\]
Les choix de coefficients $a_0, a_k, b_k$ qui minimisent cet int\'egrale sont donc donn\'es par 
$$a_0= \frac{1}{2L}\int_{-L}^L f(t)dt$$
$$a_k=\frac{1}{L}\int_{-L}^L f(t)\cos(\frac{ k\pi t}{L}){\rm d}t,$$
$$b_k=\frac{1}{L}\int_{-L}^L f(t)\sin(\frac{ k\pi t}{L}){\rm d}t.$$

\end{ex}


\subsection{Diagonalisation orthogonale des matrices sym\'etriques.}
Nous pr\'esentons ici un th\'{e}or\`{e}me sur la
 diagonalisation des matrices sym\'{e}triques, que vous avez \'etudi\'ee en
MAT234. \\ \\
On commence par un lemme.
\begin{lem}
Soit $(V,\langle,\rangle)$ un espace prehilbertien de dimension $n$, et soit ${\bf e}=(e_1,\ldots,e_n)$ une base 
orthonorm\'{e}e. Soit ${\bf v}=(v_1,\ldots,v_n)$ une autre base de $V$, et soit $P$ la matrice de passage 
correspondante (c'est-\`{a}-dire la matrice dont les colonnes sont les vecteurs coordonn\'{e}es de $(v_1,\ldots,v_n)$ 
dans la base $(e_1,\ldots,e_n)$). La base $(v_1,\ldots,v_n)$ est orthornorm\'{e}e si et seulement si ${}^tP P=I_n$, c'est-\`a-dire si
$P^'-1}= {}^tP$. 

\end{lem}

\begin{proof}

Soient $M$ et $N$ les matrices de la forme $\langle, \rangle$ dans les 
bases ${\bf e}$ et ${ \bf v}$. On sait que 
$N= {}^tP M P$ : puisque ${\bf e}$ est suppos\'ee orthonorm\'ee nous avons $M=I_n$ et $N= {}^tPP$. 
La base ${\bf v}$ est orthonorm\'ee si et seulement si $N=I_n$ c'est \`a dire ssi  
\[ I_n= {}^tP P.\]
CQFD.
\end{proof}


\begin{thm}
Soit $B\in \M_n(\rr)$ une matrice sym\'{e}trique, c'est-\`{a}-dire v\'{e}rifiant ${}^tB=B$.
Alors il existe une base de $\rr^n$ form\'{e}e de vecteurs 
propres de $B$ qui est orthonorm\'{e}e pour le produit scalaire usuel sur $\rr^n$.
\end{thm}
La d\'emonstration repose sur le crit\`ere suivant.
\begin{lem}
Soient $B$ une matrice carr\'ee $n\times n$. Alors si $B$ est sym\'etrique on a pour tout $\underline{X},\underline{Y}\in \rr^n$
\[ \langle \underline{X}, B\underline{Y}\rangle = \langle B\underline{X},\underline{Y} \rangle.\]
ou $\langle, \rangle$ est le produit scalaire canonique.
\end{lem}
\begin{proof}
On a que
\[\langle \underline{X}, B\underline{Y}\rangle=  {}^t\underline{ X} B\underline{Y}= {}^t(^tB \underline{X}) \underline{Y}= {}^t(B\underline{X})\underline{ Y}= \langle B\underline{X}, \underline{Y}\rangle\]
 CQFD.
\end{proof}
\begin{rem}\label{orthvp}
En particulier, si $B\underline{X}= \lambda \underline{X}$ (c'est-\`a-dire 
$\underline{X}$ est un vecteur propre de $B$) et 
$B\underline{Y}=\mu \underline{Y}$ alors 
\[ \lambda \langle \underline{X}, \underline{Y}\rangle= 
\mu \langle \underline{X},\underline{Y}\rangle\]
et si $\lambda\neq \mu$ alors $\underline{X}$ et $\underline{Y}$ doivent 
\^etre orthgonaux
\end{rem}
{\bf Preuce du Th\'eor\`eme 4.28}\\ \\
Vous avez  
vu dans MAT234 que toute matrice sym\'etrique r\'eelle est diagonalisable sur 
$\rr$. 
Soient $\lambda_1,\ldots, \lambda_k$ ses valeurs propres distincts avec $E_{\lambda_i}$ le sous-espace propre associ\'e \`a $\lambda_i$. Nous avons alors
\[ \rr^n = E_{\lambda_1} \underset{\perp}{\oplus} \ldots 
\underset{\perp}{\oplus} E_{\lambda_k},\]
et par la remarque \ref{orthvp}, $E_{\lambda_i}$ est orthogonal \`a $E_{\lambda_j}$ si $i\neq j$. Pour tout $i$ soit
${\bf e_i}$ une base orthonorm\'ee pour $E_{\lambda_i}$ et soit ${\bf e}$ la concat\'enation $({\bf e}_1, \ldots, {\bf e}_k)$. Par le Lemme \ref{orthbase}, ${\bf e}$ 
est une base orthonorm\'ee pour $\rr^n$ compos\'ee de vecteurs propres de $B$.
\end{proof}

%\begin{proof}
%On le d\'{e}montre uniquement pour les formes semi-lin\'{e}aires hermitiennes, la d\'{e}monstration pour les formes bilin\'{e}aires sym\'{e}triques \'{e}tant similaire.

%$(a)$ Montrons que toutes les valeurs propres de $H$ sont r\'{e}elles. Soit $\lambda\in\cc$ une valeur propre de $H$, et soit $V$ un vecteur propre associ\'{e}.
%On a $HV=\lambda V$, et donc $$\ov{H}\,\ov{V}=\ov{HV}=\ov{\lambda V}=\ov{\lambda}\, \ov{V}.$$
%On a alors $$\ov{HV}^tV=\ov{\lambda V}^tV=\ov{\lambda}\,\ov{V}^tV.$$
%Mais on a aussi  $$\ov{HV}^tV=\ov{V}^t \ov{H}^t V=\ov{V}^t HV=\ov{V}^t \lambda V=\lambda \ov{V}^t V.$$
%On a donc $$\ov{\lambda}\,\ov{V}^tV=\lambda\ov{V}^tV.$$

%Or, si $V=\begin{pmatrix}v_1\cr \vdots\cr v_n\end{pmatrix}$, on a 
%$$\ov{V}^t V=\sum_{i=1}^n \ov{v}_i v_i=\sum_{i=1}^n \vert v_i\vert^2>0,$$
%puisque $V$ a au moins une coordonn\'{e}e non nulle par d\'{e}finition.

%On en d\'{e}duit $\ov{\lambda}=\lambda$, c'est-\`{a}-dire $\lambda\in\rr$.

%Avant de d\'{e}montrer le reste du th\'{e}or\`{e}me, nous allons \'{e}tablir quelques r\'{e}sultats auxiliaires.

%$(b)$ Pour tout $X,Y\in\cc^n$, on a $$\langle HX, Y\rangle=\langle X,HY\rangle.$$

%En effet, on a $$\langle HX, Y\rangle=\ov{HX}^t Y=\ov{X}^t\ov{H}^t Y=\ov{X}^t HY=\langle X,HY\rangle.$$

%$(c)$ Soit $\lambda\in\rr$ une valeur propre de $H$. Soit $$E_\lambda=\{ V\in \cc^{n} \mid HV=\lambda V \}.$$
%Si $V\in E_\lambda^\perp$, alors $HV\in E_\lambda^\perp$.

%En effet, si $V\in E_\lambda^\perp$, alors pour tout $X\in E_\lambda$, on a 
%$$\langle X,HV\rangle=\langle HX, V\rangle=\langle \lambda X, V\rangle= \ov{\lambda}\langle X,V\rangle=0.$$

%On a donc $HV\in E_\lambda^\perp$.

%$(d)$ On d\'{e}montre maintenant la propri\'{e}t\'{e} d\'{e}sir\'{e}e par r\'{e}currence sur $n$.

%Soit $(P_n)$ la propri\'{e}t\'{e}

%$(P_n)$ Pour tout $1\leq k\leq n$, et toute matrice hermitienne de taille $k\times k$, il existe une 
%base de $\cc^n$ form\'{e}e de vecteurs propres de $H$ qui est orthonorm\'{e}e pour le produit scalaire usuel sur $\cc^n$.

%Supposons que $n=1$. Alors tout vecteur de norme $1$ convient. 

%Supposons que la propri\'{e}t\'{e} soit vraie pour toute matrice hermitienne de taille $k\times k, k\leq n$, et d\'{e}montrons-le pour les matrices hermitiennes de taille $(n+1)\times (n+1)$.

%Soit $H\in {\rm M}_{n+1}(\cc)$ une matrice hermitienne. Soit $\lambda\in\rr$ une valeur propre de $H$.
%Si $H=\lambda I_n$, la base canonique convient. Supposons maintenant que $H\neq \lambda I_n$.

%D'apr\`{e}s le Th\'{e}or\`{e}me \ref{thmprojorth}, on a 
%$$\cc^{n+1}=E_\lambda\overset{\perp}{\oplus}E_\lambda^\perp.$$

%On sait que $E_\lambda$ poss\`{e}de une base orthonorm\'{e}e. Par d\'{e}finition de $E_\lambda$, cette base est constitu\'{e}e de vecteurs propres de $H$ (associ\'{e}s \`{a} la valeur propre $\lambda$).

%Pour conclure, il reste \`{a} d\'{e}montrer que $E_\lambda^\perp$ poss\`{e}de une base orthonorm\'{e}e form\'{e}e de vecteurs propres de $H$, puisqu'il suffira alors de recoller les deux bases pour obtenir la base de $\cc^{n+1}$ d\'{e}sir\'{e}e.

%Soit $e_1,\ldots,e_k$ une base orthonorm\'{e}e de $E_\lambda$, et soit $e_{k+1},\ldots,e_{n+1}$ une base orthonorm\'{e}e de $E_\lambda^\perp$. Alors ${\bf e}=(e_1,\ldots,e_n)$ est une base orthonorm\'{e}e de $\cc^{n+1}$. Attention! Les vecteurs $e_{k+1},\ldots,e_{n+1}$ ne sont pas a priori des vecteurs propres de $H$.

%Soit $P$ la matrice de passage de {\bf e} \`{a} la base canonique, et soit $f:\cc^{n+1}\to \cc^{n+1}$ l'application lin\'{e}aire d\'{e}finie par $$f(X)=HX\mbox{ pour tout }X\in\cc^{n+1}.$$

%La matrice repr\'{e}sentative de $f$ dans la base canonique est $H$, et donc $$H':={\rm Mat}(f, {\bf e})=P^{-1}HP.$$
%D'apr\`{e}s le lemme pr\'{e}c\'{e}dent, puisque ${\bf e}$ est orthonorm\'{e}e, on a $P^{-1}=\ov{P}^t$.

%Ainsi, on a $H'=\ov{P}^t H P$. On a donc $$\ov{H'}^t= \ov{\ov{P}^t H P}=\ov{P}^t \ov{H}^t P=\ov{P}^t H P=H'.$$
%Ainsi, $H'$ est hermitienne. Mais par construction de {\bf e}, on a aussi 

%$$H'=\begin{pmatrix}\lambda I_k & 0\cr 0 & M\end{pmatrix},$$
%puisque $E_\lambda^\perp$ est stable par $f$. Remarquons que $M$ est la matrice repr\'{e}sentative de l'application lin\'{e}aire 

%$$g:E_\lambda^\perp\to E_\lambda^\perp, V\mapsto HV$$
%dans la base $e_{k+1},\ldots,e_{n+1}$.

%En utilisant le fait que $H'$ est hermitienne, on voit facilement que $M$ est \'{e}galement hermitienne, de taille $(n+1-k)\times (n+1-k)$. 
%Puisque $H\neq \lambda I_n$, on a $E_\lambda\neq \cc^{n+1}$, et donc on a $$\dim_{\cc}(E_\lambda^\perp)=n+1-
%\dim_{\cc}(E_\lambda)=n+1-k<n+1.$$ Par hypoth\`{e}se de r\'{e}currence, il existe une base de $\cc^{n+1-k}$ form\'{e}e de vecteurs propres de $M$ qui soit orthonorm\'{e}e pour le produit scalaire usuel.
%Cela signifie exactement qu'il existe une base de $E_\lambda^\perp$ form\'{e}e de vecteurs propres de $g$ qui soit orthonorm\'{e}e pour le produit scalaire. Or un vecteur propre de $g$ est aussi un vecteur propre de $H$. Ceci ach\`{e}ve la d\'{e}monstration.
%\end{proof}

Ceci se traduit en termes de formes bilin\'{e}aires de la fa\c{c}on suivante:

\begin{thm}
\trick

Soit $(V,\langle, \rangle)$ un espace prehilbertien de dimension finie, et soit 
$\varphi :V\times V\to \rr$ une forme bilin\'{e}aire sym\'{e}trique.
Alors il existe une base orthonorm\'{e}e pour $\langle, \rangle$ qui est
 aussi $\varphi$-orthogonale.
\end{thm}

\begin{proof}

Soit ${\bf e}=(e_1,\ldots,e_n)$ orthonorm\'{e}e pour $\langle, \,\rangle$, et soit $B$ sa matrice dans cette base. Alors $B$ est une matrice sym\'etrique d'apr\`{e}s le Lemme \ref{corosym}.

D'apr\`{e}s le th\'{e}or\`{e}me pr\'{e}c\'{e}dent, il existe une base $(\underline{V}_1,\ldots,\underline{V}_n)$ de $\rr^n$ form\'{e}e de 
vecteurs propres de $B$ qui est orthonorm\'{e}e pour le produit scalaire usuel de $\rr^n$.

Si $\underline{V}_j=\begin{pmatrix} v_{1j}\cr\vdots\cr v_{nj}\end{pmatrix}$, posons $v_j=\sum_{i=1}^n v_{ij}e_i,$ de telle
fa\c con que $\underline{V}_j$ est le vecteur de coordonn\'ees de $v_j$ dans la base ${\bf e}$.

Nous allons montrer que ${\bf v}=(v_1,\ldots,v_n)$ est une base de $V$ qui poss\`{e}de les propri\'{e}t\'{e}s voulues.

Puisque {\bf e} est orthonorm\'{e}e, on a $$\langle v_i, v_j\rangle={}^t
\underline{V}_i \underline{V}_j$$ d'apr\`{e}s
le Lemme \ref{calculorth}.
Par choix de $\underline{V}_1,\ldots, \underline{V}_n$, on en d\'{e}duit que $$\langle v_i,v_j\rangle=0\mbox{ si }i\neq j$$ et 
$$\langle v_i,v_i\rangle=1\mbox{ pour tout }i.$$

Il reste \`{a} voir que {\bf v} est $\varphi$-orthogonale. Soit $P$ la matrice de passage de {\bf v} \`{a} {\bf e}.
La matrice $N$ qui repr\'esente $\varphi$ dans la base ${\bf v}$ est donc  ${}^tP B P.$
Or {\bf v} \'{e}tant orthonorm\'{e}e, on a ${}^tP P=I_n$. On a ainsi $N=P^{-1} B P.$\\ \\
Mais {\bf v} \'{e}tant form\'{e}e de vecteurs propres de $B$, nous avons que $P^{-1}BP$ est diagonale. $N$ est donc 
est diagonale, ce qui revient \`{a} dire que {\bf v} est $\varphi$-orthogonale.
\end{proof}
Cette d\'emonstration nous donne en plus que nous pouvons construire une telle base orthonorm\'ee et 
$\varphi$-orthogonale en prenant des vecteurs propres de $B$.
vecteur propres de $B$.


{\bf M\'{e}thode pratique pour trouver une base de vecteurs orthonorm\'ee 
et $\varphi$-orthogonale. }

$(1)$ Soit $M$ une matrice sym\'{e}trique r\'{e}elle.

- Pour chaque valeur propre $\lambda\in\rr$ de $M$, on calcule une base de $E_\lambda$, puis on applique l'algorithme de Gram-Schmidt pour obtenir une base orthonorm\'{e}e de $E_\lambda$. 

- On recolle les bases orthonorm\'{e}es  pr\'{e}c\'{e}dentes pour obtenir une base $(\underlne{V}_1,\ldots,\underline{V}_n)$  de $\rr^n$ form\'{e}e de vecteurs propres de $M$, orthonorm\'{e}e {\bf pour le produit scalaire usuel} sur $\rr^n$.


$(2)$ Soit $\varphi:V\times V\to \rr$ une forme bilin\'{e}aire 
sym\'{e}trique. On se fixe une base {\bf e} de $V$ {\bf orthonorm\'{e}e} pour 
$\langle, \, \rangle$. 


 Soit $M$ la matrice de $\varphi$ dans la base ${\bf e}$. $M$ est une matrice sym\'{e}trique. On applique la m\'{e}thode pr\'{e}c\'{e}dente pour obtenir une base 
$(\underline{V}_1,\ldots,\underline{V}_n)$ orthonorm\'{e}e de $\rr^n$ form\'{e}e de vecteurs propres de $M$.
On prend alors $v_i$ l'unique vecteur dans $V$ qui admet pour vecteur coordonn\'ees dans la base ${\bf e}$ le vecteur 
$\underline{V}_i$. La base $(v_1,\ldots, v_n)$ est alors la base 
recherch\'ee.



\begin{exs}

\trick

$(1)$ Soit $B=\begin{pmatrix}3 & 4 \cr 4 & -3\end{pmatrix}$.

On v\'{e}rifie que les valeurs propres sont $5$ et $-5$, et que $$E_5={\rm Vect}\{\begin{pmatrix}2\cr 1\end{pmatrix}\},
E_{-5}={\rm Vect}\{\begin{pmatrix}1\cr -2\end{pmatrix}\}.$$

Une base orthonorm\'{e}e pour $E_5$ est donc $$\frac{1}{\sqrt{5}}\begin{pmatrix}2\cr 1\end{pmatrix},$$
et une base orthonorm\'{e}e pour $E_{-5}$ est donc $$\frac{1}{\sqrt{5}}\begin{pmatrix}1\cr -2\end{pmatrix}.$$

La base recherch\'{e}e est donc donn\'{e}e par $$(\frac{1}{\sqrt{5}}\begin{pmatrix}2\cr 1\end{pmatrix},\frac{1}{\sqrt{5}}\begin{pmatrix}1\cr -2\end{pmatrix}.)$$


$(2)$ Munissons $\rr^3$ de son produit scalaire usuel, et soit $$\varphi:\rr^3\times \rr^3\to \rr, \left(\begin{pmatrix}x_1\cr\vdots\cr x_3\end{pmatrix},\begin{pmatrix}y_1\cr\vdots\cr y_3\end{pmatrix}\right)\mapsto \sum_{i,j\leq 3}x_iy_j.$$

Soit {\bf e} la base canonique de $\rr^3$. C'est une base orthonorm\'{e}e pour le produit scalaire usuel.
La matrice $M$ de $\varphi$ dans la base canonique est alors

$$M=\begin{pmatrix}1&1&1\\1&1&1\\1&1&1\end{pmatrix}.$$ 


On v\'{e}rifie que les valeurs propres sont $3$ et $0$, que $E_3$ admet comme base la famille $(\begin{pmatrix}1\cr 1\cr 1\end{pmatrix})$
et que $E_{0}$ admet comme base la famille \[\left(\begin{pmatrix}1\cr -1\cr 0\end{pmatrix},\begin{pmatrix}1\cr 0\cr -1\end{pmatrix}\right).$$

Une base orthonorm\'{e}e pour $E_1$ est donc $$\frac{1}{\sqrt{3}}\begin{pmatrix}1\cr 1\cr 1\end{pmatrix}.$$

Pour trouver une base orthonorm\'{e}e de $E_0$, on applique Gram-Schmidt. On pose $v_1=\displaystyle{\frac{1}{\sqrt{2}}}\begin{pmatrix}1\cr -1\cr 0\end{pmatrix}$.
Ensuite on pose $$f_2=\begin{pmatrix}1\cr 0\cr -1\end{pmatrix}-\frac{1}{2}\begin{pmatrix}1\cr -1\cr 0\end{pmatrix}=\begin{pmatrix}1/2\cr 1/2\cr -1\end{pmatrix}.$$ Enfin on pose 
$v_2= f_2/|| f_2||= \displaystyle{\sqrt{\frac{2}{3}}}\begin{pmatrix}1/2\cr 1/2\cr -1\end{pmatrix}$

Une base orthonorm\'{e}e pour $E_{0}$ est donc $$\left(\frac{1}{\sqrt{2}}\begin{pmatrix}1\cr -1\cr 0\end{pmatrix},\sqrt{\frac{2}{3}}\begin{pmatrix}\frac{1}{2}\cr \frac{1}{2}\cr -1\end{pmatrix}\right).$$

La base recherch\'{e}e est donc donn\'{e}e par 
$$v_1=\frac{1}{\sqrt{3}}\begin{pmatrix}1\cr 1\cr 1\end{pmatrix},v_2=\frac{1}{\sqrt{2}}\begin{pmatrix}1\cr -1\cr 0\end{pmatrix},v_3=\sqrt{\frac{2}{3}}\begin{pmatrix}\frac{1}{2}\cr \frac{1}{2}\cr -1\end{pmatrix}.$$

Si $x=x'_1v_1+x'_2v_2+x'_3v_3$ et $y=xy'_1v_1+y'_2v_2+y'_3v_3$, on a $$b(x,y)=3x'_3y'_3.$$
\end{exs}
\subsection{Matrices orthogonales et unitaires.}
Nous avons vu ci-dessus que les matrices r\'eelles $M$ 
telles que ${}^tM M =I_n$, ou les matrices complexes telles que 
${}^t\overline{M}M=I_n$, sont tr\`es importantes puisqu'elle encodent des changements de la bases orthonorm\'ees. 
Nous allons maintenant \'etudier ces matrices en d\'etail.
\begin{defn}
Soit $M$ une matrice r\'eelle de taille $n\times n$.  Les conditions suivantes sont \'equivalentes :
\begin{enumerate}
\item ${}^tMM= I_n$ ;
\item pour tous $v,w\in \rr^n$ nous avons  
$\langle Mv, Mw\rangle= \langle v,w\rangle$, o\`u $\langle, \rangle$ est
le produit scalaire canonique.
\item pour tout $v\in \rr^n$ nous avons $|| Mv||= ||v||$, o\`u 
$||v||$ est 
la norme de $v$ pour le produit scalaire canonique.
\end{enumerate}
On dit qu'une matrice qui satisfait \`a ces conditions est {\it orthogonale.}
\end{defn}
\begin{proof}
Si ${}^tM M= I_n$ alors 
\[\langle Mv,Mw \rangle = {}^t(Mv) Mv = {}^tv{}^tM M w= {}^tv I_n w= {}^tv w= 
\langle v,w\rangle.\] 
Donc (1) implique (2).
(2) implique (3) en prenant $v=w$ et 
(3) implique (2) par la formule de polarisation. 
Reste \`a montrer que 2) implique 1). Si 
$\langle Mv, Mw\rangle= \langle v,w\rangle$ alors pour tout $v, w\in \rr^n$ nous avons
\[ {}^tv {}^tM M w= {}^tv I_n w={}^t v w=\langle v,w\rangle\] et donc 
${}^tM M= I_n$. 
\end{proof}
On peut bien s\^ur faire la m\^eme chose pour des matrices complexes.
\begin{defn}
Soit $M$ une matrice complexe de taille $n\times n$.  Les conditions suivantes sont \'equivalentes :
\begin{enumerate}
\item ${}^t\overline{M} M= I_n$ ;
\item pour tous $v,w\in \cc^n$ nous avons 
$h(Mv, Mw)= h(v,w)$ o\`u $h$ est le produit hermitien
canonique ;
\item pour tout $v\in \cc^n$ nous avons  $|| Mv||= ||v||$, o\`u $|| v||$ est la 
norme associ\'ee au produit hermitien canonique. 
\end{enumerate}
On dit qu'une matrice 
qui satisfait \`a ces conditions est {\it unitaire.}
\end{defn}
\begin{proof}
Si ${}^t\overline{M} M= I_n$ alors 
\[h(Mv,Mw)= {}^t\overline{(Mv)} Mv = {}^t\overline{v} {}^t\overline{M} M w= 
{}^t\overline{v} I_n w= {}^t\overline{v}= w = h(v,w).\] Donc (1) implique (2).
(2) implique (3) en prenant $v=w$ et (3) implique (2) par la formule de polarisation. Reste \`a montrer que (2) implique (1). Si 
$h(Mv, Mw)= h(v,w)$ alors pour tout $v, w\in \cc^n$ nous avons que 
\[ {}^t\overline{v}{}^t\overline{ M} M w= {}^t\overline{v} I_n w\] et donc 
${}^t\overline{M} M= I_n$. 
\end{proof}
Nous finissons cette section avec une \'etude des matrices orthogonales de taille $2\times 2$. Nous allons d\'emontrer le th\'eor\`eme suivant:
\begin{prop}
Soit $M$ une matrice $2\times 2$ orthogonale. Alors l'application $\rr^2\mapsto \rr^2$ donn\'ee par $v\mapsto Mv$ est 
\begin{enumerate}
\item une rotation autour de l'origine 
\item* ou
\item une sym\'etrie par rapport \`a une droite passant par l'origine.
\end{enumerate}
\end{prop}
\begin{proof}
Soit $M=\begin{pmatrix}{ a & b\\ c & d}\end{pmatrix} $ une matrice orthogonale. On  a alors
\[ || M\begin{pmatrix}{1\\0}\end{pmatrix}|| = || 
\begin{pmatrix}{a\\c}\end{pmatrix}|| =1\]
et, en utilisant des coordonn\'ees polaires, il existe un $\theta$ tel que $\begin{pmatrix}{a\\c\end{pmatrix}= \begin{pmatrix}{\cos(\theta)\\ \sin(\theta)}\end{pmatrix}$. De m\^eme 
$\begin{pmatrix}{b\\d}\end{pmatrix}= \begin{pmatrix}{\cos(\phi)\\ \sin(\phi)}\end{pmatrix}$ et on peut \'ecrire
\[ M=\begin{pmatrix}{ \cos{\theta} & \cos{\phi}\\ \sin{\theta} & \sin{\phi}}\end{pmatrix}.\]
On a alors
\[ {}^tM M= \begin{pmatrix}{ \cos^2{\theta}+\sin^2{\theta} & \cos{\theta}\cos{\phi}+ \sin \theta\sin \phi\\ \cos{\theta}\cos{\phi}+ \sin \theta\sin\phi &  \cos^2{\phi}+\sin^2{\phi}}\end{pmatrix}\]
\[= \begin{pmatrix}{ 1& \cos(\theta -\phi)\\ \cos(\theta-\phi)&  1}\end{pmatrix}\]
et nous avons donc $M$ orthogonale si et seulement si $\cos(\theta-\phi)= 0$, c'est-\`a-dire si et seulement si \[\phi= \theta +\pi/2\mbox{ ou }\phi= \theta -\pi/2.\] 
Dans le premier cas nous avons 
\[ M=\begin{pmatrix}{ \cos{\theta} & -\sin\theta \\ \sin\theta 
& \cos\theta}\end{pmatrix}\]
et on reconnait la matrice d'une rotation d'angle $\theta$ autour de l'origine. 
Dans le deuxi\`eme cas on a 
\[ M=\begin{pmatrix}{ \cos{\theta} & +\sin \theta \\ \sin\theta & -\cos{\theta}}\end{pmatrix}.\]
Apr\`es calcul le polyn\^ome caract\'eristique de $M$ est $\lambda^2-1$,
qui a pour solutions $1$ et $-1$. Apr\`es calcul des vecteurs propres par 
pivot de Gauss, on voit que
\[e_1= \begin{pmatrix}{\cos{\theta/2}\\ \sin{\theta/2}}\end{pmatrix}, e_2= \begin{pmatrix}{-\sin{\theta/2}\\ \cos(\theta/2)\end{pmatrix}\]
sont des vecteurs propres de $M$ de valeur propre $1$ et $-1$ respectivement.
Autrement dit, on a $M e_1=e_1$ et $Me_2= -e_2$. Puisque
$\langle e_1, e_2\rangle=0$, ou autrement dit les vecteurs $e_1$ et $e_2$ sont perpendiculaires, $M$ repr\'esente une sym\'etrie par rapport \`a  la droite engendr\'ee par 
$e_1$. \\ \\

\section{S\'{e}ries de Fourier. }

Nous allons maintenant revenir sur la question pos\'ee en d\'ebut de
semestre. Rappelons que nous cherchions \`a r\'esoudre l'\'equation de la chaleur :
\[ \frac{\partial T}{\partial t}= D\frac{\partial^2 T}{\partial x^2}\]
sur le domaine $\{(x,t)|x\in[0,L], t\geq 0}\}$ 
 en respectant les conditions initiales
\[T(x,0)= \phi(x)\]
(ou $\phi$ est une fonction donn\'ee) 
et les conditions aux bords 
\[ \frac{\partial T}{\partial x}(0, t)=\frac{\partial T}{\partial x}(L, t)=0\]
pour tout $t>0$. 
Nous avions remarqu\'e que lorsque la condition initiale $\phi$ \'etait une somme
finie de cosinus
\[ \phi=a_0+\sum_{k=1}^n a_k \cos(k\pi x/L)\]
cette \'equation poss\`ede une solution
\[ T(x,t)=  a_0+ \sum_{k=1}^n a_k \cos(k\pi x/L) e^{-Dk^2\pi^2 x/L^2}.\]
Nous allons maintenant chercher \`a r\'esoudre cette \'equation pour une
condition initiale $\phi$ quelconque en approchant $\phi$ par
des sommes finies de la forme
\[ \phi=a_0+\sum_{k=1}^n a_k \cos(k\pi x/L).\]
Pour des raisons t\'echniques, il s'av\`ere \^etre plus utile de 
d\'evelopper d'abord une m\'ethode
pour approcher une fonction quelconque $\phi$
par une somme trigonom\'etrique - c'est \`a dire, une fonction $g$ de la forme
\[ g(x)= a_0+\sum_{k=1}^n a_k \cos(k\pi x/L)+b_k\sin( k\pi x/L)\]
 quitte \`a s'assurer plus tard par un astuce 
que les coefficients de sin sont nuls.
\subsection{Approximants  de Fourier, coefficients de Fourier et
s\'eries de Fourier : d\'efinitions et exemples.}
Dans ce paragraphe, nous allons appliquer la m\'ethode de la projection 
orthogonale pour approcher une fonction par une somme trigonom\'etrique. \\ \\
Supposons donn\'ee une fonction $f$, continue, r\'eelle et d\'efinie sur 
une intervalle $[-L, L]$. On cherche \`a approcher $f$ 
par une somme trigonom\'etrique
\[ a_0+\sum_{k=1}^n a_k \cos(k\pi x/L)+ b_k\sin(k\pi x/L).\]
Pour chaque $n$, nous allons consid\'erer la fonction $S^{\rr}_n(f)$, qui sera le meilleur
approximant de $f$ de la forme 
\[a_0+\sum_{k=1}^n a_k \cos\left(k\pi x/L\right)+
b_n\sin\left(k\pi x/L\right)\]
par rapport \`a la distance d\'efinie par le produit scalaire
\[ \langle f,g\rangle = \int_{-L}^L f(x)g(x) dx.\]
Dans ce chapitre nous aurons parfois besoin de travailler avec des extensions 
d'une fonction  continue $f$ qui peuvent avoir un nombre fini de points de 
discontinuit\'e. Il nous est donc n\'ecessaire  de pouvoir travailler avec des
fonctions qui ne sont pas continues mais qui ont une nombre fini de ``sauts''.
\begin{defn}

Soit $i\geq 0$ un entier.

Une fonction $f:[a,b]\to\cc$ est dite $C^i$ {\bf par morceaux} s'il existe \[a=a_0<a_1<\ldots<a_p=b\] tels que 
\begin{enumerate}
\item la fonction $f$ est de classe $C^i$ sur chaque intervalle 
$]a_{k-1},a_k[$. 
\item Pour tout
$m=0\ldots i$ et tout $j=0\ldots p$ les limites \`a gauche et \`a droite
\[ \lim_{x\rightarrow a_j^{-}} f^{(m)}(x)\]
et \[\lim_{x\rightarrow a_j^{-}} f^{(m)}(x)\]
existent et sont finies.
\end{enumerate}
Une fonction $f:\rr\to \cc$ est dite $C^i$ par morceaux si elle est $C^i$ par morceaux sur $[-n,n]$ pour tout $n$.\\ \\
Une fonction $f:[a,b]\rightarrow \cc$ 
est $C^\infty$ par morceaux si elle est $C^i$ par morceaux pour tout $i\geq 0$.  On note l'espace vectoriel de toute les fonctions r\'eelles (resp. complexes) $C^i$ par morceaux sur une
intervalle $[a,b]$ par $C^i_{\rm mor}([a,b], \rr)$
(resp. $C^i_{\rm mor}([a,b], \cc)$.)
\end{defn}

\begin{rem}
Si $f:[a,b]\to\cc$ est continue par morceaux, alors $\int_a^b f(x){\rm d}x$ existe. Si $a=a_0,a_1,\ldots,a_p=b$ est la subdivision correspondante alors 
on a $$\int_a^b f(x){\rm d}x=\sum_{j=0}^{p-1}\int_{a_j}^{a_{j+1}} f(x){\rm d}x.$$
En particulier, on peut donc d\'efinir sur  $C^i_{\rm mor}([a,b], \rr)$ le produit scalaire
\[\langle f,g\rangle =\int_{-L}^L f(x) g(x) dx\]
\end{rem}
Nous d\'efinissons donc
\begin{defn}
Soit $f: [-L, L]\rightarrow \rr$ une fonction continue par morceaux. Nous consid\'erons $C^0_{\rm mor}([-L, L], \rr)$ avec son produit scalaire
\[ \langle f,g\rangle =\int_{-L}^L f(x)g(x)dx.\]Pour tout $n$ soit \[W_n\subset C^0_{\rm mor}([-L, L], \rr)\] le sous-espace de fonctions $g(x)$ de la forme
\[ g(x)= a_0+\sum_{k=1}^n a_k \cos(k\pi x/L)+ b_k \sin(k\pi x/L).\]
Le $n$-i\`eme approximant trigonometrique de Fourier de $f$, not\'e 
$S^{\rr}_n(f)$, est alors d\'efini par 
\[ S^{\rr}_n(f)= p_{W_n}(f).\]
Autrement dit,
 $S^{\rr}_n(f)$ est la fonction dans $W_n$ qui minimise la distance 
euclidienne \[d(S_n^{\rr}(f), f)=\sqrt{\int_{-L}^L (S_n^{\rr}(f)(x)-f(x))^2dx}.
\end{defn}
Nous allons maintenant proc\'eder au calcul explicit des approximants de Fourier. Il faut pour cela commencer par identifer une base orthonomr\'ee de $W_n$.  Par d\'efinition de cet espace, 
\[ B=(1, \cos(\pi x/L), \sin(\pi x/L), \cos(2\pi x/L)\ldots, \cos(n\pi x/L), \sin(n\pi x/L))\] est une
base pour $W_n$  
\begin{lem}
Nous avons que la base $B$ est une base orthogonale pour $W_n$ par rapport au produit
scalaire $\langle f,g\rangle =\int_{-L}^L f(x)g(x) dx.$
\end{lem}
\begin{proof}
Il nous faut d\'emontrer que
\begin{enumerate}
\item $\int_{-L}^L 1 \sin (k\pi x/L)dx=  \int_{-L}^L 1 \cos (k\pi x/L)dx=0$ pour tout $k\neq 0$
\item  $\int_{-L}^L \cos(j\pi x/L) \sin (k\pi x/L)dx= 0$ pour tout $k,j>0$.
\item $\int_{-L}^L \sin(j\pi x/L) \sin (k\pi x/L)dx=  \int_{-L}^L \cos(j\pi x)/L \cos (k\pi x/L)dx=0$ pour $k\neq j>0$.
\end{enumerate}
On a que 
\[\int_{-L}^L \sin (k\pi x/L)dx=\left[ \frac{-L}{k\pi} \cos(k\pi x/L)\right]_{-L}^L= 0\]
\[\int_{-L}^L \cos (k\pi x/L)dx=\left[ \frac{L}{k\pi} \sin(k\pi x/L)\right]_{-L}^L= 0\]
donc $(1)$ est vrai.
Ensuite, 
\[\int_{-L}^L \cos(j\pi x/L) \sin (k\pi x/L)dx= \int_{-L}^L \frac{1}{2} (\sin{(k+j)\pi x/L)}+ \sin((k-j)\pi x/L)) dx.\]
Par $(1)$ cet int\'egral est nul si $k\neq j$. Mais si $k=j$ on a que 
\[\int_{-L}^L \cos(j\pi x/L) \sin (j\pi x/L)dx= \int_{-L}^L \frac{1}{2}\sin(2k\pi x/L)dx=0.\]
Dans tous les cas, $(2)$ est v\'erifi\'e.
Enfin,  pour $k\neq j$ et $k,j>0$ on a que 
\[\int_{-L}^L \sin(j\pi x/L) \sin (k\pi x/L)dx=\int_{-L}^L \frac{1}{2} (-\cos{(k+j)\pi x/L)}+ \cos((k-j)\pi x/L))dx=0\]
par $(1)$. De m\^eme
\[\int_{-L}^L \cos(j\pi x/L) \cos (k\pi x/L)dx=\int_{-L}^L \frac{1}{2} (\cos{(k+j)\pi x/L)}+ \cos((k-j)\pi x/L))dx=0\]
par $(1)$. La condition $(3)$ est donc v\'erifi\'ee. 
Ceci termine la d\'emonstration de l'orthogonalit\'e de $B$. \\ \\
\end{proof}
Pour obtenir une base orthonorm\'ee pour $W_n(\rr)$ il suffira donc de normaliser la base
\[B=( 1, \cos(\pi x/L), \sin(\pi x/L), \cos(2\pi x/L)\ldots, \cos(n\pi x/L), \sin(n\pi x/L))\]
On a que 
\[ || 1|| =\sqrt{ \int_{-L}^L 1^2 dx}= \sqrt{2L}\]
et
\[ || \cos{k\pi x/L}|| = \sqrt{\int_{-L}^L \cos^2 (k\pi x/L)dx }\]
\[=\sqrt{\int_{-L}^L\frac{1}{2}(1+\cos( 2k\pi x/L))dx }=\sqrt{L}.\]
Par un calcul similaire \[|| \sin(k\pi x/L)||= \sqrt{L}.\] Nous avons donc
une base orthonorm\'ee $\tilde{B}$ de $W_n$ donn\'ee par
\[\tilde{B}=\left(\frac{1}{\sqrt{2L}}, \frac{\cos(\pi x/L)}{\sqrt{L}}, \frac{\sin(\pi x/L)}{\sqrt{L}}, \frac{\cos(2\pi x/L)}{\sqrt{L}}\ldots, \frac{\cos(n\pi x/L)}{\sqrt{L}}, \frac{\sin(n\pi x/L)}{\sqrt{L}}\right)\]
Nous pouvons donc calculer l'approximant de Fourier trigonom\'etriques 
$S^{\rr}_n(f)$ en utilisant le formule de la projection orthogonale. Ce 
formule nous dit que
\[ S^{\rr}_n(f)= \langle \frac{1}{\sqrt{2L}},f\rangle \frac{1}{\sqrt{2L}}+
\sum_{k=1}^n \left\langle  \frac{\cos(k\pi x/L)}{\sqrt{L}}, f\right\rangle \frac{\cos(k\pi x/L)}{\sqrt{L}}+
\left\langle \frac{\sin(k\pi x/L)}{\sqrt{L}}, f\right\rangle\frac{\sin(k\pi x/L)}{\sqrt{L}} \]
\[ = \frac{1}{2L}\int_{-L}^L f dx+ \sum_{k=1}^n a_k(f) \cos(k\pi x/L)+ 
b_k(f) \sin(k\pi x/L)\]
ou les coefficients $a_k(f)$ et $b_k(f)$ sont d\'efinis par 
\[ a_k(f)= \frac{1}{L}\int_{-L}^L \cos(k\pi x/L) f dx \]
et 
\[ b_k(f)= \frac{1}{L}\int_{-L}^L \sin(k\pi x/L) f dx. \]
\begin{defn}
Soit $f$ une fonction r\'eelle ou complexe 
continue par morceaux d\'efinie sur une intervalle $[-L, L]$. Les coefficients de Fourier trigonom\'etriques de $f$
sont les nombres $a_0(f)$, $a_k(f), b_k(f)$ $(k>0)$
d\'efinis par 
\[a_0(f)=\frac{1}{2L}\int_{-L}^L f dx\] \[a_k(f)= \frac{1}{L}\int_{-L}^L
 \cos(k\pi x/L) f dx\]\[ b_k(f)=\frac{1}{L}\int_{-L}^L \sin(k\pi x/l) f dx.\]
Le $n$-i\`eme approximation de Fourier trigonom\'etrique est alors donn\'ee par
\[ S{\rr}_n(f)= a_0(f)+\sum_k a_k(f) \cos(k\pi x/L)+ b_k \sin(k\pi x/L).\]
\end{defn}
\begin{rem}
Il suit des formules d'Euler :
\[ e^{ik\pi x/L}= \cos(k\pi x/L)+i \sin(k\pi x/L)\]
\[ \cos(k\pi x/L)= \frac{1}{2} (e^{ik\pi x/L}+ e^{-ik\pi x/L})\]
\[ \sin(k\pi x/L)= \frac{1}{i} (e^{ik\pi x/L}- e^{-ik\pi x/L})\]
qu'approcher $f$ par une somme de fonctions trigonom\'etriques \'equivaut  \`a l'approcher par une somme d'exponentielles complexes. 
Plus pr\'ecisement, 
consid\`erons la somme trigonom\'etrique
\[ a_0+\sum_{k=1}^n a_k \cos(k\pi x/L)+ b_k \sin(k\pi x/L).\]
Si on pose $c_0=a_0$ et pour tout $k>0$
\[c_k= \frac{1}{2}(a_k-ib_k), \;c_{-k}=  \frac{1}{2}(a_k+ib_k)\] alors on a 
\[  a_0+\sum_{k=1}^n a_k \cos(k\pi x/L)+ b_k \sin(k\pi x/L)=\sum_{k=-n}^n c_k e^{i k \pi x/L}.\]
\end{rem}
Pour des fonctions complexes, cette version des approximants de Fourier est particuli\`erement utile.
\begin{defn}
Soit $f: [-L, L]\rightarrow \cc$ une fonction continue par morceaux. 
On d\'efinit le $n$-i\`eme approximant exponentiel de Fourier de $f$, not\'e $S^{\cc}_n(f)$, par
\[ S^{\cc}_n(f)=\sum_{-n}^n c_k e^{ik\pi x/L} \]
ou $c_k$ sont les approximants exponentiels de $f$ d\'efinis par 
\[ c_k= \frac{1}{2}(a_k-ib_k), \;c_{-k}=  \frac{1}{2}(a_k+ib_k)\]
ou $a_k$ et $b_k$ sont les coefficients trigonom\'etriques de Fourier de $f$. 
\end{defn} 
En utilisant le formule pour les $a_k$ et $b_k$s ci-dessus, on obtient le formule suivant. 
\begin{defn}
Soit $f$ une fonction r\'eelle ou complexe  continue par morceaux et
d\'efinie sur $[-L, L]$. Les coefficients de Fourier exponentiels de $f$ sont 
les nombres $c_k(f)$ d\'efinis par
\[c_k(f)= \frac{1}{2L}\frac{\int_{-L}^L e^{-(ik\pi x/L)}f(x) dx\]
\end{defn}
Nous avons donc que l'approximant de Fourier trigonom\'etrique est
\[ S^{\rr}_n(f)= a_0(f)+ \sum_{k=1}^n a_k(f) \cos(k\pi x/L)  + b_k(f) 
\sin(k\pi x/L).\]
et que l'approximant exponentiel est
\[ S^{\cc}_n(f)= \sum_{k=-n}^n c_k(f) e^{(ik\pi x/L)}.\]
Il et pratique de mettre toutes ces termes ensemble dans une s\'erie infinie, m\^eme si c'est n'est pas encore tr\`es clair quel sens
on peut donner \`a cette somme formelle.
\begin{defn}
Soit $f$ une fonction continue par morceaux sur une intervalle $[-L,L]$.
 La {\bf s\'{e}rie de Fourier trigonom\'etrique} de $f$ est alors la somme
infinie
$$S^{\rr}(f)=\frac{a_0(f)}+\sum_{k=1}^{\infty}a_k(f)\cos(\frac{ k\pi x}{L})+b_k(f)\sin(\frac{ k\pi x}{L}).$$
La {\bf s\'{e}rie de Fourier exponentielle} de $f$ est la somme infinie
$$S^{\cc}(f)=\sum_{k=-\infty}^{\infty}c_k(f)e^{\frac{2i k\pi }{L} x}.$$
\end{defn}
\begin{exs}
\begin{enumerate}
\item
On consid\`ere la fonction $f(x)=x$ sur l'intervalle
 $[-\pi, \pi]$. Nous avons alors
\[ a_0(f)= \frac{1}{2\pi}\int_{-\pi}^\pi x dx=0.\]
Ensuite, pour tout $k>0$, en int\'egrant par parties on voit que
\[ a_k(f)=\frac{1}{\pi}\int_{-\pi}^\pi x \cos{(kx)} dx= \frac{1}{\pi}
\left[ \frac{x}{k} \sin (kx)\right]_{-\pi}^\pi -\frac{1}{k\pi}\int _{-\pi}^\pi \sin(kx)dx
\]
\[=  -\frac{1}{k\pi}\int _{-\pi}^\pi \sin(kx)dx=0.\]
De m\^eme, l'int\'egration par parties montre que 
\[ b_k(f)=\frac{1}{\pi}\int_{-\pi}^\pi x \sin{(kx)} dx= \left[ -\frac{x}{k\pi} \cos (kx)\right]_{-\pi}^\pi +\frac{1}{k\pi}\int _{-\pi}^\pi \cos(kx)dx\]
\[=  -\frac{2(-1)^k}{k}+\frac{1}{k\pi}\int _{-\pi}^\pi \cos(kx)dx=  
\frac{2(-1)^{k+1}\pi}{k}.\]
La s\'eries de Fourier trigonom\'etrique de $f$ est donc la somme infinie
\[S^{\rr}(f)=\sum_{k=1}^\infty  \frac{2(-1)^{k+1}}{k} \sin(kx).\]
Calculons maintenant la s\'erie de Fourier exponentielle de $f$.
L'int\'egration par parties nous donne que 
\[ c_k(f)= \frac{1}{2\pi}\int_{-\pi}^\pi e^{-ik x} x dx= \frac{1}{2\pi} \left[ \frac{x}{-ik} e^{-ikx}\right]_{-\pi}^\pi+\int_{-\pi}^\pi \frac{1}{2ik\pi}e^{-ikx}dx\]
\[= \frac{(-1)^k}{-2ik}= \frac{i (-1)^k}{2k}.\]
La s\'erie des Fourier exponentielle de $f$ est alors la somme infinie
\[S^{\cc}(f)=\sum_{k=-\infty}^{\infty}  \frac{i (-1)^k}{2k} {e^{ikx}}.\]
\item On consid\`ere la fonction $f$ d\'efinie sur $[-\pi, \pi]$ telle que 
$f(x)=1$ pour $x\geq 0$ et $f(x)=0$ pour $x<0$. On a que
\[ a_0(f)= \frac{1}{2\pi}\int_{-\pi}^\pi f(x) dx=\frac{1}{2}.\]
Par ailleurs, pour tout $k>0$,
\[a_k(f)= \frac{1}{\pi}\int_{-\pi}^\pi \cos(kx) f(x)dx= \frac{1}{\pi}\int_0^\pi \cos(kx)dx\]
\[= \frac{1}{k\pi}[ \sin(kx)]_0^{\pi}=0\] 
et 
\[ b_k(f)= \frac{1}{\pi}\int_{-\pi}^\pi \sin(kx) f(x)dx= \frac{1}{\pi}\int_0^\pi \sin(kx)dx\]
\[= \frac{1}{k\pi}[ -\cos(kx)]_0^{\pi}=\frac{1-(-1)^k}{k\pi}.\]
On note que $(1-(-1)^k)= 2$ si $k$ est impair et $0$ si $k$ est pair. En 
\'ecrivant tout $k$ impair dans la forme $k=2l+1$, on obtient que
la s\'eries de Fourier trigonom\'etrique de $f$ est 
\[ S^{\rr}(f)=\frac{1}{2}+ \sum_{l=0}^{+\infty} \frac{2}{(2l+1)\pi} \sin( (2l+1) x).\]
Calculons maintenant la s\'eries de Fourier exponentielle de $f$. Pour $k\neq 0$ on a que
\[ c_k(f)= \frac{1}{2\pi}\int_{-\pi}^\pi e^{(-ikx)} f(x)dx= \frac{1}{2\pi}\int_0^\pi e^{(-ikx)}dx\]
\[= \frac{1}{-2ik\pi}[ e^{-(ikx)}]_0^{\pi}=\frac{-1+(-1)^k}{-2ik\pi}}.\]
Comme nous avons d\'ej\`a calcul\'e que $c_0=a_0=\frac{1}{2}$, 
la s\'eries de Fourier exponentielle de $f$ est alors 
\[ S^{\cc}(f)=\frac{1}{2} + \sum_{l=-\infty}^\infty \frac{-i}{(2l+1)\pi} e^{i(2l+1) x}.\]
\end{enumerate}
\end{exs}
\subsection{S\'eries en sin et cos.}
La s\'eries de Fourier est une s\'eries qui m\'elange des termes de sin et en cos. Nous montrerons dans cette
section comment modifier cette construction pour obtenir des s\'eries en sin ou en cos approchant une fonction $f$
donn\'ee. Notre point de d\'epart sera la proposition suivante :
\begin{prop}\label{Fourierparie}
Soit $f: [-L, L]\rightarrow \rr$ une fonction r\'eelle. Alors 
\begin{enumerate}
\item si $f$ est paire alors $b_k(f)=0$ pour tout $k$.
\item si $f$ est impaire alors $a_k(f)=0$ pour tout $k$.
\end{enumerate}
\end{prop}
\begin{proof}
On note que pour toute fonction $g$, impaire sur $[-L, L]$, nous avons que 
\[\int_{-L}^L g(x)dx=0.\] Par ailleurs,
\begin{enumerate}
\item 
Le produit d'une fonction impaire et une fonction paire est lui-m\^eme une fonction impaire. 
\item 
Pour tout $k>0$ la fonction $\cos(kx)$ est une fonction paire et la fonction $\sin(kx)$ est une fonction impaire. 
\end{enumerate}
Si la fonction $f:[-L, L]\rightarrow \rr$ est une fonction paire alors pour tout $k$ la fonction $f(x) \sin(k \pi x/L)$ est
une fonction impaire et donc
\[ b_k(f)=\frac{1}{L}\int_{-L}^L f(x)\sin(k\pi x/L) dx=0\]
Par contre, si la fonction $f:[-L, L]\rightarrow \rr$ est une fonction impaire alors pour tout $k$ la fonction $f(x) \cos(k\pi x/L)$ est
une fonction impaire et donc
\[ a_k(f)=\frac{1}{L}\int_{-L}^L f(x)\sin(k\pi x/L) dx=0\]
Ceci termine la d\'emonstration de la proposition.
\end{proof}
Avec cette proposition, nous pourrons construire des sommes de cos 
(resp. de sin) approchant une fonction donn\'ee. 
Notre m\'ethode sera la suivante :
\begin{enumerate}
\item Etant donn\'ee une fonction $f:[0,L]\rightarrow \rr$, on construit une 
extension $g$ sur $[-L, L]$ qui
est paire (si on veut construire une s\'eries en cos) ou impaire (si on veut construire une s\'erie en sin.)
\item Nous construisons alors la s\'eries de Fourier de cette nouvelle fonction $g$.
\item Puisque $g$ est paire (resp. impaire) sa s\'eries de Fourier ne contient que des termes en cos (resp. en sin.)
C'est cette s\'erie de Fourier qui sera la s\'erie en cos (resp. en sin) de $f$.
\end{enumerate}

\begin{defn}
Soit $f: [0,L]\rightarrow \rr$ une fonction continue par morceaux. On d\'efinit sur $[-L,L]$
les extensions impaire et paire de
$f$ par
\[ f_{\rm paire}(x)= f(x)\mbox{ si } x\geq 0,\; f(-x)\mbox{ si } x<0.\]
\[ f_{\rm impaire}(x)= f(x)\mbox{ si } x> 0,\; -f(-x)\mbox{ si } x<0,\; 0
\mbox{ si } x=0.\]
\end{definition}
Notons que $f_{\rm paire}$ est paire et $f_{\rm impaire}$ est impaire par construction. Il suit de proposition \ref{Fourierparie}
que
\begin{itemize}
\item pour tout $k$, $a_k( f_{\rm impaire})=0$
\item pour tout $k$, $b_k( f_{\rm paire})=0$.
\end{itemize}

Nous pouvons maintenant d\'efinir les s\'eries en sin et cosine de notre fonction $f$.
\begin{defn}
Soit $f:[0,L]\rightarrow \rr$ une fonction continue par morceaux.  
 La s\'eries en sin de $f$ est la s\'eries trigonom\'etrique 
\[S^{\rm sin}(f)=S^{\rr}(f_{\rm impaire})=\sum_{k=1}^{\infty} b_k(f_{\rm impaire}) \sin(k\pi x/L).\] 
La s\'eries en cos de $f$ est la
s\'eries trigonom\'etrique 
\[S^{\rm cos} (f)= S^{\rr}(f_{\rm paire})= a_0(f_{\rm paire})+\sum_{k=1}^\infty a_k(f_{\rm paire}) \cos(k\pi x/L).\]
\end{defn}
\begin{rem}
Notons que par parit\'e
\[ b_k(f_{\rm impaire})= \frac{1}{L}\int_{-L}^L f_{\rm impaire} (x)\sin (k\pi x/L)dx=\frac{2}{L} \int_{0}^L f(x) \sin (k\pi x/L)dx.\]
De m\^eme 
\[a_k(f_{\rm paire})= \frac{1}{L}\int_{-L}^L f_{\rm paire} (x)\cos (k\pi x/L)dx=\frac{2}{L} \int_{0}^L f(x) \cos (k\pi x/L)dx.\]
et
\[ a_0(f)=\frac{1}{L} \int_0^L f(x) dx.\]
\end{rem}
\begin{ex}
On consid\`ere la fonction $f(x)=e^{x}$ sur l'intervalle $[0,\pi]$. 
Nous cherchons \`a calculer sa s\'erie en sin. Nous avons que
\[ S^{\rm sin}(f)= \sum_{k=1}^\infty b_k \sin (kx)\]
ou \[b_k=\frac{2}{\pi} \int_0^\pi e^x \sin(kx) dx=\]
\[ =\frac{2}{\pi} \int_0^\pi e^x \frac{e^{ikx}-e^{-ikx}}{2i} dx=\]
\[= \frac{ 1}{i\pi}\int_0^\pi e^{x(1+ik)}- e^{x(1-ik)}dx\]
\[ =\frac{ 1}{i\pi}\left[\frac{  e^{x(1+ik)}}{1+ik} -\frac{ e^{x(1-ik)}}{1-ik}
\right]_0^\pi\]
\[ =  \frac{ 1}{i\pi}\left[\frac{  (1-ik)e^{x(1+ik)}-(1+ik)e^{x(1-ik)}}{1+k^2}
\right]_0^\pi\]
\[=  \frac{ 1}{i\pi}\left[\frac{e^x(-2ik \cos(kx) +2i \sin(kx))}{1+k^2}
\right]_0^\pi\]
\[= \frac{2k(1-(-1)^ke^\pi)}{(1+k^2)\pi}.\]
La s\'erie en sin de $f$ est donc
\[S^{\rm sin}(f)= \sum_{k>0}  \frac{2k(1-(-1)^ke^\pi)}{(1+k^2)\pi} \sin(kx).\]

\end{ex}
\subsection{Convergence des s\'eries de Fourier.}
Nous avons donc cr\'e\'e, pour chaque fonction $f$ continue par morceaux d\'efinie sur une intervalle $[-L, L]$, une suite de fonctions 
$S^{\rr}_n(f)$. Chaque \'el\'ement dans cette suite de fonctions est ``plus proche''  
\`a la fonction $f$ que celle qui la pr\`ecede. 
Mais pour nous \^etre utile, il faudrait qu'on peut s'assurer que, quitte \`a prendre $n$ tr\`es 
grand, la fonction
$S^{\rr}_n(f)$ est aussi proche que l'on veut \`a la fonction $f$. Cette 
question sera le sujet de ce paragraphe. 
\\ \\
Le th\'eor\`eme suivant,
dont la d\'emonstration depasse le cadre de ce cours, nous assure que si la 
fonction $f$ est d\'erivable alors en tout point $x$ on a que
\[ \lim_{\n\rightarrow \infty}S^{\rr}_n(f)(x)= f(x).\] 
\begin{thm}[Dirichelet]
Soit $f:[-L, L]\to\cc$ une fonction $C^1$ par morceaux. Alors pour tout point $x\in ]-L, L[$ ou
$f$ est continue nous avons que
\[ \lim_{n\rightarrow \infty} S^{\rr}_n(f)(x)\rightarrow f(x)\]
et pour tout $x\in ]-L, L[$ nous avons que
\[ \lim_{n\rightarrow \infty} S^{\rr}_n(f)(x)= \frac{1}{2}(\lim_{y\rightarrow x^+} f(y)+\lim_{y\rightarrow x^-}f(y)).\] 
Par ailleurs, \[\lim_{n\rightarrow \infty}S_n^{\rr}(f)(-L)=\lim_{n\rightarrow \infty}S_n^{\rr}(f)(-L)=
\frac{\lim_{x\rightarrow L^-}f(x)+\lim_{x\rightarrow -L^+}f(x)}{2}.\]
En particulier, si $f$ est continue et $C^1$ par morceaux, $S^{\rr}_n(f)$ converge 
vers $f$ sur $]-L, L[$.
\end{thm}
Ce th\'eor\`eme est faux si la fonction $f$ n'est pas suppos\'ee d\'erivable.
\\ \\
Il y a une autre forme de convergence qui nous sera utile, c'est la convergence en distance. Encore une fois, la d\'emonstration
de ce th\'eor\`eme depasse le cadre de ce cours.
\begin{thm}[Th\'eor\`eme de Parseval.]
Soit $f$ une fonction r\'eelle continue par morceaux sur une intervalle 
$[-L,L]$. Soit $d_n$ la distance
\[ d_n= d(f, S^{\rr}_n(f))=\sqrt{\int_{-L}^L (f-S^{\rr}_n(f))^2dx}\]
entre $f$ et $S_n^{\rr}(f)$. Alors 
$d_n\rightarrow 0$ quand $n\rightarrow \infty$.
\end{thm}
En particulier, puisque l'in\'egalit\'e triangulaire nous dit que 
\[ (|| f|| -|| S^{\rr}_n(f)||)^2\leq d_n^2\]
 nous avons le corollaire suivant
 \begin{coro}[Egalit\'e de Parseval]
Soit $f$ une fonction r\'eelle continue par morceaux sur une intervalle $[-L,L]$. Alors
\[ \lim_{n\rightarrow \infty} || S_n^{\rr}(f)||= \lim_{n\rightarrow \infty} (2L a_0^2+\sum_{k=1}^n L( a_k^2+b_k^2))= \int_{-L}^L f(x)^2 dx.\]
Autrement dit, \[ (2L a_0^2+\sum_{k\geq 1} L( a_k^2+b_k^2))= \int_{-L}^L f(x)^2 dx.\]
\end{coro}
\begin{ex}
 Consid\'erons $f(x)= x$ sur $[-\pi, \pi]$. On a vu que pour cette fonction $a_k=0$ pour tout $k$ et $b_k= \frac{2 (-1)^k}{k}$. 
Par ailleurs \[\int_{-\pi}^\pi x^2 dx= \frac{2 \pi^3}{3}.\] Il en suit que 
\[ \pi \sum_{k= 1}^\infty \frac{4}{k}^2= \frac{2 \pi^3}{3}\]
et donc 
\[ \sum_{k= 1}^\infty \frac{1}{k^2}= \frac{\pi^2}{6}.\]
\end{ex}

\begin{coro}\label{egcoeffs}
Soient $f,g:\rr\to\cc$ deux fonctions r\'eelles
continues par morceaux et d\'efinies
sur $[-L, L]$. Si $a_n(f)=a_n(g)$ et $b_n(f)= b_n(g)$ pour tout $n$ 
alors pour tout
point $x$ ou $f$ et $g$ sont tous deux continues $f(x)=g(x)$.
\end{coro}

\begin{proof}
L'\'{e}galit\'{e} de Parseval implique que si 
$a_n(f)=a_n(g)$ et $b_n(f)= b_n(g)$ pour tout $n$
alors  $$\int_{-L}^L\vert f(x)-g(x)\vert^2{\rm d}x=0.$$
Ceci n'est possible que si $f(x)=g(x)$ en tout point $x$ ou $f$ et $g$ sont
continues.
\end{proof}
\subsection{Solutions d'\'equations \`a d\'eriv\'ees partielles.}
Dans cette section, nous serons a\`a faire quelques manipulations dont 
nous ne pourrons pas donner une justification compl\`ete.
\subsubsection{L'\'equation de la chaleur.} 
On rappelle que l'on cherche une fonction $C^2$ \[T: [0,L]\times \rr^+\rightarrow \rr\]
telle que
$$\frac{\partial T}{\partial t}=D\frac{\partial^2 T}{\partial x^2},x\in [0,L],t>0$$
avec les conditions aux bords $$\frac{\partial T}{\partial x}(0,t)= \frac{\partial T}{\partial x}(L,t)=0$$
et la condition initiale,
$$T(x,0)=\varphi(x)\mbox{ pour tout }x\in [0,L],t=0.$$

Remarquons que puisque nous cherchons une solution $T$ qui est $C^2$, la condition initiale
n'est possible que si $\varphi$ est $C^2$ sur $[0,L]$.

Nous allons montrer le th\'{e}or\`{e}me suivant:

\begin{thm}
Soit $\varphi:[0,L]\to\rr$ une fonction $C^2$ sur $[0,L]$
telle que $\varphi'(0)=\varphi'(L)=0$.
Alors l'\'{e}quation de la chaleur $$\frac{\partial T}{\partial t}=D\frac{\partial^2 T}{\partial x^2},x\in [0,L],t\geq 0$$
admet une unique solution $C^2$ \[T:[0,L]\times \rr^+ \to \rr\] v\'erifiant les
conditions aux bords
$$\frac{\partial T}{\partial x}(0,t)=\frac{\partial T}{\partial x}(L,t)=0$$
et la condition initiale,
$$T(x,0)=\varphi(x)\mbox{ pour tout }x\in [0,L],t=0.$$
Cette solution
est donn\'{e}e par le formule
$$T(x,t)=a_0+\sum_{k= 1}^{\infty}a_k\cos\left(\frac{k\pi}{L}x\right)e^{-\frac{\pi^2 k^2}{L^2}Dt},$$
o\`{u} $a_0=  \frac{1}{L}\int_0^L \varphi(x)dx$ et pour tout $k>0$ on a que 
$\ds a_k=\frac{2}{L}\int_0^L\varphi(x)\cos\left(\frac{k\pi}{L}x\right){\rm d}x.$
\end{thm}

\begin{proof}
La d\'emonstration que la fonction $T$ est bien d\'efinie et $C^2$ depasse le cadre de ce cours.\\ \\
Montrons que $T$ satisfait la condition initiale. 
 Puisque $\varphi$ est $C^1$, et $\varphi'(0)=\varphi'(L)=0$, son extension paire,
$\varphi_{\rm paire}$, est $C^1$. Par le th\'eor\`eme de Dirichelet il suit que
sur $[-L, L]$ nous avons que
\[ \varphi_{\rm paire} (x) = a_0+\sum_{k=1}^{\infty}
a_k \cos(k\pi x/L).\]
et en particulier  pour tout $x\in [0,L]$
\[ \varphi(x) = a_0+\sum_{k=1}^{\infty}
a_k \cos(k\pi x/L).\]
ou 
$a_0= \frac{1}{L}\int_{0}^L f(x)dx$ et pour tout $k>0$ on a que
$\ds a_k=\frac{2}{L}\int_0^L\varphi(x)\cos\left(\frac{k\pi}{L}x\right){\rm d}x$.\\ \\
Autrement dit, le th\'eor\`eme de Dirichelet nous dit dans ce cas que
$\varphi$ est \'egale \`a la somme (infinie) de sa s\'erie en cos. \\ \\
Par analogie avec le cas ou $\varphi$ est \'egale \`a une somme finie de cosinus, nous allons poser
$$T(x,t)=a_0+\sum_{k= 1}^{\infty}a_k\cos\left(\frac{k\pi}{L}x\right)e^{-\frac{\pi^2 k^2D}{L^2}t},$$ et v\'{e}rifier que $T$ est solution de l'\'equation de
la chaleur.
Pour tout $x\in[0,L]$, on a \[T(x,0)=a_0+\ds\sum_{k= 1}^{\infty}a_k
\cos\left(\frac{k\pi}{L}x\right)=\varphi(x).\] et la condition initiale est donc v\'erifi\'ee. 
De plus, $\frac{\partial T}{\partial x}(0,t)=\frac{\partial T}{\partial x}(L,t)=0$
d'apr\`{e}s les propri\'{e}t\'{e}s du cosinus : les conditions aux bords sont donc v\'erif\'ees.\\ \\
Reste \`a \'etablir que $T$ satisfait  $\frac{\partial T}{\partial t} =
-D\frac{\partial^2 T}{\partial x^2}$. Posons \[u_k(x,t)=a_k\cos(\frac{k\pi}{L}x)e^{-\frac{\pi^2 k^2D}{L^2}t},\] 
de telle fa\c con que $T(x,t)= a_0+\sum_k u_k(x,t)$
Nous allons maintenant faire l'hypoth\`ese que lorsque nous avons une somme infinie de fonctions
\[ T(x,t)=\sum_{k=1}^\infty u_k(x,t)\]
on a bien que
\[ \frac{\partial T}{\partial x} = \sum_{k=1}^{\infty}\frac{\partial u_k}{\partial x}\]
et
\[ \frac{\partial T}{\partial t} = \sum_{k=1}^{\infty}\frac{\partial u_k}{\partial t}\]
(ce qui est math\'ematiquement assez d\'elicat \`a justifier). 
Alors
\[ \frac{\partial T}{\partial t} =
\sum_{k\geq 1}a_k\frac{\partial }{\partial t}\left(\cos(\frac{k\pi}{L}x)e^{-\frac{\pi^2 k^2D}{L^2}t}\right)\]
\[= \sum_{k\geq 1}-D \frac{\pi^2 k^2}{L^2}
a_k\left(\cos(\frac{k\pi}{L}x)e^{\frac{-\pi^2 k^2D}{L^2}t}\right).\]
Mais on a aussi
\[
 \frac{\partial^2 T}{\partial x^2} =
\sum_{k\geq 1}a_k\frac{\partial^2 }{\partial x^2}\left(\cos(\frac{k\pi}{L}x)e^{-\frac{
\pi^2 k^2D}{L^2}t}\right)\]
\[= \sum_{k\geq 1} -\frac{\pi^2 k^2}{L^2}
a_k\left(\cos(\frac{k\pi}{L}x)e^{-\frac{\pi^2 k^2D}{L^2}t}\right).\]
On a donc bien
\[ \frac{\partial T}{\partial t} = 
D\frac{\partial^2 T}{\partial x^2}\]
et la fonction donn\'ee est donc une solution de notre \'equation.\\ \\
Montrons maintenant que cette solution est unique. Pour cela, supposons que
l'on ait deux solutions du probl\`{e}me, disons $T_1$ et $T_2$, et posons
$u=T_1-T_2$. Il est facile de v\'{e}rifier que l'on a
$$\frac{\partial u}{\partial t}=D\frac{\partial^2 u}{\partial x^2},x\in [0,L],t\in\rr^+,$$
et que $$\frac{\partial u}{\partial x}(0,t)=\frac{\partial u}{\partial x}(L,t)=u(x,0)=0\mbox{ pour tout }x\in [0,L],t\in\rr^+.$$


Consid\'erons la fonction $J(t)=\frac{1}{2D}\int_0^L u(x,t)^2dx$. 
On a (modulo la justification de la d\'{e}rivation sous l'int\'{e}grale) $$J'(t)=\frac{1}{D}\int_0^L u(x,t)\frac{\partial u(x,t)}{\partial t}{\rm d}x=\int_0^L u(x,t)\frac{\partial^2 u(x,t)}
{\partial x^2}{\rm d}x.$$
En int\'{e}grant par parties, il vient, en tenant compte des conditions aux bords,
$$J'(t)=-\int_0^L \left(\frac{\partial u(x,t)}{\partial x}\right)^2{\rm d}x.$$
On a donc $J'(t)\leq 0$ pour tout $t\geq 0$. Ainsi $J$ est d\'{e}croissante, et on a $$J(t)\leq J(0)\mbox{ pour tout }t\geq 0.$$
Or on a $$J(0)=\frac{1}{2D}\int_0^L u(x,0)^2{\rm d}x=0,$$
d'apr\`{e}s les conditions au bord v\'{e}rifi\'{e}es par $u$.
Ainsi, $J(t)\leq 0$ pour tout $t\geq 0$. Mais $J$ est l'int\'{e}grale d'une fonction positive ou nulle, et donc on a aussi $J(t)\geq 0$ pour tout $t\geq 0$.
On en d\'{e}duit que $J=0$, et donc $u(x,t)=0$ pour tout $t> 0$ et tout $x\in [0,L]$. Comme on a aussi $u(x,0)=0$ pour tout $x\in [0,L]$, $u$ est identiquement nulle, c'est-\`{a}-dire $T_1=T_2$.


\subsubsection{L'\'equation des ondes.}
On rappelle que l'on cherche une fonction \[T: [0,L]\times \rr^+\rightarrow \rr\] 
telle que
$$\frac{\partial^2 T}{\partial t^2}=D\frac{\partial^2 T}{\partial x^2},x\in [0,L],t>0$$
avec les conditions aux bords $$T(0,t)= T(L,t)=0$$
et les conditions initiales,
$$T(x,0)=\varphi(x)\mbox{ pour tout }x\in [0,L],t=0.$$
$$\frac{\partial T}{\partial t}(x,0)=0\mbox{ pour tout }x\in [0,L],t=0.$$

Remarquons que puisque la solution $T$ est $C^2$ , la condition initiale
n'est possible que pour une fonction $\varphi$ qui est $C^2$ sur $[0,L]$.

Nous allons montrer le th\'{e}or\`{e}me suivant:

\begin{thm}
Soit $\varphi:[0,L]\to\rr$ une fonction $C^2$ sur $[0,L]$
telle que $\varphi(0)=\varphi(L)=0$.
Alors l'\'{e}quation des ondes $$\frac{\partial T^2}{\partial t^2}=D\frac{\partial^2 T}{\partial x^2},x\in [0,L],t\geq 00$$
admet une unique solution $C^2$ \[T:[0,L]\times \rr^+ \to \rr\] v\'erifiant les
conditions aux bords
$$T(0,t)=T(L,t)=0$$
et les conditions initiales,
$$T(x,0)=\varphi(x)\mbox{ pour tout }x\in [0,L],t=0.$$
$$\frac{\partial T}{\partial t}(x,0)=0\mbox{ pour tout }x\in [0,L],t=0.$$
Cette solution
est donn\'{e}e par le formule
$$T(x,t)=\sum_{k=1}^{infty}_k\sin\left(\frac{k\pi}{L}x\right)\cos\left(\frac{\pi k\sqrt{D}}{L}t\right),$$
o\`{u} pour tot $k>0$ on a que $\ds b_k=\frac{2}{L}\int_0^L\varphi(x)\sin\left(\frac{k\pi}{L}x\right){\rm d}x.$
\end{thm}

\begin{proof}
Puisque $\varphi$ est $C^1$, et $\varphi(0)=\varphi(L)=0$, son extension impaire,
$\varphi_{\rm impaire}$ est $C^1$. Par le th\'eor\`eme de Dirichelet il suit que
sur $[-L, L]$ nous avons que
\[ \varphi_{\rm impaire} (x) = \sum_{k=1}^{\infty}
b_k(\varphi_{\rm impaire}) \sin(k\pi x/L).\]
et en particulier  pour tout $x\in [0,L]$
\[ \varphi(x) = \sum_{k=1}^{\infty}
b_k \sin(k\pi x/L).\]
 ou $\ds b_k=\frac{2}{L}\int_0^L\varphi(x)\sin(\frac{k\pi}{L}x){\rm d}x$.\\ \\
Autrement dit, le th\'eor\`eme de Dirichelet nous dit dans ce cas que
$\varphi$ est \'egale \`a la somme (infinie) de sa s\'erie en sin. \\ \\
Par analogie avec le cas ou $\varphi$ est donn\'ee par une somme finie de sinus, nous allons poser
$$T(x,t)=\sum_{k\geq 1}b_k\sin\left(\frac{k\pi}{L}x\right)\cos\left(\frac{\pi k\sqrt{D}}{L}t\right),$$ et v\'{e}rifier que $T$ est solution de l'\'equation des ondes.
Pour tout $x\in[0,L]$, on a $$T(x,0)=\ds\sum_{k\geq 1}b_k
\sin(\frac{k\pi}{L}x)=\varphi(x)$$
et la premi\`ere condition initiale est donc v\'erifi\'ee. De plus, $T(0,t)=T(L,T)=0$
d'apr\`{e}s les propri\'{e}t\'{e}s du sinus : les conditions aux bords sont donc v\'erifi\'ees.\\ \\
Posons \[u_k(x,t)=b_k\sin(\frac{k\pi}{L}x)\cos{\frac{\pi k\sqrt{D}}{L}t},\] de telle fa\c con
que $T(x,t)=\sum_k u_k(x,t)$.
Rappelons que nous avons suppos\'e que lorsqu'on a une somme infinie de fonctions
\[ T(x,t)=\sum_{k} u_k(x,t)\]
on a bien que
\[ \frac{\partial T}{\partial x} = \sum_{k=1}^{\infty}\frac{\partial u_k}{\partial x}\]
et
\[ \frac{\partial T}{\partial t} = \sum_{k=1}^{\infty}\frac{\partial u_k}{\partial t}\]
V\'erifions maintenant la deuxi\`eme condition initiale. Puisque pour tout $k$,
$\frac{\partial u_k}{\partial t}(x,0)=0$, il suit que $ \frac{\partial T}{\partial t} (x,0)=0$ pour tout $x\in [0,L]$.\\ \\
Il reste \`a v\'erifier que $T$ satisfait l'\'equation $\frac{\partial^2 T}{\partial t^2} =
D\frac{\partial^2 T}{\partial x^2}$. On a que 
\[ \frac{\partial^2 T}{\partial t^2} =
\sum_{k\geq 1}b_k\frac{\partial^2 }{\partial t^2}\left(\sin(\frac{k\pi}{L}x)\cos{\frac{\pi k\sqrt{D}}{L}t}\right)\]
\[= \sum_{k\geq 1}-D \frac{\pi^2 k^2}{L^2}
b_k\left(\sin(\frac{k\pi}{L}x)\cos(\frac{\pi k\sqrt{D}}{L}t}\right).\]
Mais on a aussi
\[
 \frac{\partial^2 T}{\partial x^2} =
\sum_{k\geq 1}b_k\frac{\partial^2 }{\partial x^2}\left(\sin(\frac{k\pi}{L}x)\cos(\frac{
\pi k\sqrt{D}}{L}t}\right)\]
\[= \sum_{k\geq 1} -\frac{\pi^2 k^2}{L^2}
b_k\left(\sin(\frac{k\pi}{L}x)\cos(\frac{\pi k\sqrt{D}}{L}t}\right).\]
On a donc bien
\[ \frac{\partial^2 T}{\partial t^2} =
D\frac{\partial^2 T}{\partial x^2}\]
et la fonction donn\'ee est donc une solution de notre \'equation.\\ \\
Montrons maintenant que cette solution est unique. Pour cela, supposons que
l'on ait deux solutions $C^2$ du probl\`{e}me, disons $T_1$ et $T_2$, et posons
$u=T_1-T_2$. Il est facile de v\'{e}rifier que l'on a
$$\frac{\partial u}{\partial t}=D\frac{\partial^2 u}{\partial x^2},x\in [0,L],t\in\rr^+,$$
et que $$u(0,t)=u(L,t)=u(x,0)=0\mbox{ pour tout }x\in [0,L],t\in\rr^+.$$


Consid\'erons la fonction 
$$b_k(t)=\frac{1}{2D}\int_0^L u(x,t) \sin{
(k\pix/L){\rm d}x.$$ Autrement dit, $b_k(t)$ est le $k$-i\`eme coefficient dans la s\'eries en sin de la fonction $x\rightarrow u(x,t)$. Par
Corollaire \ref{egcoeffs} il suffira de montrer que $b_k(t)$  est nul pour tout $t$.
Par les conditions initiales on a que
\[ b_k(0)= b_k,\; \frac{\partial b_k}{\partial t}(0)=\int_0^L \frac{\partial u_k(x,0)}{\partial t}\sin{
(k\pix/L){\rm d}x.=0\]
Calculons 
\[ b''_k(t)= \int_0^L \frac{\partial^2 u(x,t)}{\partial t^2}\sin{
(k\pi x/L){\rm d}x\]
\[= D \int_0^L \frac{\partial^2 u(x,t)}{\partial x^2}\sin{
(k\pi x/L){\rm d}x.=0\]
ce qui est \'egale, apr\`es une double int\'egration par parties et en utilisant les
conditions aux bords, \`a
\[ (-Dk^2\pi^2/L^2) \int_0^L \frac{\partial^2 u(x,t)}{\partial x^2}\sin{
(k\pi x/L){\rm d}x.=-Dk^2\pi^2/L^2 b_k(t).\]
Autrement dit,
\[ b''_k(t)=(-Dk^2\pi^2/L^2)b_k(t).\]
Mais la seule solution $C^2$ de cette fonction telle que $b_k(0)= b_k'(0)=0$ est la fonction nulle. On a donc $b_k(t)=0$ pour
tout $k$ et tout $t$, ce qui donne bien que $u(x,t)=0$ pour tout $x,t$.  
\subsection{La transform\'ee de Fourier.}
La s\'eries de Fourier que nous avons \'etudi\'e ci-dessus s'adapte bien
pour des fonctions d\'efinies sur une intervalle, c'est \`a dire, \`a des
ph\'enom\`enes de physique born\'ees. Or, en physique, il nous arrive souvent
de vouloir analyser une fonction qui n'est pas a priori born\'ee dans le temps 
ou l'espace. \\ \\ Ceci nous inspire la question suivante :
\\ \\
\centerline{\bf  Que pourrait \^etre la s\'erie de Fourier d'une fonction sur 
$\mathbb{R}$ ?}
\\ \\
Soit $f: \rr\rightarrow \rr$ une fonction : pour plus de simplicit\'e, nous 
ferons l'hypoth\`ese que l'int\'egrale $\int_{\rr} |f(x)| dx$ a une valeur finie. 
Nous allons essayer de trouver une analogue de la s\'erie de Fourier pour $f$ 
en regardant ce qui se passe 
quand on prend, pour $L$ de plus en plus grand, la s\'erie de Fourier de
la fonction restreinte $f_L: [-L,L]\rightarrow \rr$ donn\'ee par 
\[ f_L(x)=f(x) \forall x\in[-L,L].\]
Il convient d'utiliser la s\'erie de Fourier exponentielle, pour ne pas avoir \`a consid\'erer s\'eparemment les termes en cosinus et celles
en sinus.\\ \\
Le coefficient de $e^{i\lambda x}$ dans la s\'erie de Fourier exponentielle de $f_L$\footnote{On fait ici l'hypoth\`ese que
le r\'eel ${L \lambda/\pi}$ est un entier, et donc il existe une terme $e^{i\lambda x}$ dans la s\'erie de Fourier de $f$}
est donn\'ee par $c_k$ ou $k= \frac{ L \lambda}{\pi}$. Autrement dit, le coefficient de $e^{i\lambda x}$ dans la s\'erie de Fourier de $f_L$ est
\[\frac{1}{2L} \int_{-L}^L f(x) e^{-i\lambda x}dx.\]
Appelons cette valeur $c_L(\lambda)$. Que se passe-t-il pour des valeurs de 
$L$ de plus en plus grande ?
Notre premier instinct pourrait \^etre de consid\'erer le comportement de 
$c_L(\lambda)$ quand  $L \rightarrow \infty$
mais ce n'est pas tr\`es int\'eressant : il converge toujours vers $0$. Comment contourner cette difficult\'e pour arriver \`a une 
limite int\'eressante ?
\\ \\
{\bf Observation cl\'e.} Lorsque nous essayons de 
construire une ``s\'eries de Fourier'' pour une fonction sur $\rr$, a priori toutes les fonctions de la forme 
\[ e^{i\lambda x}\]
peuvent y appara\^itre, puisqu'il n'y a plus de conditions aux bords qui 
pourraient restreindre les valeurs possibles de $\lambda$.
Notre  ``s\'eries de Fourier'' devraient donc \^etre une ``somme'' 
d'\'el\'ements de la forme 
\[ c(\lambda) e^{i\lambda x}\]
index\'e sur {\bf tous} les nombre r\'eels $\lambda$ : je mets 
le mot somme en guillemets, car c'est math\'ematiquement 
impossible de faire une somme sur tous les nombres r\'eels. En 
math\'ematiques, qu'est ce que c'est qui  joue le r\^ole de ``somme'' sur les 
nombres r\'eels ? C'est l'int\'egrale. On s'attend donc \`a ce que la ``s\'eries de Fourier'' d'une fonction $f$ sur $\rr$ soit une expression {\bf int\'egrale}
pour $f$ de la forme
\[ f(x)= \int_{\rr} c(\lambda) e^{i\lambda x}.\]
On veut comprendre ce qui devrait \^etre la fonction 
$c(\lambda)$ qui va jouer le r\^ole des coefficients de Fourier $c_k(f)$. 
Ce raisonnement heuristique nous sugg\`ere l'id\'ee suivante :\\ \\
{\it Si on veut consid\'erer les limites quand $L\rightarrow \infty$ de la s\'erie de Fourier de la fonction $f_L$, il faudrait commencer par \'ecrire cette s\'erie
comme une int\'egrale.}\\ \\
Rappelons que 
\[ S_{\cc}(f_L)= \sum_{k=-\infty}^\infty c_{k}
e^{ik \pi x/L}\]
que j'\'ecris comme une int\'egrale de la fa\c con suivante 
\[ S_{\cc}= \sum _{k=-\infty}^\infty \frac{L}{\pi} \int_{k\pi/L}^{(k+1)\pi/L} c_{k} 
e^{i k \pi x/L} d\lambda.\]
Ici, j'ai \'ecrit la terme $c_k e^{i k \pi x/L}$ comme l'int\'egrale par rapport \`a une variable $\lambda$
d'une constante sur une intervalle de longeur $\frac{\pi}{L}$.
Appliquant la r\'elation de Chasles \`a cette somme
d'int\'egrales, j'obtiens 
\[ S_{\cc}(f)(x)=\int_{\rr} \frac{L}{\pi} c_L(\lambda) e_L(\lambda,x) d\lambda\]
ou ici, $c_L(\lambda)$ est la fonction en palier telle que pour tout $\lambda
\in  \left[\frac{k\pi}{L}, \frac{(k+1)\pi}{L}\right[$
\[ c_L(\lambda)= \int_{-L}^L e^{-ik\pi x/ L}f(x) dx\]
et $e_L(\lambda ,x)$ est la fonction en palier  telle que pour tout 
$\lambda
\in  \left[\frac{k\pi}{L}, \frac{(k+1)\pi}{L}\right[$
\[ e_L(\lambda,x)= e^{\frac{k\pi i x}{L}} \]
Que se passe-t-il dans cette expression quand $L\rightarrow \infty$,
pour $x$ et $\lambda$ fixes ?   
Les intervalles $[\frac{k\pi}{L},\frac{(k+1)\pi}{L}[$ sont alors de plus
en plus petit, et si $\lambda \in [\frac{k\pi}{L},\frac{(k+1)\pi}{L}[$ 
alors $\lambda \sim \frac{k\pi}{L}$.  Quand $L\rightarrow \infty$
\begin{enumerate}
\item la fonction $e_L(\lambda, x)$ est approch\'ee par la fonction 
$e^{i\lambda x}$
\item la fonction $c_L(\lambda)$ est approch\'ee par la fonction 
$c(\lambda)= \int_{\rr} e^{-i\lambda x}f(x) dx$.
\end{enumerate} 
Ces consid\'erations inspire la d\'efinition suivante. 
\begin{defn}
Soit $f$ une fonction continue sur $\mathbb{R}$ telle que 
$\int_{-\infty}^\infty |f(x)|dx$ est finie. Alors on d\'efinit
la transform\'ee de Fourier de $f$, $\hat{f}(\lambda)$ par
\[ \hat{f}: \lambda \rightarrow \int_{-\infty}^\infty e^{-i\lambda x} f(x)dx.\]
\end{defn}
Nous avons choisi ici de supprimer la terme $\frac{1}{2\pi}$ pour coller 
aux conventions utilis\'ees en physique.
\\ \\
{\bf Exemples}
\begin{enumerate}
\item Soit $f$ la fonction d\'efinie par $f(x)=1$ si $x\in [a,b]$; $f=0$ sinon. Alors
\[ \hat{f}(\lambda)=\int_a^b e^{-i\lambda x}dx = \frac{e^{ib\lambda}- e^{ia \lambda}}{\lambda}\]
\item Soit $f$ la fonction d\'efinie par $f(e)= e^{-x}$ si $x\geq 0$ et $f(x)=0$ sinon. Alors
\[ \hat{f}(\lambda)=\int_0^\infty e^{-i\lambda x} e^{-x}dx=\frac{1}{1+i\lambda}.\]
\end{enumerate}
{\bf Premi\`ere propri\'et\'es des transform\'ees de Fourier.}
\begin{lem}
Soient $f,g$ deux fonctions de $\mathcal{C}^0(\rr,\rr)$ telles que $\int_{\rr} |f(x)| dx$ et $\int_{\rr} |g(x)|dx$ soient finies.
Alors pour tout scalaire $a$ nous avons que 
\[\widehat{f+ag}= \hat{f}+a \hat{g}.\]
\end{lem}
Ce lemme est une cons\'equence imm\'ediate de la lin\'earit\'e de l'int\'egrale.
\begin{lem}
Soit $f$ une fonction de $\mathcal{C}^0(\rr,\rr)$ telle que $\int_{\rr} |f(x)| dx$ et $\int_{\rr} |  \frac{\partial f}{\partial x}}| dx$ soient finies.
On suppose en plus que \[\lim_{x\rightarrow -\infty} f(x)= \lim_{x\rightarrow \infty} f(x)=0.\]
Alors
\[\widehat{ \frac{\partial f}{\partial x}}= i\lambda \hat{f}(\lambda).\]
\end{lem}
{\bf D\'emonstration.}
\\ \\
Par d\'efinition 
\[ \widehat{ \frac{\partial f}{\partial x}}(\lambda)= \lim_{L\rightarrow \infty}
\int_{-L}^L f'(x) e^{-i\lambda x}dx.\]
En faisant une IPP sur cette expression on obtient
\[  \frac{1}{2\pi}\left([f(x) e^{-i\lambda x}]_{-L}^L + 
\int_{-L}^L f(x) (i\lambda) e^{-i\lambda x}dx\right)\]
ce qui compte tenu de l'hypoth\`ese $\lim_{x\rightarrow -\infty} f(x)= \lim_{x\rightarrow \infty} f(x)=0$ nous donne
\[\lim_{L\rightarrow \infty}  \int_{-L}^L i\lambda f(x) e^{-i\lambda x}dx\]
\[= i\lambda \hat{f}(\lambda).\]
Notre derni\`ere proposition, dont la d\'emonstration depasse le cadre de ce cours, dit qu'une \'equivalente 
du th\'eor\`eme de Dirichelet vaut pour cette transform\'ee de Fourier. 
\begin{prop}[Inversion de Fourier.]
Soit $f$ une fonction r\'eelle et continue sur $\rr$. Supposons que 
$\int_{\rr}|f(x)| dx$ et $\int_{\rr} | \hat{f}(\lambda)| d\lambda$ sont tous deux 
finis. Alors, nous pouvons reconstruire $f$ de sa transform\'ee de Fourier par le formule
\[ f(x)=\frac{1}{2\pi}\int_{-\infty}^\infty e^{i\lambda x} \hat{f}(\lambda)d\lambda.\]
\end{prop}
La terme $\frac{1}{2\pi}$ supprim\'ee dans notre d\'efinition de la 
transform\'ee de Fourier rappara\^it ici.\\ \\
Ces deux propri\'et\'es font de la transform\'ee de Fourier un outil pratique pour la solution d'\'equations diff\'erentielles.
\\ \\
{\bf Exemple}
On consid\`ere l'\'equation diff\'erentielle
\[ \frac{\partial f}{\partial x} - f= e^{-|x|}.\]
Appliquant la transform\'ee de Fourier on obtient que
\[ (i\lambda -1) \hat{f}(\lambda)= \frac{2}{1+\lambda^2}\]
Apr\`es reorganisation on a que
\[ \hat{f}=\frac{2}{(1+\lambda^2)(i\lambda-1)}\]
et en utilisant le formule d'inversion de Fourier on obtient que
\[ f(x)=\int_{-\infty}^\infty \frac{2e^{i\lambda x}}{(1+\lambda^2)(i\lambda-1)}d\lambda.\]
\newpage
\section*{Appendice : espace-temps, bases et forme de Minkowski.}
La th\'eorie de la r\'elativit\'e pose que la s\'eparation que nous observons entre l'espace et le temps est 
une illusion et qu'en r\'ealit\'e les \'ev\'enements sont plac\'es dans un continuum de dimension 4 que l'on appelle l'espace-temps.   
Cet espace-temps dont les
points repr\'esentent des \'ev\'enements est de dimension 4, puisque pour pr\'eciser un 
\'ev\'enement il faut donner :
\begin{enumerate}
\item le lieu ou il s'est produit (pr\'ecis\'e dans des coordonn\'ees cartesiennes par 3 donn\'ees num\'eriques) et
\item l'heure \`a laquelle il s'est produit (pr\'ecis\'ee par 1 donn\'ee num\'erique).
\end{enumerate}
Donc, pour pr\'eciser un \'ev\'enement il faut 4 coordonn\'ees. \\ \\
Contrairement \`a la physique Newtonienne, il n'est plus possible dans la r\'elativit\'e restreinte de donner une d\'ecomposition
de l'espace temps en une partie spatiale plus une partie temps. Plus pr\'ecisement, la physique Newtonienne, si elle place bien
les \'evenements dans un continuum de dimension 4 (lieu ou l'\'ev\'enement a eu lieu, plus l'heure \`a laquelle il s'est produit) pose
aussi l'existence une d\'ecomposition intrins\`eque
\[\mbox{Espace-temps $=$ espace $\oplus$ temps.}\]
Deux observateurs Newtoniens, m\^eme en mouvement, seront toujours d'accord pour dire que deux \'ev\'enements ont
lieu au m\^eme moment (c'est \`a dire, que le vecteur qui les s\'epare dans l'espace-temps est contenu dans le sous-espace ``espace'')
ou qu'ils ont lieu au m\^eme endroit  (c'est \`a dire, que le vecteur qui les s\'epare dans l'espace-temps est contenu dans le sous-espace ``temps'').
\\ \\
Ceci n'est plus vrai dans la r\'elativit\'e restreinte : selon les observateurs la d\'ecomposition de l'espace temps en espace et temps va varier. 
Lorsqu'un observateur $O$, non-soumis \`a une acc\'eleration, observe une \'ev\'enement, il va le mesurer utilisant 
son 
{\it r\'ef\'erentiel}, et le r\'esultat de ce mesure sera un 
{\it quadrivecteur} $(T_O, X_O, Y_O, Z_O)$ ou 
\begin{enumerate}
\item $\frac{T_O}{c}$ est l'heure \`a laquelle l'\'ev\`enement s'est produit, 
mesur\'e par l'observateur $O$,
\item  $(X_O, Y_O, Z_O)$ est la position de l'\'ev\'enement, mesur\'e par $O$. 
\end{enumerate}
Ces mesures n'ont plus rien d'absolu ; ils varieront
selon l'observateur.
\\ \\
Qu'est ce que c'est un r\'ef\'erentiel ? {\bf C'est la donn\'ee d'un origine $A$ dans l'espace-temps}\footnote{Ce qui permet de consid\`erer l'espace
temps comme un espace vectoriel, en identifiant un point $P$ avec le vecteur $\stackrel{\rightarrow}{AP}$}  {\bf et d'une base
de l'espace temps}. Apr\`es avoir fix\'e une origine $A$, l'observateur $O$ mesure
l'espace temps utilisant une base qui lui est propre $(t_O, x_O, y_O, z_O)$. Si $v$ est un \'el\'ement de l'espace-temps (que l'on consid\`ere comme
un espace vectoriel avec origine $A$), les coordonn\'ees de $v$
dans la base $(t_O, x_O, y_O, z_O)$ sont pr\'ecisement les 
coefficients du quadri-vecteur $(T_O, X_O, Y_O, Z_O)$.\\ \\
{\bf Attention : si tous les r\'ef\'erentiels ont des bases, seulement certaines bases sp\'eciales peuvent \^etre utilis\'ees dans des 
r\'ef\'erentiels.}  \\ \\
Comment parler des \'ev\'enements ? On peut, bien sur, choisir un observateur et identifier un \'ev\'enemment avec son quadrivecteur
dans le referentiel de cet observateur. Cette solution 
est peu satisfaisante, puisqu'elle nous oblige \`a choisir un r\'ef\'erentiel sp\'ecial auquel nous
donnons une signification particuli\`ere, alors que le principe fondamental de la r\'elativit\'e restreinte est que
toutes les r\'ef\'erentiels se valent. Mais il est aussi possible avec les \'el\'ements ci-dessus
de donner une d\'escription de la r\'elativit\'e
restreinte sans r\'ef\'erentiel distingu\'e.\\ \\
Recapitulons :
\begin{enumerate}
\item L'espace-temps, est un espace {\it affine}\footnote{c'est \`a dire, un espace qui devient un espace vectoriel apr\`es choix d'une origine.} 
de 4 dimensions dont les points repr\'esentent des \'ev\'enements. Il existe ind\'ependemment 
de la choix du r\'ef\'erentiel et du syst\`eme de coordonn\'ees. Nous l'appelerons $ET$.
\item Chaque observateur, $O$, mesure l'espace temps utilisant son propre r\'ef\'erentiel, 
consistant en une choix d'une origine $A$ et une base de $ET$,   
$(t_O, x_O,y_O,z_O)$. ($ET$ est un espace vectoriel apr\`es choix de $A$.) 
\item Soit $v\in ET$ un \'ev\'enement et soient $(T_O,X_O,Y_O,Z_O)$ les coordonn\'ees de $v$ dans la base $(t_O,x_O,y_O,z_O)$. 
L'observateur $O$ verra l'\'ev\'enement $v$ en un temps $T_O$ et une position $(X_O, Y_O, Z_O)$. {\it Le temps et la 
position d'un \'ev\'enement dans le r\'ef\'erentiel de $O$ sont simplement ses coordonn\'ees dans cette base particuli\`ere.}
\end{enumerate}
Une question naturelle se pose :\\ \\
{\bf Question : y a-t-il des propri\'et\'es d'un vecteur $v\in ET$ qui ne d\'ependent pas du choix d'observateur ?}\\ \\
 Si $v$ est un vecteur de $ET$ -- pensons-le comme la s\'eparation entre deux \'ev\'enements, $A$
et $B$ --
alors ni le temps ni la distance r\'epresent\'e par $v$ n'est ind\'ependent de 
l'observateur. 
Mais il y une notion intrins\`eque, au moins pour 
les vecteurs de type temporels, c'est celui du 
{\it temps propre}.
\begin{defn}
Soit $v= \stackrel{\rightarrow}{AB}$ un vecteur de $ET$. Le temps propre de $v$ est
 {\it le temps v\'ecu par un observateur qui voyage de A et B sans acc\'eleration.}
\end{defn}
De nombreuses exp\'eriences ont \'etablie comme donn\'ee exp\'erimentale la 
r\'elation suivante
entre le temps propre $TP(v)$ d'un vecteur temporel 
et ses coordonn\'ees $(T_O, X_O, Y_O, Z_O)$ mesur\'es par un observateur $O$ : 
\[ TP(v)^2= T_O^2- X_O^2 -Y_O^2 -Z_O^2.\]
On reconna\^it dans le membre de droite la forme quadratique associ\'ee \`a une forme bilin\'eaire, auxquelles nous donnons le nom de ``produit
scalaire de Minkowski''.
\begin{defn}
Soit $v,v'\in ET$, soit $O$ un observateur, soit le r\'ef\'erentiel de $O$ donn\'e par $(t_O, x_O, y_O, z_O)$. Soient
$(T_O, X_O, Y_O, Z_O)$ et $(T'_O, X'_O, Y'_O, Z'_O)$ les quadrivecteurs de $v$ et $w$ mesur\'es par $O$.
Le produit scalaire  de Minkowski est la forme bilin\'eaire sur $ET$ donn\'ee par
\[ M(v,v')= T_O T'_O - X_O X'_O -Y_OY'_O-Z_OZ'_O.\]
Sa forme quadratique associ\'ee $q_M(v)= M(v,v)$ a le propri\'et\'e que pour tout $v$ temporel
\[ q_M(v)=(\mbox{temps propre de }v)^2.\]
\end{defn}
{\bf Attention : 
il y ici un abus de notation. Puisqu'il existe des vecteurs pour lesquels $q_M(v)<0$, la forme de Minkowski  n'est pas un produit scalaire au sens des 
math\'ematiciens.}
\\ \\
Plusieurs notions de la r\'elativit\'e restreinte admettent une interpretation en termes de la forme de Minkowski.
\begin{enumerate}
\item Un vecteur $v\in ET$ est temporel si $q_M(v)>0$, luminaire si $q_M(v)=0$ et spatial si $q_M(v)<0$.
\item Les transformations de Lorentz  sont des matrices $P$ de changement de base (ou de changement de referentiel
en gardant le m\^eme origine) qui laissent invariant la forme de Minkowski.
\end{enumerate} 
\newpage
\section*{Appendice : les coniques et quadriques.}

On va maintenant appliquer les r\'{e}sultats pr\'{e}c\'{e}dents \`{a} l'\'{e}tude des coniques et des quadriques.

Jusqu'\`{a} la fin de  ce paragraphe, on se place dans l'espace affine $\rr^2$ ou $\rr^3$, muni de son rep\`{e}re orthonorm\'{e} usuel.

\begin{defn}
Une {\bf conique} est le lieu g\'{e}om\'{e}trique de $\rr^2$ d\'{e}fini par une \'{e}quation de la forme 
$$ax^2+by^2+2cxy+dx+ey+f=0,$$

o\`{u} au moins un des termes quadratiques est non nul.

Une {\bf quadrique} est le lieu g\'{e}om\'{e}trique de $\rr^3$ d\'{e}fini par une \'{e}quation de la forme 
$$ax^2+by^2+cz^2+2dxy+2exz+2fyz+\alpha x+\beta y+\gamma z+ \delta=0,$$
o\`{u} au moins un des termes quadratiques est non nul.
\end{defn}

On veut classer les diff\'erents types de coniques et quadriques. Puisque l'on veut conserver le lieu g\'{e}om\'{e}trique, on ne s'autorise qu'\`{a} faire des changements de variables qui
remplacent le rep\`Ã¨re canonique par un autre rep\`ere orthonorm\'ee (pour les produit  scalaire canonique), donc des translations ou des isom\'{e}tries (i.e. qui conservent les distances et les 
angles non orient\'{e}s).

Ceci est n\'{e}cessaire: en effet, l'\'{e}quation $$\frac{x^2}{9}+\frac{y^2}{4}=1$$ repr\'{e}sente une ellipse de centre $O$, alors que le changement de variables $x'=\frac{x}{3}$ et $y'=\frac{y}{2}$ donne l'\'{e}quation $$x'^2+y'^2=1$$ repr\'{e}sente le cercle unit\'{e}, qui n'est pas le m\^{e}me lieu g\'{e}om\'{e}trique.

{\bf Cas des coniques. }

Consid\'{e}rons la conique $$ax^2+by^2+2cxy+dx+ey+f=0, $$
et soit $$q:\rr^2 \to \rr, (x,y)\mapsto ax^2+by^2+2cxy.$$





D'apr\`{e}s le th\'{e}or\`{e}me 4.32, il existe une base orthornorm\'{e}e $(v_1,v_2)$ qui est $\phi_q$-orthogonale, donc $q$-orthogonale.

Dans le nouveau rep\`{e}re $(O,v_1,v_2)$, l'\'{e}quation de la conique s'\'{e}crit
$$a'x'^2+c'y'^2+d'x+e'y+f'=0.$$

On se d\'{e}barasse ensuite d'un ou deux termes lin\'{e}aires en compl\`{e}tant les carr\'{e}s et en effectuant une translation d'origine.


Apr\`{e}s \'{e}ventuellement permutation des nouvelles variables et/ou changements du type $X\leftrightarrow -X$ ou $Y\leftrightarrow -Y$    
on obtient une \'{e}quation d'un des types suivants, selon la signature de $q$, pour des r\'eels $U,V >0$


$(1)$ Signature $(2,0)$ ou $(0,2)$: 
\begin{enumerate}
\item $\ds\frac{X^2}{U^2}+\frac{Y^2}{V^2}=1$: c'est une ellipse (ou un cercle si $U=V$).
\item $\ds\frac{X^2}{U^2}+\frac{Y^2}{V^2}=0$ : c'est le point $(X,Y)=(0,0)$.
\item $\ds\frac{X^2}{U^2}+\frac{Y^2}{V^2}=-1$ : c'est l'ensemble vide.
\end{enumerate}

$(2)$ Signature $(1,1)$: 
\begin{enumerate}
\item $\ds\frac{X^2}{U^2}-\frac{Y^2}{V^2}=1$: c'est une hyperbole.
\item  $\ds\frac{X^2}{U^2}-\frac{Y^2}{V^2}=0$: c'est la r\'eunion des
deux droites d'\'equation $X/U=Y/V$ et $X/U=-Y/V$.
\end{enumerate}

$(3)$ Signature $(1,0)$ ou $(0,1)$: $Y^2=2\lambda X$: c'est une parabole.

\begin{ex}
Consid\'{e}rons la conique d'\'{e}quation $$3x^2-3y^2+8xy+6\sqrt{5}x+2\sqrt{5}y+5=0.$$

Soit $q:\rr^2\to\rr, \begin{pmatrix}x\cr y\end{pmatrix}\mapsto 3x^2-3y^2+8xy$.

Sa matrice repr\'{e}sentative dans la base canonique est $$\begin{pmatrix}3 & 4\cr 4 & -3\end{pmatrix}.$$

D'apr\`{e}s un exemple pr\'{e}c\'{e}dent, une base orthonorm\'{e}e qui est aussi $q$-orthogonale est donn\'{e}e par 

$$\frac{1}{\sqrt{5}}\begin{pmatrix}2\cr 1\end{pmatrix},\frac{1}{\sqrt{5}}\begin{pmatrix}1\cr -2\end{pmatrix},$$
les vecteurs \'{e}tant respectivement des vecteurs propres pour $5$ et $-5$.


Soient $x',y'$ les coordonn\'{e}es dans cette nouvelle base. On a donc

 $$\begin{pmatrix}x\cr y\end{pmatrix}=\frac{1}{\sqrt{5}}\begin{pmatrix}2& 1\cr 1& -2\end{pmatrix}\begin{pmatrix}x'\cr y'\end{pmatrix}=\frac{1}{\sqrt{5}}\begin{pmatrix}2x'+y'\cr x'-2y'\end{pmatrix}.$$

Par construction de cette base, la forme $q$ dans cette base s'\'{e}crit $$5x^{'2}-5y^{'2}.$$
Elle est donc de signature $(1,1)$, et on a donc une hyperbole (sauf cas d\'{e}g\'{e}n\'{e}r\'{e}).
 
On a alors $$5x^{'2}-5y^{'2}+10x'+10y'+5=0,$$
soit 
$$x^{'2}-y^{'2}+2x'+2y'+1=0.$$
On a donc $$(x'+1)^2-(y'-1)^2+1=0.$$
En posant $X=x'-1,Y=y'-1$, on obtient $X^2-Y^2=-1.$ En posant $X'=Y$ et $Y'=X$, on obtient finalement l'\'{e}quation r\'{e}duite de l'hyperbole $$X^{'2}-Y^{'2}=1.$$
\end{ex}

{\bf Cas des quadriques. }

Comme pr\'{e}c\'{e}demment, on se ram\`{e}ne au cas d'une \'{e}quation sans termes crois\'{e}s, et on se d\'{e}barasse d'un ou plusieurs termes lin\'{e}aires. 

Apr\`{e}s \'{e}ventuellement permutation des nouvelles variables et/ou changements du type $X\leftrightarrow -X$, $Y\leftrightarrow -Y$, $Z\leftrightarrow -Z$, et \'{e}ventuellement une nouvelle translation/rotation,
   
on obtient la classification suivante :

$(1)$ Signature $(3,0)$ ou $(0,3)$. On obtient 3 cas : 

$(a)$ $\ds\frac{X^2}{U^2}+\frac{Y^2}{V^2}+\frac{Z^2}{W^2}=1$: c'est un ellipso\"{\i}de. 

$(b)$ $\ds\frac{X^2}{U^2}+\frac{Y^2}{V^2}+\frac{Z^2}{W^2}=0$: c'est le point
$(X,Y,Z)=(0,0,0)$

$(c)$ $\ds\frac{X^2}{U^2}+\frac{Y^2}{V^2}+\frac{Z^2}{W^2}=-1$: c'est 
l'ensemble vide.  

$(2)$ Signature $(2,1)$ ou $(1,2)$. On obtient 3 cas:

$(a)$ $\ds\frac{X^2}{U^2}+\frac{Y^2}{V^2}-\frac{Z^2}{W^2}=-1$: c'est un hyperbolo\"{\i}de \`{a} deux nappes

$(b)$ $\ds\frac{X^2}{U^2}+\frac{Y^2}{V^2}-\frac{Z^2}{W^2}=1$: c'est un hyperbolo\"{\i}de \`{a} une nappe
 
$(c)$ $\ds\frac{X^2}{U^2}+\frac{Y^2}{V^2}=\frac{Z^2}{W^2}$: c'est un c\^{o}ne.

$(3)$ Signature $(2,0)$ ou $(0,2)$.
On obtient quatre cas :

$(a)$ $\ds\frac{X^2}{U^2}+\frac{Y^2}{V^2}=1$: c'est un cyclindre elliptique

$(b)$ $\ds\frac{X^2}{U^2}+\frac{Y^2}{V^2}=0$: c'est la droite $X=Y=0$.

$(c)$ $\ds\frac{X^2}{U^2}+\frac{Y^2}{V^2}=-1$: c'est l'ensemble vide.

$(d)$ $\ds\frac{X^2}{U^2}+\frac{Y^2}{V^2}=\frac{Z}{W}$: c'est un parabolo\"{\i}de elliptique.

$(4)$ Signature $(1,1)$:

$(a)$ $\ds\frac{X^2}{U^2}-\frac{Y^2}{V^2}=1$: c'est un cyclindre hyperbolique.

$(b)$ $\ds\frac{X^2}{U^2}-\frac{Y^2}{V^2}=0$: c'est la r\'eunion des deux 
plans d'\'equation $X/U-Y/V=0$ et $X/U+Y/V=0$

$(c)$ $\ds\frac{X^2}{U^2}-\frac{Y^2}{V^2}=-1$: c'est l'ensemble vide.

$(d)$ $\ds\frac{X^2}{U^2}-\frac{Y^2}{V^2}=\frac{Z}{W}$: c'est un parabolo\"{\i}de hyperbolique.

Signature $(1,0)$ ou $(0,1)$: 

$(a)$ $X^2=2pY$: cylindre parabolique. 

$(b)$ $X^2/U^2=1$: r\'eunion de deux plans parall\`eles d'\'equation $X=1$ et $X=-1$.

$(c)$ $X^2/U^2=0$: plan $X=0$

$(d)$ $X^2/U^2=-1$: ensemble vide.

\begin{ex}

Soit la quadrique $$x^2+y^2+z^2+2xy+2xz+2yz+ \sqrt{3}x+\sqrt{3}y+2=0.$$


Soit $q:\rr^3\to\rr, \begin{pmatrix}x\cr y\end{pmatrix}\mapsto x^2+y^2+z^2+2xy+2xz+2yz$.

Sa matrice repr\'{e}sentative dans la base canonique est $$\begin{pmatrix}1 & 1 & 1\cr 1 & 1 & 1\cr 1&1&1\end{pmatrix}.$$

D'apr\`{e}s un exemple pr\'{e}c\'{e}dent, une base orthonorm\'{e}e qui est aussi $q$-orthogonale est donn\'{e}e par 
$$\frac{1}{\sqrt{3}}\begin{pmatrix}1\cr 1\cr 1\end{pmatrix},\frac{1}{\sqrt{2}}\begin{pmatrix}1\cr -1\cr 0\end{pmatrix},\sqrt{\frac{2}{3}}\begin{pmatrix}\frac{1}{2}\cr \frac{1}{2}\cr -1\end{pmatrix},$$
ces vecteurs \'{e}tant respectivement des vecteurs propres pour $1,0$ et $0$.


Soient $x',y',z'$ les coordonn\'{e}es dans cette nouvelle base. 

Par construction de cette base, la forme $q$ dans cette base s'\'{e}crit $$3x^{'2}.$$
Elle est donc de signature $(1,0)$, et on a donc un cyclindre parabolique, un ensemble vide, un plan ou une r\'eunion de deux plans.


On v\'{e}rife que l'\'{e}quation de cette quadrique dans cette base est 

$$3x^{'2}+2x'+\sqrt{2}z' +2=0,  $$
soit $$3(x'+\frac{1}{3})^2+\sqrt{2}z'+\frac{5}{3}=0.$$
Si on pose $\ds X=x'+\frac{1}{3}, Y=z'+\frac{5}{3\sqrt{2}}, Z=y'$, on obtient $$X^2=-\sqrt{2}Y.$$
\end{ex}
\newpage
\section*{Appendice : Formes hermitiennes.}

Dans beaucoup d'applications en physique - notamment en m\'ecanique quantique, mais pas exclusivement - 
nous avons besoin d'utiliser des espaces complexes. Par exemple, la fonction d'onde qui repr\'esente
un particule dans la repr\'esentation de Schr\" odinger, est un \'el\'ement de $C^0(\rr^3, \cc)$, c'est-`a-dire ,
une fonction complexe sur l'espace $\rr^3$. \\ \\
Mais si on essaie de d\'efinir une notion de longueur sur un espace complexe $V$ utilisant des formes bilin\'eaires
complexes on se rend rapidement compte que c'est impossible. En effet, aucune 
forme bilin\'eaire complexe $\varphi$ ne peut
avoir une forme quadratique associ\'ee 
qui est r\'eelle positive partout, puisque si
\[ \varphi(v,v)>0\]
alors on a
\[ \varphi(iv,iv)= i^2 \varphi (v,v)= -\varphi (v,v)<0\]
Par contre, on sait que la fonction\[f(z)=\overline{z}z\] est partout
r\'eelle et positive sur $\mathbb{C}$ : d'ailleurs, la distance euclidienne sur
$\mathbb{C}$, vu comme un $\rr$-espace vectoriel, est donn\'ee par 
\[ d(z_1, z_2) =\sqrt{(\overline{z_1-z_2}) (z_1-z_2)}.\] 
Nous allons donc essayer de d\'efinir des distances sur des espaces complexes en utilisant des formes {\it hermitiennes},
c'est-\`a-dire, des functions de deux variables se comportant comme la fonction 
\[ (z_1, z_2)\mapsto \overline{z_1}z_2.\] 
\begin{defn}
Soit $V$ un espace vectoriel complexe. Une fonction
\[h:V\times V\rightarrow \mathbb{C}\] est une forme hermitienne si et
seulement si
\begin{enumerate}
\item $h(x+y,z)=h(x,z)+h(y,z)$
\item $h(x,\lambda y)= \lambda h(x,y)$
\item $h(x,y)=\overline{h(y,x)}$.
\end{enumerate}
\end{defn}
Notez qu'il suit de (3) que $h(x,x)$ est r{\'e}el pour tout $x$. Nous
avons,
 parailleurs, que $h(\lambda x,y)=\overline{\lambda}h(x,y)$.
\begin{exs}
\begin{enumerate}
\item
La forme $h$ d{\'e}finie sur $\mathbb{C}^n\times \mathbb{C}^n$ par
\[h\left(\begin{pmatrix}{x_1}\\{x_2}\\{\vdots}\\{x_n}\end{pmatrix}
, \begin{pmatrix}{y_1}\\{y_2}\\{\vdots}\\{y_n}\end{pmatrix}\right)
=\sum \overline{x_i}y_i\]
est une forme hermitienne. Celle-ci s'appelle la forme hermitienne
canonique sur $\mathbb{C}^n$.
\item La forme $h$ d\'efinie sur $C^0([a, b], \cc)\times C^0([a, b], \cc)$ 
par
\[ h(f,g)=\int_a^b \overline{f}(x) g(x) dx\]
est une forme hermitienne sur $C^0([-1, 1], \cc)$.
\item La forme $h$ d\'efinie sur $M_n(\cc)\times M_n(\cc)$ par
\[ h(M,N)= {\rm Tr}({}^t\overline{M} N)\]
est une forme hermitienne.
\end{enumerate}
\end{exs}
On dit qu'une forme hermitienne $h$ est d\'efinie positive si pour tout 
$x\in V\setminus {0_V}$ nous avons que 
\[ h(x,x)>0.\]
\begin{defn}
Un espace hermitien $(V,h)$ est la donn{\'e}e d'un espace vectoriel complexe 
$V$ et d'une forme hermitienne $h$, d{\'e}finie positive sur $E$.
\end{defn}
Bien s\^ur, tout ce que nous avons fait pour les formes bilin{\'e}aires
 sym{\'e}triques peut aussi se faire pour les formes hermitiennes.
\begin{prop}
Soit $E$ un espace vectoriel complexe et soit $h$ une forme hermitienne sur 
$E$.
Soit ${\bf e}= (e_1\ldots e_n)$ une base pour $E$ et
soit $M$ la matrice d{\'e}finie par $M_{i,j}=h(e_i,e_j)$. La matrice $M$ est appell\'ee la matrice de $h$ dans la base ${\bf e}$.
Soient $x,y\in E$
et soient $\underline{X}, \underline{Y}$ leurs vecteurs de coordonn\'ees dans 
la base ${\bf e}$. Alors nous avons
\[ h(x,y)= {}^t \overline{\underline{X}}M \underline{Y}\] 
\end{prop}
\begin{prop}[R{\`e}gles de changement de la base]
Soit $E$ un espace vectoriel complexe et soit $h$ une forme hermitienne sur
$E$.
Soient ${\bf e}= (e_1\ldots e_n)$ et ${\bf f}=(f_1\ldots f_n)$ deux bases pour 
$E$.
Soit $M$ la matrice de $h$ dans la base ${\bf e}$ et soit
$N$ la matrice de $h$ dans la base ${\bf f}$. Soit $P$ la
matrice de passage de ${\bf e}$ vers ${\bf f}$.
Alors
\[ N={}^t\overline{P}MP.\]
\end{proposition}
Nous notons que condition (3) de la d\'efinition des formes hermitiennes
implique
la proposition suivante.
\begin{prop}
Soit $h$ une forme hermitienne sur un espace complexe, $V$. Soit $M$ la
matrice de $h$ dans une base ${\bf e}$. Alors 
\[{}^t\overline{M}=M\]
\end{prop}
Cette proposition inspire la d\'efinition suivante.
\begin{defn}
Soit $M$ une matrice complexe. L'adjointe $M^*$ de $M$ est la matrice d\'efinie par
\[{}^t\overline{M}=M^*.\]
\end{defn}
\begin{defn}
Une matrice complexe $M$ de taille $n\times n$ est dite {\it hermitienne} (ou
{\it auto-adjointe}) si et
seulement si
\[M^*={}^t\overline{M}=M.\]
\end{definition}
Nous pouvons d\'efinir le longueur d'un vecteur et la distance entre deux 
\'el\'ements dans un espace hermitien comme pour un espace prehilbertien.
\begin{defn}
Soit $(V,h)$ un espace hermitien et soient $v,w\in V$. On d\'efinit la 
longueur de $v$ par
\[ || v || =\sqrt{h(v,v)},\]
 et la distance entre $v$ et $w$ par $d(v,w)= ||v-w||$.
\end{defn}
Avec cette notion de distance et de longueur, une version du proc\'ed\'e de 
Gram-Schmidt et la projection orthogonale sont valables aussi sur
des espaces hermitiens.
\begin{prop}[Projection orthogonale dans les espaces hermitiens.]
Soit $(V,h)$ un espace hermitien et soit $W\in E$ un sous-espace vectoriel
de dimension finie. Soit $(v_1,\ldots, v_n)$ une base orthonorm\'ee pour $W$.
Alors pour tout $v\in V$ on d\'efinit la projection orthogonale de $v$ sur $W$
par
\[ p_W(v)= \sum_{i=1}^n h(v_i, v) v_i.\]
La projection $p_W(v)$ est alors l'\'el\'ement de $W$ qui minimise la distance
$d(v,w)$ lorsque $w$ parcourt $W$.
\end{prop}
La d\'emonstration est identique \`a celle donn\'ee dans le cas des espaces 
prehilbertiens.
\begin{prop}
Soit $(V,h)$ un espace hermitien de dimension finie et soit ${\bf e}=(e_1,
\ldots, e_n)$ une base. On construit une nouvelle base $(v_1,\ldots, v_n)$
r\'ecursivement par
l'algorithme suivant :
\begin{enumerate}
\item On pose $v_1=\frac{e_1}{||e_1||}$.
\item La famille $(v_1,\ldots,v_k)$ \'etant construite, nous posons
\[f_{k+1} = e_{k+1} -\sum_{i=1}^k h(v_i, e_{k+1}) v_i.\]
\item On pose $v_{k+1} = f_{k+1}/|| f_{k+1}||$.
\item On a maintenant construit $(v_1,\ldots, v_{k+1})$ et on revient \`a
l'\'etape (2) pour construire $v_{k+1}$. 
\end{enumerate} 
La base de $(V,h)$ ainsi construite est orthonorm\'ee.
\end{prop}
La d\'emonstration est identique \`a celle donn\'ee dans le cas des espaces
prehilbertiens.
\newpage
\section*{Appendice : le tenseur d'inertie d'un corps rigide.}
Dans cette appendice, pour rester plus proche des notations utilis\'ees en physique, 
nous d\'enoterons les quantit\'es vectorielles dans $\mathbb{R}^3$ par des 
lettres en gras. \\ \\
Soit $C$ un corps rigide massif dans l'espace $\mathbb{R}^3$. Nous voudrions comprendre 
le moment d'inertie de ce corps, c'est \`à dire, la resistence qu'elle oppose \`a \^etre mise en
rotation. 
\\ \\
Pour plus de simplicit\'e nous nou pla\c cons dans
un referentiel inertiel\footnote{c'est \`a dire la donn\'ee d'une origine dans l'espace et
une base de $\mathbb{R}^3$ ne variant pas avec le temps.} dont l'origine est le centre de gravit\'e 
$G$ du corps $C$, et consid\'erons une rotation du corps $C$ autour de son centre de gravit\'e $G$ qui resterait fixe. 
 Supposons que le corps $C$ poss\`ede une vitesse angulaire ${\bf \omega}$.\footnote{La vitesse
angulaire d'un corps en rotation dont le centre de gravit\'e est fixe est une quantit\'e vectorielle  ${\bf \omega}$ telle que 
la vitesse d'un point $P\in C$ est donn\'ee par ${\bf \omega}\wedge \stackrel{\rightarrow}{GP}$.
La direction de ${\bf \omega}$ donne l'axe de rotation et sa longueur donne la vitesse.}
Quel serait le moment angulaire produit par cette rotation autour d'un autre axe
${\bf \nu}$ ? 

Dans un premier temps, consid\'erons un r\'eseau rigid de points massifs $P_i$,
ou chaque $P_i$ a masse $m_i$. En un temps $t$, soit ${\bf r_i}$ le vecteur de position
du point massif $P_i$ (c'est \`a dire qu'on a ${\bf r_i}= \stackrel{\longrightarrow}{GP_i}$)
Soit ${\bf v}_i$ la vitesse (normale) du point $P_i$. Le moment angulaire du point massif $P_i$
autour de l'axe ${\bf \nu}$ est donc 
\[ m_i{\bf \nu}\cdot({\bf r}_i\wedge {\bf v}_i).\] 
Mais on sait par ailleurs par la d\'efinition de la vitesse angulaire que 
\[ {\bf v}_i= {\bf \omega}\wedge {\bf r}_i\]
ou $\omega$ est la vitesse angulaire. Lorsque $P_i$ tourne avec une vitesse 
angulaire $\omega$ autour de son centre de gravit\'e, 
le moment angulaire autour de l'axe $\nu$ du point massif $P_i$ est 
donn\'e par l'expression
\[m_i{\bf \nu}\cdot({\bf r}_i\wedge({\bf \omega}\wedge {\bf r}_i)).\]
Le moment total de ce r\'eseau de points est alors donn\'e par la somme
\[\sum_i m_i {\bf \nu}\cdot({\bf r}_i\wedge({\bf \omega}\wedge {\bf r}_i)\]
Approchons maintenant $C$ par un maillage de point massifs $P_i$, chacun
de masse $m_i$, en une position ${\bf r}_i$. Le moment angulaire de $C$ autour de l'axe $\nu$ est alors approch\'e par la quantit\'e
\[\sum_i m_i {\bf \nu}\cdot({\bf r}_i\wedge({\bf \omega}\wedge {\bf r}_i)\]
ou $\omega$ est la vitesse angulaire du corps $C$. \\ \\ 
Pour un maillage assez fini de $C$, la masse $m_i$ du point massif 
$P_i$ est tr\`es 
proche de 
$\rho dV$, ou $\rho$ est la densit\'e locale de $C$  au point $P_i$ et $dV$ est l'\'el\'ement de volume autour de $P_i$. On
obtient, en approchant $C$ par un maillage toujours plus fin de points $P_i$ que le moment angulaire de $C$ autour de 
$\nu$ est donn\'e par 
\[\int_C  \rho {\bf \nu}\cdot({\bf r}_i\wedge({\bf \omega}\wedge {\bf r}_i) dV\]
\[ ={\bf \nu} \cdot \int_C \rho (({\bf r}\wedge({\bf r}\wedge{\bf \omega})) dV\]
\[={\bf\nu}\cdot \int_C\rho ({\bf \omega}\cdot || {\bf r}^2|| - {\bf r}({\bf \omega}\cdot  {\bf r}))dV\]
\[={\bf \nu}\cdot {\bf \omega} \left(\int_C\rho ||{\bf r}||^2dV\right)-\int_C ({\bf \nu}\cdot {\bf r}) ({\bf \omega}\cdot {\bf r}).\]
ou $\oemga$ est la vitesse angulaire du corps $C$. \\ \\
Pour r\'esumer, nous avons la proposition suivante.
\begin{prop}
Soit $I_C$ la fonction de deux vecteurs dans $\mathbb{R}^3$, ${\bf \omega}$ et ${\bf \nu}$, telle que
$I_C({\bf \omega}, {\bf \nu})$ le moment d'inertie du corps rigid $C$ autour de l'axe ${\bf \nu}$ lorsque $C$ tourne autour de $G$ 
avec vitesse angulaire ${\bf \omega}$}.\\ \\
Alors nous avons que
\[ I_C({\bf \omega}, {\bf \nu})= {\bf \nu}\cdot {\bf \omega} \left(\int_C\rho ||{\bf r}||^2dV\right)-\int_C ({\bf \nu}\cdot {\bf r}) ({\bf \omega}\cdot {\bf r}).\]
\end{prop}
L'exercice suivant est laiss\'e au lecteur.\\ \\
{\bf Exercice} L'application $I_C$ est une forme bilin\'eaire sym\'etrique.
\begin{defn}
Soit $C$ un corps rigide massif dans l'espace. La forme bilin\'eaire sym\'etrique $I_C$ s'appelle 
le {\it tenseur d'inertie} du corps $C$. 
\end{defn}
\begin{rem}
L'expression du tenseur d'inertie dans une base inertielle d\'epende de l'orientation de $C$ dans l'espace. En particulier, lorsque 
l'orientation de $C$ varie dans le temps\footnote{Notamment, lorsque $C$
est en rotation.} l'expression de $I_C$ dans une base inertielle ne sera pas constante. 
\end{rem}
\begin{rem}
Le th\'eor\`eme sur la diagonalisation orthonorm\'ee des formes bilin\'eaires sym\'etrique nous garantit l'existence d'une
base orthonorm\'ee pour $\mathbb{R}^3$, ${\bf i}, {\bf j}, {\bf k}$, qui est une base orthogonale pour le tenseur de l'inertie.
\end{rem}
\begin{defn}
Les \'el\'ements ${\bf i}, {\bf j}, {\bf k}$ de la base qui est orthonorm\'ee pour $\mathbb{R}^3$ et orthogonale pour $I_C$ 
s'appellent les axes principaux d'inertie du corps $C$.
\end{defn}
\begin{rem}
Ces axes principaux dépendent, bien sur, de l'orientation du corps $C$.
\end{rem}
Calculons maintenant le moment total de $C$, $\mu_C$.\footnote{Ce moment total d'un corps en rotation est une
quantit\'e vectorielle ${\bf \mu}_C$ qui a le propri\'et\'e que pour tout axe ${\bf \nu}$ le moment de $C$ autour de ${\bf \nu}$ est 
donn\'e
par le produit scalaire ${\bf \nu}\cdot {\bf \mu}_C$.} 
Il suit imm\'ediatement de notre formule pour $I_C(\omega, \nu)$ que 
$\mu_C$ est donn\'e par le formule
\[ {\bf \mu}_C=  {\bf \omega} \left(\int_C\rho ||{\bf r}||^2dV\right) -\int_C \rho {\bf r}({\bf \omega}\cdot {\bf r})dV.\] 
%\begin{rem}
%Si le corps $C$ n'a pas de sym\'etrie alors rien ne garanti
%que ${\bf \mu}_C$ sera align\'e avec l'axe de rotation $\omega$. En particulier, 
%si la vitesse angulaire $\omega$ ne varie pas dans le temps alors le moment total ${\bf \mu}_C$ tourne, comme
%$C$, autour de l'axe $\omega$ et n'est donc pas constante. Or, par le principe de conservation du moment, nous savons que
%${\bf \mu}_C$ ne peut varier que si $C$ est soumis \`a une torque ext\'erieure. Autrement dit, {\it si le moment total
%${\bf \mu}_C$ n'est pas align\'e avec ${\bf \omega}$ alors rotation autour de l'axe $\omega$ n'est pas un \'etat
%stable de $C$.}
%\end{rem}
\subsection{Application : rotation libre
d'un objet avec symm\'etrie rotationelle.}

Nous savons qu'il existe une base une base ${\bf i}$, ${\bf j}$, ${\bf k}$ telle que \`a tout moment,
 ${\bf i}$, ${\bf j}$, ${\bf k}$ est la base orthonorm\'ee de $\mathbb{R}^3$ qui est aussi orthogonale pour 
$I_C$. Cette base est variable dans le temps - elle n'est pas un referentielle inertielle - mais elle
est constante du point de vue du corps rigid et en particulier, si $C$ est en rotation autour de son centre de
gravit\'e avec vitesse angulaire $\omega$ alors on a que
\[ {\bf i}'= {\bf i}\wedge \omega \, {\bf j}'={\bf j}\wedge \omega 
\, {\bf k}'={\bf k}\wedge \omega.\]
Ici, pour toute quantit\'e vectorielle ${\bf a}$ nous notons ${\bf a}'$ 
sa deriv\'ee dans une base inertielle. Par ailleurs, nous notons
$\frac{\partial {\bf a}}{\partial t'}$ sa d\'eriv\'ee 
dans la base ${\bf i}$, ${\bf j}$, ${\bf k}$. Nous avons alors que
\[ \frac{\partial {\bf a}}{\partial t'}+ {\bf a}\wedge \omega={\bf a}'.\] 
Dans le referentiel $({\bf i}$, ${\bf j}$, ${\bf k})$ la matrice de 
$I_C$ est constante et diagonale. Consid\'erons le cas ou $C$ a une 
symm\'etrie rotationelle autour de l'axe ${\bf i}$ : la matrice de $I_C$ dans 
la base ${\bf i}, {\bf j}, {\bf k}$ est alors de la forme
\[\begin{pmatrix}{a&0&0\\0&b&0\\0&0&b}\end{pmatrix}.\]
Supposons maintenant que le corps $C$ tourne librement,
sans torque ext\'erieure. On sait alors que son moment d'inertie $\mu_C$
satisfait l'\'equation
\[ {\bf \mu}_C'=0\]
En passant dans le rep\`ere ${\bf i}$, ${\bf j}$, ${\bf k}$ on obtient
\[ \frac{\partial {\bf \mu}_C}{\partial t'}= -\mu_C\wedge\omega \]
ce qui, en utilisant le fait que $\mu_C= I_C(\omega)$
nous donne
\[ I_C\frac{\partial {\bf \omega}}{\partial t'}= -{I_C{\omega}\wedge\omega. \]
Ecrivant $\omega= x{\bf i}+y{\bf j}+z{\bf k}$ on voit que 
\[ ax'= 0\]
\[ by'= (a-b)xz\]
\[ bz'=(b-a) xy\]
La valeur $x$ est donc une constante. Posons $\frac{(b-a)x}{b}=C$,
 nous avons alors les \'equations
\[y'= -Cz\]
\[z'= Cy\]
qui a pour solution g\'en\'erale 
\[ y= \lambda\cos{Ct+\theta}\]
\[z=\lambda \sin{Ct+\theta}.\]
L'axe de rotation $\omega$ n'est donc pas constante dans le referentiel 
corporel. 
Il tourne autour de l'axe de sym\'etrie rotationnelle ${\bf i}$ avec une vitesse qui d\'epende de
\begin{enumerate}
\item l'angle entre ${\bf \omega}$ et ${\bf i}$
\item le rapport entre $a$ et $b$.
\end{enumerate}
\end{document}
